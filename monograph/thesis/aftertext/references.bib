
%
% Graphical Java application for managing BibTeX and BibLaTeX (.bib) databases (Windows/Linux/MAC)
% https://github.com/JabRef/jabref
%
% BibDesk-like software for Windows
% https://tex.stackexchange.com/questions/9454/bibdesk-like-software-for-windows
%
% LaTeX Bibliography Management
% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#BibTeX
%
%
% $ date "+%Y-%m-%d %H:%M:%S"
% 2017-08-28 09:18:02
%
% YYYY-MM-DD format date in shell script
% https://stackoverflow.com/questions/1401482/yyyy-mm-dd-format-date-in-shell-script
%


@thesis{structuredEditorStudy,
    title         = {Estudo e Criação de um Editor de Código Estruturado},
    author        = {Lucas Boppre Niehues},
    year          = {2013},
    publisher     = {University Library},
    address       = {Florianópolis, Santa Catarina, Brazil},
    abstract      = {Virtually all of the code editors for business use in general\hyp{}purpose
                    languages are oriented to lines and characters. The alternative, editors working
                    directly with the syntactic tree, has received most attention in this area,
                    aiming to have important advantages, such as automatic control of the structure
                    and greater efficiency in the script. The purpose of this paper is to study these
                    alternating active editors, called structured editors or syntax-driven editors,
                    comparing them with traditional editors. Once the advantages and disadvantages
                    have been raised, a structured editor will be developed that best fits in with
                    this medium. This work first provides for an in-depth study of these structured
                    editors, in the context of programming in general or non-general purpose
                    languages. Previous efforts in this area will be sought, assessing their impact,
                    lessons learned and limitations encountered. In a second moment these lessons
                    will be used for the development of a proprietary, fully structured or hybrid
                    editor, in order to increase the efficiency of the programmer. Preferably this
                    editor will be prepared for a standard general purpose language and will be able
                    to edit actual programs. This work aimed to better understand the principles
                    behind structured editors, seeking information on their effectiveness,
                    advantages, disadvantages and factors involved in their lack of adoption. It is
                    also envisaged the construction of a functional structured editor, capable of
                    being used on a daily basis, that makes better use of these advantages.},
    type          = {Graduation Thesis submitted to the Computer Science Department},
    institution   = {Federal University of Santa Catarina},
    location      = {Florianópolis, Santa Catarina, Brazil},
    note          = {[Departamento de Informática e Estatística]},
    url           = {https://tcc.inf.ufsc.br},
    urlaccessdate = {2017-03-01},
}


@thesis{codeComprehensionComparedToOO,
    title         = {An empirical study on code comprehension DCI compared to OO},
    author        = {Héctor Adrián Valdecantos},
    journal       = {Thesis submitted for degree of Master of Science in Software Engineering},
    year          = {2016},
    month         = {08},
    publisher     = {Online},
    address       = {Research Gate},
    abstract      = {Comprehension of source code affects software development, especially its
                    maintenance where reading code is the most time consuming performed activity. A
                    programming paradigm imposes a style of arranging the source code that is
                    aligned with a way of thinking toward a computable solution. Then, a programming
                    paradigm with a programming language represents an important factor for source
                    code comprehension. Object-Oriented (OO) is the dominant paradigm today.
                    Although, it was criticized from its beginning and recently an alternative has
                    been proposed. In an OO source code, system functions cannot escape outside the
                    definition of classes and their descriptions live inside multiple class
                    declarations. This results in an obfuscated code, a lost sense the run-time, and
                    in a lack of global knowledge that weaken the understandability of the source
                    code at system level. A new paradigm is emerging to address these and other OO
                    issues, this is the Data Context Interaction (DCI) paradigm. We conducted the
                    first human subject related controlled experiment to evaluate the effects of DCI
                    on code comprehension compared to OO. We looked for correctness, time
                    consumption, and focus of attention during comprehension tasks. We also present
                    a novel approach using metrics from Social Network Analysis to analyze what we
                    call the Cognitive Network of Language Elements (CNLE) that is built by
                    programmers while comprehending a system. We consider this approach useful to
                    understand source code properties uncovered from code reading cognitive tasks.
                    The results obtained are preliminary in nature but indicate that DCI approach
                    produces more comprehensible source code and promotes a stronger focus the
                    attention in important files when programmers are reading code during program
                    comprehension. Regarding reading time spent on files, we were not able to
                    indicate with statistical significance which approach allows programmers to
                    consume less time.},
    location      = {National University of Tucuman, Tucumán Province, Argentina},
    doi           = {10.13140/RG.2.2.29331.48169},
    url           = {https://www.researchgate.net/publication/308314679_An_empirical_study_on_code_comprehension_DCI_compared_to_OO},
    urlaccessdate = {2017-11-02},
}


@thesis{enhancingLegacySoftwareSystemAnalysis,
    title         = {Enhancing legacy software system analysis by combining behavioural and semantic information sources},
    author        = {David Cutting},
    journal       = {Thesis submitted for degree of Doctor of Philosophy},
    year          = {2016},
    month         = {11},
    publisher     = {Online},
    address       = {Research Gate},
    abstract      = {Computer software is, by its very nature highly complex and invisible yet
                    subject to a near-continual pressure to change. Over time the development
                    process has become more mature and less risky. This is in large part due to the
                    concept of software traceability; the ability to relate software components back
                    to their initial requirements and between each other. Such traceability aids
                    tasks such as maintenance by facilitating the prediction of “ripple effects”
                    that may result, and aiding comprehension of software structures in general.
                    Many organisations, however, have large amounts of software for which little or
                    no documentation exists; the original developers are no longer available and yet
                    this software still underpins critical functions. Such “legacy software” can
                    therefore represent a high risk when changes are required. Consequently, large
                    amounts of effort go into attempting to comprehend and understand legacy
                    software. The most common way to accomplish this, given that reading the code
                    directly is hugely time consuming and near-impossible, is to reverse engineer
                    the code, usually to a form of representative projection such as a UML class
                    diagram. Although a wide number of tools and approaches exist, there is no
                    empirical way to compare them or validate new developments. Consequently there
                    was an identified need to define and create the Reverse Engineering to Design
                    Benchmark (RED-BM). This was then applied to a number of industrial tools. The
                    measured performance of these tools varies from 8.8\% to 100\%, demonstrating
                    both the effectiveness of the benchmark and the questionable performance of
                    several tools. In addition to the structural relationships detectable through
                    static reverse engineering, other sources of information are available with the
                    potential to reveal other types of relationships such as semantic links. One
                    such source is the mining of source code repositories which can be analysed to
                    find components within a software system that have, historically, commonly been
                    changed together during the evolution of the system and from the strength of
                    that infer a semantic link. An approach was implemented to mine such semantic
                    relationships from repositories and relationships were found beyond those
                    expressed by static reverse engineering. These included groups of relationships
                    potentially suitable for clustering. To allow for the general use of multiple
                    information sources to build traceability links between software components a
                    uniform approach was defined and illustrated. This includes rules and formulas
                    to allow combination of sources. The uniform approach was implemented in the
                    field of predictive change impact analysis using reverse engineering and
                    repository mining as information sources. This implementation, the Java Code
                    Relationship Anlaysis (jcRA) package, was then evaluated against an industry
                    standard tool, JRipples. Depending on the target, the combined approach is able
                    to outperform JRipples in detecting potential impacts with the risk of
                    over-matching (a high number of false-positives and overall class coverage on
                    some targets).},
    location      = {Queen's University Belfast, Belfast, Northern Ireland},
    doi           = {10.13140/RG.2.2.21231.64160},
    url           = {https://www.researchgate.net/publication/311289366_Enhancing_legacy_software_system_analysis_by_combining_behavioural_and_semantic_information_sources},
    urlaccessdate = {2017-11-02},
}


@book{ahoCompilerDragonBook,
    author = {Aho, Alfred V. and Lam, Monica S. and Sethi, Ravi and Ullman,
    Jeffrey D.},
    title = {Compilers: Principles, Techniques, and Tools (2Nd Edition)},
    year = {2006},
    isbn = {0321486811},
    abstract = {Compilers: Principles, Techniques and Tools, known to
    professors, students, and developers worldwide as the "Dragon Book," is
    available in a new edition.  Every chapter has been completely revised to
    reflect developments in software engineering, programming languages, and
    computer architecture that have occurred since 1986, when the last edition
    published.  The authors, recognizing that few readers will ever go on to
    construct a compiler, retain their focus on the broader set of problems
    faced in software design and software development.},
    publisher = {Addison-Wesley Longman Publishing Co., Inc.},
    address = {Boston, MA, USA},
}


@inbook{introductionToContextFreeGrammars,
    author="Paul, Wolfgang J.
    and Baumann, Christoph
    and Lutsyk, Petro
    and Schmaltz, Sabine",
    title="Context-Free Grammars",
    bookTitle="System Architecture: An Ordinary Engineering Discipline",
    year="2016",
    publisher="Springer International Publishing",
    address="Cham",
    pages="159--178",
    abstract="In this chapter the authors introduce context-free grammars, and they explain grammars for expressions.",
    isbn="978-3-319-43065-2",
    doi="10.1007/978-3-319-43065-2_10",
    url="https://doi.org/10.1007/978-3-319-43065-2_10"
}


@inbook{sipserBook,
    author = {Sipser, Michael},
    title = {Introduction to the Theory of Computation},
    isbn = {978-1133187790},
    year = {2012},
    url = {http://doi.acm.org/10.1145/230514.571645},
    doi = {10.1145/230514.571645},
    publisher={Pearson Education},
    address = {New York, NY, USA},
    abstract = {Gain a clear understanding of even the most complex, highly
    theoretical computational theory topics in the approachable presentation found
    only in the market-leading INTRODUCTION TO THE THEORY OF COMPUTATION, 3E. The
    number one choice for today's computational theory course, this revision
    continues the book's well-know, approachable style with timely revisions,
    additional practice, and more memorable examples in key areas. A new
    first-of-its-kind theoretical treatment of deterministic context-free languages
    is ideal for a better },
}


@inbook{hopcroftBook,
    author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
    title={Introduction to Automata Theory, Languages, and Computation},
    year = {2001},
    isbn={978-0321455369},
    numpages = {6},
    url = {http://doi.acm.org/10.1145/568438.568455},
    doi = {10.1145/568438.568455},
    year={2006},
    publisher={Pearson Education},
    address = {New York, NY, USA},
    abstract = {This classic book on formal languages, automata theory, and
    computational complexity has been updated to present theoretical concepts in
    a concise and straightforward manner with the increase of hands-on,
    practical applications. This new edition comes with Gradiance, an online
    assessment tool developed for computer science. Gradiance is the most
    advanced online assessment tool developed for the computer science
    discipline. With its innovative underlying technology, Gradiance turns basic
    homework assignments and programming labs into an interactive learning },
}


@inbook{gettingProductive,
    title         = {Getting Productive},
    booktitle     = {Professional Git},
    author        = {Laster, Brent},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {11},
    pages         = {73--97},
    abstract      = {This chapter explains how people can work with help in Git, understand the
                    multiple repositories model, and stage files. Git includes two different forms
                    of help: an abbreviated version and a full version. Partial and interactive
                    staging, committing files into the local repository, and writing good commit
                    messages are also discussed. The chapter also elucidates concepts such as SHA1,
                    options for staging files, and forming good commit messages. Although working
                    with multiple repositories at the same time is common in Git, it is a different
                    way of working for most people. Git commit includes a --verbose option. The
                    first time this option is used on the command line, it results in the diff
                    output between the staging area and the local repository being included. The
                    chapter also talks about ways to amend commits and use some advanced techniques
                    such as commit message template files to improve commit messages.},
    keywords      = {commit message, interactive staging, multiple repositories model, partial staging, SHA1, staging area},
    location      = {Cary, North Carolina, USA},
    doi           = {10.1002/9781119285021.ch5},
    isbn          = {9781119285021},
    url           = {https://www.researchgate.net/publication/311666005_Getting_Productive},
    urlaccessdate = {2017-11-02},
}


@inbook{trackingChanges,
    title         = {Tracking Changes},
    booktitle     = {Professional Git},
    author        = {Laster, Brent},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {11},
    pages         = {105-125},
    abstract      = {This chapter considers different versions of files at the different levels in
                    Git to keep track of where everything is and how the versions at the different
                    levels may differ from each other. It explains ways to keep track of all of the
                    work that's in progress. Git has two commands that can help with this: status
                    and diff. Using these two commands users can quickly understand the state of
                    their changes in the local environment and ensure that the correct changes are
                    tracked and stored in Git. For files that are in the working directory or
                    staging area, the status command answers three questions: whether or not a file
                    is tracked, what is in the staging area, and whether or not a file is modified.
                    Git can report the status of untracked files in a couple of different ways,
                    depending on whether or not something is staged.},
    location      = {Cary, North Carolina, USA},
    doi           = {10.1002/9781119285021.ch6},
    isbn          = {9781119285021},
    url           = {https://www.researchgate.net/publication/311666333_Tracking_Changes},
    urlaccessdate = {2017-11-06},
}


@inbook{commandLineInterface,
    title         = {The Command Line Interface},
    author        = {Adam B. Singer},
    booktitle     = {Practical C++ Design},
    year          = {2017},
    abstract      = {This is a very exciting chapter. While command line interfaces (CLIs) may not
                    have the cachet of modern graphical user interfaces (GUIs), especially those of
                    phones or tablets, the CLI is still a remarkably useful and effective user
                    interface. This chapter details the design and implementation of the command
                    line interface for pdCalc. By the end of this chapter, we will, for the first
                    time, have a functioning (albeit feature incomplete) calculator, which is a
                    significant milestone in our development.},
    pages         = {97-113},
    edition       = {1},
    publisher     = {Apress},
    address       = {Berkeley, CA},
    isbn          = {978-1-4842-3056-5},
    doi           = {10.1007/978-1-4842-3057-2\_5},
    location      = {Online},
    url           = {https://www.researchgate.net/publication/320120365_The_Command_Line_Interface},
    urlaccessdate = {2017-10-10},
}


@inbook{automaticSynthesis,
    title         = {Towards Automatic Synthesis of Software Verification Tools},
    author        = {Rybalchenko, Andrey},
    editor        = {Donaldson, Alastair and Parker, David},
    bookTitle     = {Model Checking Software: 19th International Workshop, SPIN 2012, Oxford, UK, July 23-24, 2012. Proceedings},
    year          = {2012},
    volume        = {7385},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {22--22},
    abstract      = {Software complexity is growing, so is the demand for software verification. Soon,
                    perhaps within a decade, wide deployment of software verification tools will be
                    indispensable or even mandatory to ensure software reliability in a large number of
                    application domains, including but not restricted to safety and security critical
                    systems. To adequately respond to the demand we need to eliminate tedious aspects of
                    software verifier development, while providing support for the accomplishment of
                    creative aspects.},
    location      = {Technische Universität München, Germany},
    doi           = {10.1007/978-3-642-31759-0\_3},
    isbn          = {978-3-642-31759-0},
    url           = {https://link.springer.com/chapter/10.1007/978-3-642-31759-0_3},
    urlaccessdate = {2017-10-30},
}


@inbook{debuggingIntoExamples,
    title         = {Debugging into Examples},
    author        = {Steinert, Bastian and Perscheid, Michael and Beck, Martin and Lincke, Jens and Hirschfeld, Robert},
    bookTitle     = {Testing of Software and Communication Systems: 21st IFIP WG 6.1 International Conference , November 2-4, 2009. Proceedings},
    year          = {2009},
    pages         = {235--240},
    publisher     = {Springer},
    address       = {Berlin, Heidelberg},
    abstract      = {Enhancing and maintaining a complex software system requires detailed
                    understanding of the underlying source code. Gaining this understanding by
                    reading source code is difficult. Since software systems are inherently dynamic,
                    it is complex and time consuming to imagine, for example, the effects of a
                    method's source code at run-time. The inspection of software systems during
                    execution, as encouraged by debugging tools, contributes to source code
                    comprehension. Leveraged by test cases as entry points, we want to make it easy
                    for developers to experience selected execution paths in their code by debugging
                    into examples. We show how links between test cases and application code can be
                    established by means of dynamic analysis while executing regular tests.},
    location      = {Eindhoven, The Netherlands},
    isbn          = {978-3-642-05031-2},
    doi           = {10.1007/978-3-642-05031-2_18},
    url           = {https://www.researchgate.net/publication/221047094_Debugging_into_Examples},
    urlaccessdate = {2017-10-31},
}


@inbook{aPrettyGoodFormatting,
    title         = {A Pretty Good Formatting Pipeline},
    author        = {Bagge, Anya Helee and Hasu, Tero},
    editor        = {Erwig, Martn and Paige, Richard F. and Van Wyk, Eric},
    bookTitle     = {Software Language Engineering: 6th International Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. Proceedings},
    year          = {2013},
    month         = {10},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {177--196},
    abstract      = {Proper formatting makes the structure of a program apparent and aids program
                    comprehension. The need to format code arises in code generation and
                    transformation, as well as in normal reading and editing situations. Commonly
                    used pretty-printing tools in transformation frameworks provide an easy way to
                    produce indented code that is fairly readable for humans, without reaching the
                    level of purpose-built reformatting tools, such as those built into IDEs. This
                    paper presents a library of pluggable components, built to support style-based
                    formatting and reformatting of code, and to enable further experimentation with
                    code formatting.},
    location      = {Bergen Language Design Laboratory, Dept. of Informatics, University of Bergen, Norway},
    isbn          = {978-3-319-02654-1},
    doi           = {10.1007/978-3-319-02654-1_10},
    url           = {https://www.researchgate.net/publication/300351526_A_Pretty_Good_Formatting_Pipeline},
    urlaccessdate = {2017-10-31},
}


@inbook{learningSupportSystem,
    title         = {Development of a Learning Support System for Source Code Reading Comprehension},
    author        = {Arai, Tatsuya and Kanamori, Haruki and Tomoto, Takahito and Kometani, Yusuke and Akakura, Takako"},
    editor        = {Yamamoto, Sakae},
    bookTitle     = {Human Interface and the Management of Information, 16th International Conference},
    year          = {2014},
    month         = {06},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {12--19},
    abstract      = {In this paper, we describe the development of a support system that facilitates
                    the process of learning computer programming through the reading of computer
                    program source code. Reading code consists of two steps: reading comprehension
                    and meaning deduction. In this study, we developed a tool that supports the
                    comprehension of a program's reading. The tool is equipped with an error
                    visualization function that illustrates a learner's mistakes and makes them
                    aware of their errors. We conducted experiments using the learning support tool
                    and confirmed that the system is effective.},
    location      = {Heraklion, Crete, Greece},
    isbn          = {978-3-319-07863-2},
    doi           = {10.1007/978-3-319-07863-2_2},
    url           = {https://www.researchgate.net/publication/295290682_Development_of_a_Learning_Support_System_for_Source_Code_Reading_Comprehension},
    urlaccessdate = {2017-11-01},
}


@inbook{howNovicesRead,
    title         = {How Novices Read Source Code in Introductory Courses on Programming: An Eye-Tracking Experiment},
    author        = {Yenigalla, Leelakrishna and Sinha, Vinayak and Sharif, Bonita and Crosby, Martha},
    editor        = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
    bookTitle     = {Foundations of Augmented Cognition: Neuroergonomics and Operational Neuroscience: 10th International Conference, Proceedings, Part II},
    year          = {2016},
    month         = {07},
    publisher     = {Springer-Verlag},
    address       = {New York, NY, USA},
    pages         = {120--131},
    abstract      = {We present an empirical study using eye tracking equipment to understand how
                    novices read source code in the context of two introductory programming classes.
                    Our main goal is to begin to understand how novices read source code and to
                    determine if we see any improvement in program comprehension as the course
                    progresses. The results indicate that novices put in more effort and had more
                    difficulty reading source code as they progress through the course. However,
                    they are able to partially comprehend code at a later point in the course. The
                    relationship between fixation counts and durations is linear but shows more
                    clusters later in the course, indicating groups of students that learned at the
                    same pace. The results also show that we did not see any significant shift in
                    learning (indicated by the eye tracking metrics) during the course, indicating
                    that there might be more than one course that needs to be taken over the course
                    of a few years to realize the significance of the shift. We call for more
                    studies over a student's undergraduate years to further learn about this
                    shift.},
    location      = {Toronto, ON, Canada},
    keywords      = {Eye tracking study, Novices, Program comprehension},
    isbn          = {978-3-319-39952-2},
    doi           = {10.1007/978-3-319-39952-2_13},
    url           = {https://www.researchgate.net/publication/304190149_How_Novices_Read_Source_Code_in_Introductory_Courses_on_Programming_An_Eye-Tracking_Experiment},
    urlaccessdate = {2017-11-02},
}


@inbook{usingSourceControl,
    title         = {Using Source Control},
    author        = {Yener, Murat and Dundar, Onur},
    booktitle     = {Expert Android Studio},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {10},
    pages         = {245--279},
    abstract      = {Git is currently the most widely accepted source control system among Android
                    developers, and has built-in support for Android Studio. This chapter covers how
                    to create a Git repository, add files to it, and perform commits. Unlike other
                    systems that just watch changes in the file system and commit changes to the
                    version control server, Git runs on a client computer and changes need to be
                    committed locally first. This way, a developer can revert any change/branch or
                    version locally through Git. The local Git can push the set of changes committed
                    to network Git servers. Android Studio comes with Git support. However, one may
                    still need to install Git to be able to use it through the command line. GitHub
                    is a popular Git-based project-hosting site that offers free hosting for public
                    repositories. One reason for GitHub's popularity is the available easy-to-use
                    tools for Git.},
    location      = {San Jose, California},
    keywords      = {Android developers, Android Studio, Git repository, GitHub, source control system},
    doi           = {10.1002/9781119419310.ch9},
    isbn          = {9781119419310},
    url           = {https://www.researchgate.net/publication/316657029_Using_Source_Control},
    urlaccessdate = {2017-11-03},
}


@inbook{continuousIntegration,
    title         = {Continuous Integration},
    author        = {Yener, Murat and Dundar, Onur},
    booktitle     = {Expert Android Studio},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {10},
    pages         = {281--307},
    abstract      = {This chapter focuses on continuous integration (CI) servers that act as the
                    cement between all other processes and convert them into an automated life
                    cycle. It explains how to download and install CI server. The chapter explores
                    how to set up a build job from a Git repository, how to trigger a build cycle on
                    every commit, and how to publish one's app automatically to Google Play. CI
                    servers are very flexible and easy to integrate and can handle Android projects
                    that use make files, Maven, or Gradle. One needs to choose one of those to fully
                    integrate his/her project with a CI server. Version control systems are another
                    crucial part of the CI process. Each code commit triggers a build process that
                    results in compilation, running tests, and packaging the app on the CI server.
                    The chapter also focuses on installing Jenkins.},
    location      = {San Jose, California},
    keywords      = {Android developers, Android Studio, Git repository, GitHub, source control system},
    doi           = {10.1002/9781119419310.ch9},
    isbn          = {9781119419310},
    url           = {https://www.researchgate.net/publication/316657029_Using_Source_Control},
    urlaccessdate = {2017-11-03},
}


@online{Atwood,
    title         = {Death to the Space Infidels!},
    author        = {Jeff Atwood},
    abstract      = {Ah, spring. What a wonderful time of year. A time when young programmers' minds
                    turn to thoughts of ... never ending last-man-standing filibuster arguments
                    about code formatting. Naturally. And there is no argument more evergreen than
                    the timeless debate between tabs and spaces.},
    year          = {2009},
    location      = {Online},
    url           = {http://www.codinghorror.com/blog/2009/04/death-to-the-space-infidels.html},
    urlaccessdate = {2017-03-01},
}


@online{Geukens,
    title         = {Is imposing the same code format for all developers a good idea?},
    author        = {Stijn Geukens},
    abstract      = {We are considering to impose a single standard code format in our project (auto
                    format with save actions in Eclipse). The reason is that currently there is a
                    big difference in the code formats used by several (>10) developers which makes
                    it harder for one developer to work on the code of another developer. The same
                    Java file sometimes uses 3 different formats.},
    year          = {2013},
    location      = {Online},
    url           = {https://softwareengineering.stackexchange.com/questions/189274/is-imposing-the-same-code-format-for-all-developers-a-good-idea},
    urlaccessdate = {2017-03-01},
}


@online{Schweitzer,
    title         = {Powerful code indenter front-end, UniversalIndentGUI},
    author        = {Thomas Schweitzer},
    publisher     = {Online Material; \url{http://universalindent.sourceforge.net/index.php}},
    year          = {2012},
    abstract      = {Ever concerned about how your code looks like? Ever heard of different
                    indenting styles, for example K\&R? Ever received code from someone else who
                    didn't care about code formatting? Ever tried to configure a code indenter to
                    convert such code to your coding style? Ever got bored by that tedious "changing
                    a parameter"-"call the indeter"-"try and error" procedure? Help is close to you.
                    UniversalIndentGUI offers a live preview for setting the parameters of nearly
                    any indenter. You change the value of a parameter and directly see how your
                    reformatted code will look like. Save your beauty looking code or create an
                    anywhere usable batch/shell script to reformat whole directories or just one
                    file even out of the editor of your choice that supports external tool calls.},
    location      = {Online},
    url           = {https://github.com/danblakemore/universal-indent-gui},
    urlaccessdate = {2017-03-01},
}


@online{Skinner,
    title         = {SUBLIME TEXT 3 DOCUMENTATION, Syntax Definitions},
    author        = {Jon Skinner},
    year          = {2016},
    abstract      = {Sublime Text can use both .sublime-syntax and .tmLanguage files for syntax
                    highlighting. This document describes .sublime-syntax files. Sublime Syntax
                    files are YAML files with a small header, followed by a list of contexts. Each
                    context has a list of patterns that describe how to highlight text in that
                    context, and how to change the current text.},
    location      = {Online},
    url           = {https://www.sublimetext.com/docs/3/syntax.html},
    urlaccessdate = {2017-03-01},
}


@online{prettyPrinter,
    title         = {prettyprinter.de},
    author        = {J.M.},
    year          = {2017},
    abstract      = {This is a source code beautifier (source code formatter), similiar to indent.
                    Please make a backup before you replace your code!},
    location      = {Online},
    url           = {http://prettyprinter.de/},
    urlaccessdate = {2017-03-01},
}


@online{tabsAndSpacesConversion,
    title         = {How to replace spaces with tabs when pasting on a view},
    author        = {Evandro Coan},
    year          = {2017},
    abstract      = {The problem is that I will certainly not notice when I paste something indented
                    with spaces instead of tabs. This is problem because for some file types as
                    .sublime-settings files (or a Makefile), which has the setting
                    translate\_tabs\_to\_spaces set to false, so I would expect to all
                    .sublime-settings files to be indented with tabs, not spaces.},
    location      = {Online},
    url           = {https://forum.sublimetext.com/t/how-to-replace-spaces-with-tabs-when-pasting-on-a-view-with-translate-tabs-to-spaces-set-to-false/32193},
    urlaccessdate = {2017-09-29},
}


@online{synchronizingFolders,
    title         = {Synchronizing folders with rsync},
    author        = {Juan Valencia Escalante},
    year          = {2017},
    abstract      = {In this post I cover the basics of rsync, in preparation for a subsequent post
                    that will cover backups and it's use in conjunction with cronjobs to automatize
                    the backup process. From the copying and synchronization of local files and
                    folders, to it's use for transfer information among computers. Its use as a
                    daemon when SSH is unavailable was moved to it's own section.},
    location      = {Online},
    url           = {http://www.jveweb.net/en/archives/2010/11/synchronizing-folders-with-rsync.html},
    urlaccessdate = {2017-10-10},
}


@INPROCEEDINGS{visualizationsInAFunctionalProgramming,
    author={J. A. {Velazquez-Iturbide} and A. {Presa-Vazquez}},
    booktitle={FIE'99 Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing the Future of Science and Engineering Education. Conference Proceedings (IEEE Cat. No.99CH37011},
    title={Customization of visualizations in a functional programming environment},
    year={1999},
    volume={2},
    number={},
    pages={12B3/22-12B3/28 vol.2},
    abstract={CS first-year students expect the user interface of programming
    environments to be similar to that of common PC applications. A natural
    evolution of educational programming environments consists in incorporating
    many of their user-friendly facilities. The authors concentrate in this
    paper on the facilities that WinHIPE, an environment for functional
    programming, provides to students for customizing the visualization of
    expressions. Expressions can be either pretty-printed as text or displayed
    graphically, showing drawings of lists and binary trees. Besides, fonts,
    sizes, colors, lines and distances are parameters that can be customized for
    any visualization. Finally, the visualization of large expressions can be
    simplified to show only their most relevant parts. Students obtain several
    benefits from customization facilities. They feel more comfortable with the
    programming environment WinHIPE, because they can develop more readable
    programs written "in their personal style". Students can also experiment at
    small effort with different formats, becoming profident in style issues.
    Finally customization facilities allow making clear in a course on
    programming languages the relevant role of visualization in programming
    tools and their relative independence from language syntax.},
    keywords={computer science education;educational courses;functional
    programming;programming environments;user interfaces;functional programming
    environment;visualisations customisation;first-year computer science
    students;PC applications;user interface;educational programming
    environments;WinHIPE;programming languages course;programming
    tools;Visualization;Functional programming;Programming environments;User
    interfaces;Programming profession;Application software;Binary trees;Computer
    languages;Cognitive science;Computer errors},
    doi={10.1109/FIE.1999.841580},
    ISSN={0190-5848},
    month={Nov},
}


@INPROCEEDINGS{transformationForDomainSpecificOptimisation,
    author={O. S. {Bagge} and K. T. {Kalleberg} and M. {Haveraaen} and E. {Visser}},
    booktitle={Proceedings Third IEEE International Workshop on Source Code Analysis and Manipulation},
    title={Design of the CodeBoost transformation system for domain-specific optimisation of C++ programs},
    year={2003},
    volume={},
    number={},
    pages={65-74},
    abstract={The use of a high-level, abstract coding style can greatly
    increase developer productivity. For numerical software, this can result in
    drastically reduced run-time performance. High-level, domain-specific
    optimisations can eliminate much of the overhead caused by an abstract
    coding style, but current compilers have poor support for domain-specific
    optimisation. We present CodeBoost, a source-to-source transformation tool
    for domain-specific optimisation of C++ programs. CodeBoost performs
    parsing, semantic analysis and pretty-printing, and transformations can be
    implemented either in the Stratego program transformation language, or as
    user-defined rewrite rules embedded within the C++ program. CodeBoost has
    been used with great success to optimise numerical applications written in
    the Sophus high-level coding style. We discuss the overall design of the
    CodeBoost transformation framework, and take a closer look at two important
    features of CodeBoost: user-defined rules and totem annotations. We also
    show briefly how CodeBoost is used to optimise Sophus code, resulting in
    applications that run twice as fast, or more.},
    keywords={optimising compilers;C++ language;software tools;abstract coding
    style;domain-specific optimization;program compiler;source-to-source
    transformation tool;C++ program optimisation;parsing;semantic
    analysis;Stratego program transformation language;user-defined rewrite
    rules;CodeBoost transformation framework;totem annotation;Sophus code;Design
    optimization;Runtime;Optimizing compilers;Informatics;Productivity;Software
    performance;Performance analysis;Bridges;Software engineering;Software
    libraries},
    doi={10.1109/SCAM.2003.1238032},
    ISSN={},
    month={Sep.},
}


@INPROCEEDINGS{improvingRefactoringSpeed,
    author={J. {Kim} and D. {Batory} and D. {Dig} and M. {Azanza}},
    booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)},
    title={Improving Refactoring Speed by 10X},
    year={2016},
    volume={},
    number={},
    pages={1145-1156},
    abstract={Refactoring engines are standard tools in today's Integrated
    Development Environments (IDEs). They allow programmers to perform one
    refactoring at a time, but programmers need more. Most design patterns in
    the Gang-of-Four text can be written as a refactoring script - a
    programmatic sequence of refactorings. In this paper, we present R3, a new
    Java refactoring engine that supports refactoring scripts. It builds a
    main-memory, non-persistent database to encode Java entity declarations
    (e.g., packages, classes, methods), their containment relationships, and
    language features such as inheritance and modifiers. Unlike classical
    refactoring engines that modify Abstract Syntax Trees (ASTs), R3
    refactorings modify only the database; refactored code is produced only when
    pretty-printing ASTs that reference database changes. R3 performs comparable
    precondition checks to those of the Eclipse Java Development Tools (JDT) but
    R3's codebase is about half the size of the JDT refactoring engine and runs
    an order of magnitude faster. Further, a user study shows that R3 improved
    the success rate of retrofitting design patterns by 25\% up to 50\%.},
    keywords={Java;software maintenance;refactoring speed;integrated development
    environment;IDE;Gang-of-Four text;refactoring script;refactoring
    sequence;Java refactoring engine;Java entity declarations;abstract syntax
    trees;AST;Eclipse Java development tools;JDT;retrofitting design
    patterns;Java;Engines;Databases;Graphics;Computer bugs;Graphical user
    interfaces;Maintenance engineering},
    doi={10.1145/2884781.2884802},
    ISSN={1558-1225},
    month={May},
}


@INPROCEEDINGS{prettyPrintingOfVisualSentences,
    author={T. B. {Dinesh} and S. M. {Uskudarh}},
    booktitle={Proceedings. 1997 IEEE Symposium on Visual Languages (Cat. No.97TB100180)},
    title={Pretty-printing of visual sentences},
    year={1997},
    volume={},
    number={},
    pages={242-243},
    abstract={When input sentences are processed in some manner, say evaluated
    with some set of rules, the resulting sentence must be pretty-printed in
    order to be presented to the user. We introduce a technique called
    "Share-Where maintenance" which is used to preserve layout information by
    annotating abstract representations of visual sentences. The annotations in
    the abstract representation point to the presentation where sub-terms
    originated which were created either by the user (initial term) or by the
    language specifier (equations, as in introduced terms). Both kinds are
    visual presentations which are used for presenting the new term.},
    keywords={visual programming;grammars;visual languages;algebraic
    specification;formal specification;visual sentences;input
    sentences;pretty-printed;Share-Where maintenance;layout information;abstract
    representation annotation;language specifier;visual
    presentations;Equations;Labeling;Logic programming;Computer
    science;Usability;Information analysis;Calculus;Design
    engineering;Prototypes;Application software},
    doi={10.1109/VL.1997.626589},
    ISSN={1049-2615},
    month={Sep.},
}


@INPROCEEDINGS{prettyPrintingForSoftware,
    author={M. {de Jonge}},
    booktitle={International Conference on Software Maintenance, 2002. Proceedings.},
    title={Pretty-printing for software reengineering},
    year={2002},
    volume={},
    number={},
    pages={550-559},
    abstract={Automatic software reengineering changes or repairs existing
    software systems. They are usually tailor-made for a specific customer and
    language dependent. Maintaining similar reengineering for multiple customers
    and different language dialects may, therefore, soon become problematic
    unless advanced language technology is used. Generic pretty-printing is part
    of such technology and is the subject of this paper. We discuss specific
    pretty-print aspects of software reengineering such as fulfilling
    customer-specific format conventions, preserving existing layout, and
    producing multiple output formats. In addition, we describe pretty-print
    techniques that help to reduce maintenance effort of tailor-made
    reengineering supporting multiple language dialects. Applications such as
    COBOL reengineering and SDL documentation generation show that our
    techniques, implemented in the generic pretty-printer GPP, are feasible.},
    keywords={systems re-engineering;software maintenance;system
    documentation;COBOL;automatic software reengineering;generic
    pretty-printing;customer-specific format conventions;layout
    preservation;multiple output formats;maintenance;multiple language
    dialects;COBOL reengineering;SDL documentation
    generation;GPP;Documentation;Software systems;Network address
    translation;Application software;Time to
    market;Humans;Pipelines;Inspection;Software maintenance;Computer languages},
    doi={10.1109/ICSM.2002.1167816},
    ISSN={1063-6773},
    month={Oct},
}


@INPROCEEDINGS{translatorGenerationCompilier,
    author={ {Ai Hua Wu} and J. {Paquet}},
    booktitle={8th International Conference on Computer Supported Cooperative Work in Design},
    title={The translator generation in the general intensional programming compilier},
    year={2004},
    volume={2},
    number={},
    pages={668-672 Vol.2},
    abstract={General intensional programming compiler (GIPC) is one component
    of the general intensional programming system (GIPSY) that aims at the
    development of a programming system that would allow dynamic investigations
    on the possibilities of intensional programming and all its widely different
    flavors and domains of application. To cope with the constant evolution of
    intensional programming languages, we design the system in a very flexible
    infrastructure for the generation of compiler components upon the creation
    of a new version. This work focuses on one and most important component that
    is about the generation of the translator between generic and specific
    intensional programming languages.},
    keywords={program interpreters;parallel languages;program
    compilers;configuration management;translator generation;general intensional
    programming complier;GIPC;general intensional programming system;GIPSY
    system;compiler components;version creation;specific intensional programming
    languages;Computer languages;Dynamic programming;Program
    processors;Filters;Tensile stress;Genetic programming;Runtime
    environment;Pipelines;Multidimensional systems;Differential equations},
    doi={10.1109/CACWD.2004.1349274},
    ISSN={},
    month={May},
}


@inproceedings{documentingAndSharingKnowledge,
    author        = {Guzzi, Anja},
    title         = {Documenting and Sharing Knowledge About Code},
    booktitle     = {Proceedings of the 34th International Conference on Software Engineering},
    series        = {ICSE '12},
    year          = {2012},
    isbn          = {978-1-4673-1067-3},
    pages         = {1535--1538},
    numpages      = {4},
    publisher     = {IEEE Press},
    address       = {Piscataway, NJ, USA},
    abstract      = {Software engineers spend a considerable amount of time on program
                    comprehension. Current research has primarily focused on assisting the developer
                    trying to build up his understanding of the code. This knowledge remains only in
                    the mind of the developer and, as time elapses, often “disappears”. In this
                    research, we shift the focus to the developer who is using her Integrated
                    Development Environment (IDE) for writing, modifying, or reading the code, and
                    who actually understands the code she is working with. The objective of this PhD
                    research is to seek ways to support this developer to document and share her
                    knowledge with the rest of the team. In particular, we investigate the full
                    potential of micro-blogging integrated into the IDE for addressing the program
                    comprehension problem.},
    url           = {http://dl.acm.org/citation.cfm?id=2337223.2337476},
    location      = {Zurich, Switzerland},
    acmid         = {2337476},
    urlaccessdate = {2017-10-31},
}


@inproceedings{analysisOfCodeReading,
    title         = {Analysis of Code Reading to Gain More Insight in Program Comprehension},
    author        = {Busjahn, Teresa and Schulte, Carsten and Busjahn, Andreas},
    booktitle     = {Proceedings of the 11th Koli Calling International Conference on Computing Education Research},
    series        = {Koli Calling '11},
    year          = {2011},
    isbn          = {978-1-4503-1052-9},
    location      = {Koli, Finland},
    pages         = {1--9},
    numpages      = {9},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    abstract      = {Code reading, although an integral part of program comprehension, is rarely
                    reflected. In this paper, we want to argue for a research approach and direction
                    exploiting the potential that lies in the analysis of reading processes. Based
                    on the vast experience compiled in psychology and some studies in computing, eye
                    tracking and think aloud were elaborated as a viable research instrument for
                    code reading studies. We conducted a feasibility study, designed to examine the
                    actual process of code reading as the sensory starting point of comprehension.
                    Computational and statistical tools were developed to facilitate data capture
                    and analysis for eye tracking experiments. Results do not just provide proof of
                    concept but already emphasize differences between reading natural language text
                    and source code, as well as a distinct attention allocation within different
                    code elements like keywords and operators. In conclusion we suggest a
                    combination of theory-driven selected stimuli material, a carefully designed
                    procedure of eye tracking, complemented with suitable post-tests on
                    comprehension as well as retrospective think aloud in order to obtain additional
                    information on the linking process between perception and comprehension. As an
                    addition to other research approaches this should most certainly help us to
                    improve our knowledge of comprehension within an educational research
                    framework.},
    keywords      = {CS Ed research, code comprehension, code reading, educational research, eye tracking, program comprehension},
    doi           = {10.1145/2094131.2094133},
    acmid         = {2094133},
    location      = {Freie Universität Berlin, Germany},
    url           = {https://www.researchgate.net/publication/254004382_Analysis_of_code_reading_to_gain_more_insight_in_program_comprehension},
    urlaccessdate = {2017-10-31},
}


@inproceedings{howProgrammersRead,
    author        = {A. Jbara and D. G. Feitelson},
    booktitle     = {2015 IEEE 23rd International Conference on Program Comprehension},
    title         = {How Programmers Read Regular Code: A Controlled Experiment Using Eye Tracking},
    year          = {2015},
    month         = {05},
    pages         = {244-254},
    abstract      = {Regular code, which includes repetitions of the same basic pattern, has been
                    shown to have an effect on code comprehension: a regular function can be just as
                    easy to comprehend as an irregular one with the same functionality, despite
                    being longer and including more control constructs. It has been speculated that
                    this effect is due to leveraging the understanding of the first instances to
                    ease the understanding of repeated instances of the pattern. To verify and
                    quantify this effect, we use eye tracking to measure the time and effort spent
                    reading and understanding regular code. The results are that time and effort
                    invested in the initial code segments are indeed much larger than those spent on
                    the later ones, and the decay in effort can be modeled by an exponential or
                    cubic model. This shows that syntactic code complexity metrics (such as LOC and
                    MCC) need to be made context-sensitive, e.g. By giving reduced weight to
                    repeated segments according to their place in the sequence.},
    doi           = {10.1109/ICPC.2015.35},
    ISSN          = {1092-8138},
    keywords      = {ergonomics;gaze tracking;software metrics;source code (software);LOC;McCabe
                    cyclomatic complexity;cubic model;exponential model;eye tracking;lines of
                    code;read regular code;regular function;source code complexity;syntactic code
                    complexity metrics;Complexity theory;Correlation;Diamonds;Time
                    measurement;Tracking;Visualization;Code complexity metrics;Code
                    regularity;Controlled experiment;Eye tracking},
    location      = {Florence, Italy},
    url           = {https://www.researchgate.net/publication/281579264_How_Programmers_Read_Regular_Code_A_Controlled_Experiment_Using_Eye_Tracking},
    urlaccessdate = {2017-10-31},
}


@inproceedings{improvingCodeReadability,
    title         = {Improving code readability models with textual features},
    author        = {S. Scalabrino and M. Linares-Vásquez and D. Poshyvanyk and R. Oliveto},
    booktitle     = {2016 IEEE 24th International Conference on Program Comprehension (ICPC)},
    year          = {2016},
    month         = {05},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    pages         = {1-10},
    abstract      = {Code reading is one of the most frequent activities in software maintenance;
                    before implementing changes, it is necessary to fully understand source code
                    often written by other developers. Thus, readability is a crucial aspect of
                    source code that may significantly influence program comprehension effort. In
                    general, models used to estimate software readability take into account only
                    structural aspects of source code, e.g., line length and a number of comments.
                    However, source code is a particular form of text; therefore, a code readability
                    model should not ignore the textual aspects of source code encapsulated in
                    identifiers and comments. In this paper, we propose a set of textual features
                    aimed at measuring code readability. We evaluated the proposed textual features
                    on 600 code snippets manually evaluated (in terms of readability) by 5K+ people.
                    The results demonstrate that the proposed features complement classic structural
                    features when predicting code readability judgments. Consequently, a code
                    readability model based on a richer set of features, including the ones proposed
                    in this paper, achieves a significantly higher accuracy as compared to all of
                    the state-of-the-art readability models.},
    keywords      = {software maintenance;source code (software);text analysis;code readability
                    models;code snippets;line length;program comprehension effort;software
                    maintenance;software readability;source code;textual features;Computational
                    modeling;Feature extraction;Semantics;Software
                    quality;Syntactics;Visualization},
    location      = {Austin, TX, USA},
    isbn          = {978-1-5090-1428-6},
    doi           = {10.1109/ICPC.2016.7503707},
    url           = {https://www.researchgate.net/publication/301685380_Improving_Code_Readability_Models_with_Textual_Features},
    urlaccessdate = {2017-11-01},
}


@inproceedings{moldableCodeEditor,
    title         = {Towards a Live, Moldable Code Editor},
    booktitle     = {Companion to the First International Conference on the Art, Science and Engineering of Programming},
    series        = {Programming '17},
    author        = {Syrel, Aliaksei},
    year          = {2017},
    month         = {04},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    pages         = {43:1--43:3},
    articleno     = {43},
    numpages      = {3},
    abstract      = {Creating and evolving object-oriented applications requires developers to
                    reason about source code and run-time state. Integrated development environments
                    (IDEs) are tools that support developers in this activity. Many mainstream IDEs,
                    however, focus on code editors that promote reading of static text even in the
                    debugger. This affects program comprehension, as developers have to manually
                    link the static code with the run-time objects. In this work we explore how to
                    address this problem through a moldable code editor that enables developers to
                    select executable snippets of code and replace them with graphical views of the
                    resulting objects. Each object can define multiple views that developers can
                    select. This way objects and code coexist in the same editor.},
    keywords      = {Code editors, Program comprehension, User interfaces},
    location      = {Brussels, Belgium},
    doi           = {10.1145/3079368.3079376},
    isbn          = {978-1-4503-4836-2},
    acmid         = {3079376},
    url           = {https://www.researchgate.net/publication/318873843_Towards_a_live_moldable_code_editor},
    urlaccessdate = {2017-11-02},
}


@inproceedings{quitDiffCalculating,
    title         = {Quit Diff: Calculating the Delta Between RDF Datasets Under Version Control},
    author        = {Arndt, Natanael and Radtke, Norman},
    booktitle     = {Proceedings of the 12th International Conference on Semantic Systems},
    series        = {SEMANTiCS 2016},
    year          = {2016},
    month         = {09},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    isbn          = {978-1-4503-4752-5},
    pages         = {185--188},
    numpages      = {4},
    abstract      = {Distributed actors working on a common RDF dataset regularly encounter the
                    issue to compare the status of one graph with another or generally to
                    synchronize copies of a dataset. A versioning system helps to synchronize the
                    copies of a dataset, combined with a difference calculation system it is also
                    possible to compare versions in a log and to determine, in which version a
                    certain statement was introduced or removed. In this demo we present Quit Diff1,
                    a tool to compare versions of a Git versioned quad store, while it is also
                    applicable to simple unversioned RDF datasets. We are following an approach to
                    abstract from differences on a syntactical level to differences on the level of
                    the RDF data model, while we leave further semantic interpretation on the schema
                    and instance level to specialized applications. Quit Diff can generate patches
                    in various output formats and can be directly integrated in the distributed
                    version control system Git which provides a foundation for a comprehensive
                    co-evolution work flow on RDF datasets.},
    location      = {Leipzig, Germany},
    doi           = {10.1145/2993318.2993349},
    acmid         = {2993349},
    url           = {https://www.researchgate.net/publication/309430151_Quit_Diff_Calculating_the_Delta_Between_RDF_Datasets_Under_Version_Control},
    urlaccessdate = {2017-11-03},
}


@article{knuthLrParser1965,
    title = {On the translation of languages from left to right},
    journal = {Information and Control},
    volume = {8},
    number = {6},
    pages = {607 - 639},
    year = {1965},
    issn = {0019-9958},
    doi = {https://doi.org/10.1016/S0019-9958(65)90426-2},
    url = {http://www.sciencedirect.com/science/article/pii/S0019995865904262},
    author = {Donald E. Knuth},
    abstract = {There has been much recent interest in languages whose grammar
    is sufficiently simple that an efficient left-to-right parsing algorithm can
    be mechanically produced from the grammar. In this paper, we define LR(k)
    grammars, which are perhaps the most general ones of this type, and they
    provide the basis for understanding all of the special tricks which have
    been used in the construction of parsing algorithms for languages with
    simple structure, e.g. algebraic languages. We give algorithms for deciding
    if a given grammar satisfies the LR(k) condition, for given k, and also give
    methods for generating recognizes for LR(k) grammars. It is shown that the
    problem of whether or not a grammar is LR(k) for some k is undecidable, and
    the paper concludes by establishing various connections between LR(k)
    grammars and deterministic languages. In particular, the LR(k) condition is
    a natural analogue, for grammars, of the deterministic condition, for
    languages.},
}


@ARTICLE{chomskyGrammars1956,
    author={N. {Chomsky}},
    journal={IRE Transactions on Information Theory},
    title={Three models for the description of language},
    year={1956},
    volume={2},
    number={3},
    pages={113-124},
    abstract={We investigate several conceptions of linguistic structure to
    determine whether or not they can provide simple and "revealing" grammars
    that generate all of the sentences of English and only these. We find that
    no finite-state Markov process that produces symbols with transition from
    state to state can serve as an English grammar. Furthermore, the particular
    subclass of such processes that producen-order statistical approximations to
    English do not come closer, with increasingn, to matching the output of an
    English grammar. We formalize-the notions of "phrase structure" and show
    that this gives us a method for describing language which is essentially
    more powerful, though still representable as a rather elementary type of
    finite-state process. Nevertheless, it is successful only when limited to a
    small subset of simple sentences. We study the formal properties of a set of
    grammatical transformations that carry sentences with phrase structure into
    new sentences with derived phrase structure, showing that transformational
    grammars are processes of the same elementary type as phrase-structure
    grammars; that the grammar of English is materially simplified if phrase
    structure description is limited to a kernel of simple sentences from which
    all other sentences are constructed by repeated transformations; and that
    this view of linguistic structure gives a certain insight into the use and
    understanding of language.},
    keywords={Languages;Markov processes;Natural
    languages;Testing;Laboratories;Markov processes;Impedance
    matching;Kernel;Research and development},
    doi={10.1109/TIT.1956.1056813},
    ISSN={0096-1000},
    month={Sep.},
}


@article{lalrDeRemer1982,
    author = {DeRemer, Frank and Pennello, Thomas},
    title = {Efficient Computation of LALR(1) Look-Ahead Sets},
    journal = {ACM Trans. Program. Lang. Syst.},
    issue_date = {Oct. 1982},
    volume = {4},
    number = {4},
    month = oct,
    year = {1982},
    issn = {0164-0925},
    pages = {615--649},
    numpages = {35},
    url = {http://doi.acm.org/10.1145/69622.357187},
    doi = {10.1145/69622.357187},
    acmid = {357187},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {Two relations that capture the essential structure of the
    problem of computing LALR(1) look-ahead sets are defined, and an efficient
    algorithm is presented to compute the sets in time linear in the size of the
    relations. In particular, for a PASCAL grammar, the algorithm performs fewer
    than 15 percent of the set unions performed by the popular compiler-compiler
    YACC. When a grammar is not LALR(1), the relations, represented explicitly,
    provide for printing useroriented error messages that specifically indicate
    how the look-ahead problem arose. In addition, certain loops in the digraphs
    induced by these relations indicate that the grammar is not LR(k) for any k.
    Finally, an oft-discovered and used but incorrect look-ahead set algorithm
    is similarly based on two other relations defined for the fwst time here.
    The formal presentation of this algorithm should help prevent its
    rediscovery.},
}


@article{beatty1982,
    author = {Beatty, John C.},
    title = {On the Relationship Between LL(1) and LR(1) Grammars},
    journal = {J. ACM},
    volume = {29},
    number = {4},
    month = {oct},
    year = {1982},
    issn = {0004-5411},
    pages = {1007--1022},
    numpages = {16},
    url = {http://doi.acm.org/10.1145/322344.322350},
    doi = {10.1145/322344.322350},
    acmid = {322350},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {It is shown that every p-reduced LL(1) grammar is LALR(1) and,
    as a corollary, that every A-free LL(1) grammar is SLR(1) A partial converse to
    this result is also demonstrated: If there is at most one marked rule m the
    basis of every state set in the canonical collection of sets of LR(k) items for
    a grammar G in which S =+> Sy impossible, then G is LL(k).},
}


@Article{generatingInterpretiveTranslators,
    author="Murphree, E. L.
    and Fenves, S. J.",
    title="A technique for generating interpretive translators for problem-oriented languages",
    journal="BIT Numerical Mathematics",
    year="1970",
    month="Sep",
    day="01",
    volume="10",
    number="3",
    pages="310--323",
    abstract="The paper presents a technique for generating translators for problem-oriented and other command- or data-oriented languages which can be interpretively executed. The system consists of: (a) a stored grammar or table, representing in a modified graph form the syntactically valid statements and the corresponding semantic actions; (b) a set of application procedures, written in a procedural language; (c) a universal translator, which performs reading, input conversion, matching input items against the grammar, and calling the procedures specified by the grammar. The generator consists of the translator and a specific grammar and set of procedures, which together convert the problem-oriented description of the source grammar into the table used by the translator for execution.",
    issn="1572-9125",
    doi="10.1007/BF01934200",
    url="https://doi.org/10.1007/BF01934200"
}


@ARTICLE{anAbstractPrettyPrinter,
    author={R. D. {Cameron}},
    journal={IEEE Software},
    title={An abstract pretty printer},
    year={1988},
    volume={5},
    number={6},
    pages={61-67},
    abstract={The author has distilled the basic operations of the pretty
    printer into an abstract pretty printer that uses procedural parameters to
    perform low-level printing actions. By encapsulating the algorithm in one
    place, all the pretty-printing utilities will use the same algorithm, and
    the algorithm itself can be changed easily. The author describes how the
    abstract pretty printer can be used for basic design, printing to files and
    screens, setting the cursor, identifying a node, formatting text, and
    lexical changes.<>},
    keywords={programming environments;utility programs;programming
    environments;abstract pretty printer;low-level printing;pretty-printing
    utilities;Printers;Printing;Reactive power;Algorithms},
    doi={10.1109/52.10004},
    ISSN={0740-7459},
    month={Nov},
}


@article{universalCodeFormatter,
    title         = {Towards a Universal Code Formatter through Machine Learning},
    author        = {Terence Parr and J.J. Vinju},
    month         = {10},
    year          = {2016},
    journal       = {SLE 2016 Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    abstract      = {There are many declarative frameworks that allow us to implement code
                    formatters relatively easily for any specific language, but constructing them is
                    cumbersome. The first problem is that “everybody” wants to format their code
                    differently, leading to either many formatter variants or a ridiculous number of
                    configuration options. Second, the size of each implementation scales with a
                    language’s grammar size, leading to hundreds of rules. In this paper, we solve
                    the formatter construction problem using a novel approach, one that
                    automatically derives formatters for any given language without intervention
                    from a language expert. We introduce a code formatter called CODEB UFF that uses
                    machine learning to abstract formatting rules from a representative corpus,
                    using a carefully designed feature set. Our experiments on Java, SQL, and ANTLR
                    grammars show that CODEB UFF is efficient, has excellent accuracy, and is
                    grammar invariant for a given language. It also generalizes to a 4th language
                    tested during manuscript preparation.},
    pages         = {137-151},
    doi           = {10.1145/2997364.2997383},
    isbn          = {978-1-4503-4447-0},
    location      = {Amsterdam, The Netherlands},
    url           = {https://www.researchgate.net/publication/309363024_Towards_a_universal_code_formatter_through_machine_learning},
    urlaccessdate = {2017-03-01},
}


@article{industrialApplication,
    title         = {An industrial application of context-sensitive formatting},
    author        = {M.G.J. van den Brand and A.T. Kooiker and N.P. Veerman and J.J. Vinju},
    year          = {2005},
    abstract      = {Automated formatting is an important technique for the software maintainer. It
                    is either applied separately to improve the readability of source code, or as
                    part of a source code transformation tool chain. In this paper we report on the
                    application of generic tools for constructing formatters. In an industrial
                    setting automated formatters need to be tailored to the requirements of the
                    customer. The (legacy) programming language or dialect and the corporate
                    formatting conventions are specific and non-negotiable. Can generic formatting
                    tools deal with such unexpected requirements? Driven by an industrial case of 78
                    thousand lines of Cobol code, several limitations in existing formatting
                    technology have been addressed. We improved its flexibility by replacing a
                    generative phase by a generic tool, and we added a little expressiveness to the
                    formatting backend. Most importantly, we employed a multi-stage formatting
                    architecture that can cope with any kind of formatting convention using more
                    computational power.},
    publisher     = {Online},
    url           = {https://www.researchgate.net/publication/228540036_An_industrial_application_of_context-sensitive_formatting},
    urlaccessdate = {2017-09-07},
}


@article{programIndentation,
    title         = {Program indentation and comprehensibility},
    author        = {Richard J. Miara and Joyce A. Musselman and Juan A. Navarro and Ben Shneiderman},
    month         = {11},
    year          = {1983},
    journal       = {Communications of the ACM},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    volume        = {26},
    issue         = {11},
    abstract      = {The consensus in the programming community is that indentation aids program
                    comprehension, although many studies do not back this up. We tested program
                    comprehension on a Pascal program. Two styles of indentation were used --
                    blocked and non-blocked -- in addition to four passible levels of indentation
                    (0, 2, 4, 6 spaces). Both experienced and novice subjects were used. Although
                    the blocking style made no difference, the level of indentation had a
                    significant effect on program comprehension. (2--4 spaces had the highest mean
                    score for program comprehension.) We recommend that a moderate level of
                    indentation be used to increase program comprehension and user satisfaction.},
    pages         = {861-867},
    location      = {Online},
    doi           = {10.1145/182.358437},
    url           = {https://www.researchgate.net/publication/234809222_Program_indentation_and_comprehensibility},
    urlaccessdate = {2017-09-07},
}


@article{independentFramework,
    title         = {A Language Independent Framework for Context-sensitive Formatting},
    author        = {M.G.J. Van den Brand and A.T. Kooiker and J.J. Vinju},
    month         = {03},
    year          = {2006},
    journal       = {CSMR '06 Proceedings of the Conference on Software Maintenance and Reengineering},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    volume        = {26},
    issue         = {1},
    abstract      = {Automated formatting is an important technique for the software maintainer. It
                    is either applied separately to improve the readability of source code, or as
                    part of a source code transformation tool chain. In this paper we report on the
                    application of generic tools for constructing formatters. In an industrial
                    setting automated formatters need to be tailored to the requirements of the
                    customer. The (legacy) programming language or dialect and the corporate
                    formatting conventions are specific and non-negotiable. Can generic formatting
                    tools deal with such unexpected requirements? Driven by an industrial case of
                    nearly 80 thousand lines of Cobol code, several limitations in existing
                    formatting technology have been addressed. We improved its flexibility by
                    replacing a generative phase by a generic tool, and we added a little
                    expressiveness to the formatting back end. Most importantly, we employed a
                    multi-stage formatting framework that can cope with any kind of formatting
                    convention using more computational power.},
    pages         = {103-112},
    doi           = {10.1109/CSMR.2006.4},
    isbn          = {0-7695-2536-9},
    location      = {Bari, Italy},
    url           = {https://www.researchgate.net/publication/4226896_A_language_independent_framework_for_context-sensitive_formatting},
    urlaccessdate = {2017-09-07},
}


@article{architectureFormatting,
    title         = {An architecture for context-sensitive formatting},
    author        = {M.G.J. van den Brand and A.T. Kooiker and J.J. Vinju and N.P. Veerman},
    month         = {09},
    year          = {2005},
    abstract      = {We developed an architecture for context-sensitive formatting of source code.
                    The architecture was implemented and applied in an industrial formatting case.},
    journal       = {21st IEEE International Conference on Software Maintenance (ICSM'05)},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    doi           = {10.1109/ICSM.2005.17},
    isbn          = {0-7695-2368-4},
    location      = {Budapest, Hungary, Hungary},
    url           = {https://www.researchgate.net/publication/4175894_An_architecture_for_context-sensitive_formatting},
    urlaccessdate = {2017-09-07},
}


@article{prettyPrinting,
    title         = {Pretty-printing for software reengineering},
    author        = {Merijn De Jonge},
    month         = {10},
    year          = {2002},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    abstract      = {Automatic software reengineerings change or repair existing software systems.
                    They are usually tailor-made for a specific customer and language dependent.
                    Maintaining similar reengineerings for multiple customers and different language
                    dialects might therefore soon become problematic unless advanced language
                    technology is being used. Generic pretty-printing is part of such technology and
                    is the subject of this paper. We discuss specific pretty-print aspects of
                    software reengineering such as fulfilling customer-specific format conventions,
                    preserving existing layout, and producing multiple output formats. In addition,
                    we describe pretty-print techniques that help to reduce maintenance effort of
                    tailor-made reengineerings supporting multiple language dialects. Applications,
                    such as COBOL reengineering and SDL documentation generation show that our
                    techniques, implemented in the generic pretty-printer GPP, are feasible.},
    journal       = {International Conference on Software Maintenance, 2002. Proceedings},
    doi           = {10.1109/ICSM.2002.1167816},
    isbn          = {0-7695-1819-2},
    location      = {Montreal, Quebec, Canada},
    url           = {https://www.researchgate.net/publication/3998748_Pretty-Printing_for_Software_Reengineering},
    urlaccessdate = {2017-09-07},
}


@article{massMaintenance,
    title         = {Automated Mass Maintenance of Software Assets},
    author        = {Niels Veerman},
    month         = {03},
    year          = {2007},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    abstract      = {This is a research summary of a PhD project in the area of massive software
                    maintenance automation. We explain the context, approach, and contributions.},
    journal       = {11th European Conference on Software Maintenance and Reengineering (CSMR'07)},
    doi           = {10.1109/CSMR.2007.15},
    isbn          = {0-7695-2802-3},
    location      = {Amsterdam, The Netherlands},
    url           = {https://www.researchgate.net/publication/221569579_Automated_Mass_Maintenance_of_Software_Assets},
    urlaccessdate = {2017-09-11},
}


@article{legacyAssets,
    title         = {Towards automated modification of legacy assets},
    author        = {Alex Sellink and Chris Verhoef},
    month         = {01},
    year          = {2000},
    journal       = {Annals of Software Engineering},
    publisher     = {Kluwer Academic Publishers},
    address       = {Dordrecht, The Netherlands},
    volume        = {9},
    issue         = {1-4},
    pages         = {315-336},
    abstract      = {In this paper we argue that there is a necessity for automating modifications
                    to legacy assets. We propose a five layered process for the introduction and
                    employment of tool support that enables automated modification to entire legacy
                    systems. Furthermore, we elaborately discuss each layer on a conceptual level,
                    and we make appropriate references to sources where technical contributions
                    supporting that particular layer can be found. We sketch the perspective that
                    more and more people working in the software engineering area will be
                    contributing to working on existing systems and/or tools to support such work.},
    location      = {Red Bank, NJ, USA},
    doi           = {10.1023/A:1018941228255},
    url           = {https://www.researchgate.net/publication/2825635_Towards_Automated_Modification_of_Legacy_Assets},
    urlaccessdate = {2017-09-11},
}


@article{softwarePortfolio,
    title         = {Automated maintenance of a software portfolio},
    author        = {Niels Veerman},
    month         = {10},
    year          = {2006},
    journal       = {Science of Computer Programming - Special issue on source code analysis and manipulation (SCAM 2005)},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {62},
    issue         = {3},
    pages         = {287-317},
    abstract      = {This is an experience report on automated mass maintenance of a large Cobol
                    software portfolio. A company in the financial services and insurance industry
                    upgraded their database system to a new version, affecting their entire software
                    portfolio. The database system was accessed by the portfolio of 45 systems,
                    totalling nearly 3000 programs and covering over 4 million lines of Cobol code.
                    We upgraded the programs to the new database version using several automatic
                    tools, and we performed an automated analysis supporting further manual
                    modifications by the system experts. The automatic tools were built using a
                    combination of lexical and syntactic technology, and they were deployed in a
                    mass update factory to allow large-scale application to the software portfolio.
                    The updated portfolio has been accepted and taken into production by the
                    company, serving over 600 employees with the new database version. In this
                    paper, we discuss the automated upgrade from problem statement to project
                    costs.},
    doi           = {10.1016/j.scico.2006.04.006},
    location      = {Amsterdam, The Netherlands},
    url           = {https://www.researchgate.net/publication/222831264_Automated_maintenance_of_a_software_portfolio},
    urlaccessdate = {2017-09-12},
}


@article{pushdownAutomata,
    title         = {2-Head Pushdown Automata},
    author        = {Awe Ayodeji Samson},
    month         = {06},
    year          = {2015},
    journal       = {Procedia - Social and Behavioral Sciences},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {195},
    issue         = {1},
    pages         = {2037-2046},
    abstract      = {Finite state automata recognize regular languages which can be used in text
                    processing, compilers, and hardware design. Two head finite automata accept
                    linear context free languages. In addition, pushdown automata are able to
                    recognize context free languages which can be used in programming languages and
                    artificial intelligence. The finite automaton has deterministic and
                    non-deterministic version likewise the two head finite automata and the pushdown
                    automata. The deterministic version of these machines is such that there is no
                    choice of move in any situation while the non-deterministic version has a choice
                    of move. In this research the 2-head pushdown automata are described which is
                    more powerful than the pushdown automata and it is able to recognize some
                    non-context free languages as well. During this work, the main task is to
                    characterize these machines.},
    doi           = {10.1016/j.sbspro.2015.06.225},
    location      = {Department of Mathematics, Eastern Mediterranean University, Mersin 10 Turkey, Famagusta, North Cyprus},
    url           = {https://www.researchgate.net/publication/282556609_2-Head_Pushdown_Automata},
    urlaccessdate = {2017-09-12},
}


@article{aspectOriented,
    title         = {Aspect-oriented model-driven code generation: A systematic mapping study},
    author        = {Abid Mehmood and Dayang Norhayati Abang Jawawi},
    month         = {09},
    year          = {2012},
    journal       = {Information and Software Technology},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {55},
    issue         = {2},
    pages         = {395-411},
    abstract      = {Context: Model-driven code generation is being increasingly applied to enhance
                    software development from perspectives of maintainability, extensibility and
                    reusability. However, aspect-oriented code generation from models is an area
                    that is currently underdeveloped. Objective: In this study we provide a survey
                    of existing research on aspect-oriented modeling and code generation to discover
                    current work and identify needs for future research. Method: A systematic
                    mapping study was performed to find relevant studies. Classification schemes
                    have been defined and the 65 selected primary studies have been classified on
                    the basis of research focus, contribution type and research type. Results: The
                    papers of solution proposal research type are in a majority. All together
                    aspect-oriented modeling appears being the most focused area divided into
                    modeling notations and process (36\%) and model composition and interaction
                    management (26\%). The majority of contributions are methods. Conclusion:
                    Aspect-oriented modeling and composition mechanisms have been significantly
                    discussed in existing literature while more research is needed in the area of
                    model-driven code generation. Furthermore, we have observed that previous
                    research has frequently focused on proposing solutions and thus there is need
                    for research that validates and evaluates the existing proposals in order to
                    provide firm foundations for aspect-oriented model-driven code generation.},
    location      = {Department of Software Engineering, Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia},
    doi           = {10.1016/j.infsof.2012.09.003},
    url           = {https://www.researchgate.net/publication/257391227_Aspect-oriented_model-driven_code_generation_A_systematic_mapping_study},
    urlaccessdate = {2017-09-12},
}


@article{aspectOrientationReview,
    title         = {A Review: Analysis of Aspect Orientation and Model Driven Engineering for Code Generation},
    author        = {Dhiraj Gurunule and Madhu Nashipudimath},
    month         = {03},
    year          = {2015},
    journal       = {Procedia Computer Science},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {45},
    issue         = {1},
    pages         = {852-861},
    abstract      = {In the development of large and complex software application software engineers
                    has to focuses on many requirements other than desired application’s requirement
                    at the coding and design level. Code Generation is a technique which is use to
                    automatically generates lower level executable code from higher level design
                    artifact. Code generation provides design of the code at higher abstract level
                    so that software developers can focuses on higher level design problem
                    simultaneously meeting goals of desired application. Aspect Orientation (AO) is
                    characterizing by identification and separation of different concerns and
                    encapsulates them in modules. Concern is an interest which pertains to system
                    operation, function, development or any other things which is important to one
                    of the stakeholder. Model Driven Engineering (MDE) is a development paradigm
                    which is characterize by model transformation and uses models to support various
                    stages of the development life cycle. Model is primary artifact in MDE. In this
                    paper we analyze both techniques i.e. AO and MDE, and how they can be used for
                    code generation.},
    location      = {Information Technology Department and Computer Department PIIT, New Panvel, India},
    doi           = {10.1016/j.procs.2015.03.171},
    url           = {https://www.researchgate.net/publication/276899518_A_Review_Analysis_of_Aspect_Orientation_and_Model_Driven_Engineering_for_Code_Generation},
    urlaccessdate = {2017-09-12},
}


@article{quantificationOfInterface,
    title         = {Quantification of interface visual complexity},
    author        = {Aliaksei Miniukovich and Antonella De Angeli},
    month         = {03},
    year          = {2014},
    journal       = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    pages         = {153-160},
    abstract      = {Designers strive for enjoyable user experience (UX) and put a significant
                    effort into making graphical user interfaces (GUI) both usable and beautiful.
                    Our goal is to minimize their effort: with this purpose in mind, we have been
                    studying automatic metrics of GUI qualities. These metrics could enable
                    designers to iterate their designs more quickly. We started from the
                    psychological findings that people tend to prefer simpler things. We then
                    assumed visual complexity determinants also determine visual aesthetics and
                    outlined eight of them as belonging to three dimensions: information amount
                    (visual clutter and color variability), information organization (symmetry,
                    grid, ease-of-grouping and prototypicality), and information discriminability
                    (contour density and figure-ground contrast). We investigated five determinants
                    (visual clutter, symmetry, contour density, figure-ground contrast and color
                    variability) and proposed six associated automatic metrics. These metrics take
                    screenshots of GUI as input and can thus be applied to any type of GUI. We
                    validated the metrics through a user study: we gathered the ratings of immediate
                    impressions of GUI visual complexity and aesthetics, and correlated them with
                    the output of the metrics. The output explained up to 51\% of aesthetics ratings
                    and 50\% of complexity ratings. This promising result could be further extended
                    towards the creation of tLight, our automatic GUI evaluation tool.},
    location      = {University of Trento, Como, Italy},
    doi           = {10.1145/2598153.2598173},
    isbn          = {978-1-4503-2775-6},
    url           = {https://www.researchgate.net/publication/266657991_Quantification_of_interface_visual_complexity},
    urlaccessdate = {2017-10-10},
}


@article{toolsForProjectManagement,
    title         = {Comparison of open source tools for project management},
    author        = {André Marques Pereira and Rafael Queiroz Gonçalves and Christiane Gresse von Wangenheim and Luigi Buglione},
    month         = {03},
    year          = {2013},
    journal       = {International Journal of Software Engineering and Knowledge Engineering},
    publisher     = {World Scientific Publishing, },
    address       = {Singapore},
    volume        = {23},
    issue         = {02},
    pages         = {189-209},
    abstract      = {Software projects often fail, because they are not adequately managed. The
                    establishment of effective and efficient project management practices still
                    remains a key challenge to software organizations. Striving to address these
                    needs, "best practice" models, such as, the Capability Maturity Model
                    Integration (CMMI) or the Project Management Body of Knowledge (PMBOK), are
                    being developed to assist organizations in improving project management.
                    Although not required, software tools can help implement the project management
                    process in practice. In order to provide comprehensive, low-cost tool support
                    for project management, specifically, for small and medium enterprises (SMEs),
                    in this paper we compare the most popular free/open-source web-based project
                    management tools with respect to their compliance to PMBOK and CMMI for
                    Development (CMMI-DEV). The results of this research can be used by
                    organizations to make decisions on tool adoptions as well as a basis for
                    evolving software tools in alignment with best practices models.},
    location      = {Federal University of Santa Catarina (UFSC), Florianópolis, Santa Catarina, Brazil},
    doi           = {10.1142/S0218194013500046},
    url           = {https://www.researchgate.net/publication/273569026_Comparison_of_open_source_tools_for_project_management},
    urlaccessdate = {2017-10-27},
}


@article{naturalCodingConventions,
    title         = {Learning Natural Coding Conventions},
    author        = {Miltiadis Allamanis and Earl T. Barr and Charles Sutton},
    month         = {11},
    year          = {2014},
    journal       = {Proceeding FSE 2014 Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    volume        = {23},
    issue         = {02},
    pages         = {281-293},
    abstract      = {Every programmer has a characteristic style, ranging from preferences about
                    identifier naming to preferences about object relationships and design patterns.
                    Coding conventions define a consistent syntactic style, fostering readability
                    and hence maintainability. When collaborating, programmers strive to obey a
                    project’s coding conventions. However, one third of reviews of changes contain
                    feedback about coding conventions, indicating that programmers do not always
                    follow them and that project members care deeply about adherence. Unfortunately,
                    programmers are often unaware of coding conventions because inferring them
                    requires a global view, one that aggregates the many local decisions programmers
                    make and identifies emergent consensus on style. We present NATURALIZE, a
                    framework that learns the style of a codebase, and suggests revisions to improve
                    stylistic consistency. NATURALIZE builds on recent work in applying statistical
                    natural language processing to source code. We apply NATURALIZE to suggest
                    natural identifier names and formatting conventions. We present four tools
                    focused on ensuring natural code during development and release management,
                    including code review. NATURALIZE achieves 94\% accuracy in its top suggestions
                    for identifier names. We used NATURALIZE to generate 18 patches for 5 open
                    source projects: 14 were accepted.},
    location      = {Hong Kong, China},
    doi           = {10.1145/2635868.2635883},
    url           = {https://www.researchgate.net/publication/260250447_Learning_Natural_Coding_Conventions},
    urlaccessdate = {2017-10-27},
}


@article{codePlagiarismDetection,
    title         = {Style Analysis for Source Code Plagiarism Detection},
    author        = {Olfat Mirza and Mike Joy},
    month         = {06},
    year          = {2015},
    journal       = {International Conference Plagiarism across Europe and Beyond 2015},
    publisher     = {Online},
    address       = {\url{https://plagiarism.pefka.mendelu.cz/?sec=cf15#proc}},
    pages         = {53–61},
    abstract      = {Plagiarism has become an increasing problem in higher education in recent
                    years. A number of research papers have discussed the problem of plagiarism in
                    terms of text and source code and the techniques to detect it in various
                    contexts. There is a variety of easy ways of copying others’ work because the
                    source code can be obtained from online source code banks and textbooks, which
                    makes plagiarism easy for students. Source code plagiarism has a very specific
                    definition, and Parker and Hamblen define plagiarism on software as “A program
                    that has been produced from another program with a small number of routine
                    transformations”. The transformations can range from very simple changes to very
                    difficult ones, which can be one of the six levels of program modifications that
                    are given by Faidhi and Robinson. Coding style is a way to detect source code
                    plagiarism because it relates to programmer personality without affecting the
                    logic of a program, and can be used to differentiate between different code
                    authors. This paper reviews a number of publications which report style
                    comparison to detect source code plagiarism in order to determine research gaps
                    and explore areas where this approach can be improved. A summary of the
                    plagiarism techniques in which style analysis can help identify plagiarism is
                    presented.},
    location      = {Brno, Czech Republic},
    url           = {https://www.researchgate.net/publication/303932091_Style_Analysis_for_Source_Code_Plagiarism_Detection},
    urlaccessdate = {2017-10-27},
}


@article{annotationAssistant,
    title         = {An Annotation Assistant for Interactive Debugging of Programs with Common Synchronization Idioms},
    author        = {Elmas, Tayfun and Sezgin, Ali and Tasiran, Serdar and Qadeer, Shaz},
    booktitle     = {Proceedings of the 7th Workshop on Parallel and Distributed Systems: Testing, Analysis, and Debugging},
    series        = {PADTAD '09},
    year          = {2009},
    pages         = {10:1--10:11},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    abstract      = {This paper explores an approach to improving the practical usability of static
                    verification tools for debugging synchronization idioms. Synchronization idioms such
                    as mutual exclusion and readers/writer locks are widely-used to ensure atomicity of
                    critical regions. We present an annotation assistant that automatically generates
                    program annotations. These annotations express noninterference between program
                    statements, ensured by the synchronization idioms, and are used to identify atomic
                    code regions. This allows the programmer to debug the use of the idioms in the
                    program. We start by formalizing several well-known idioms by providing an abstract
                    semantics for each idiom. For programs that use these idioms, we require the
                    programmer to provide a few predicates linking the idiom with its realization in
                    terms of program variables. From these, we automatically generate a proof script
                    that is mechanically checked. These scripts include steps such as automatically
                    generating assertions and annotating program actions with them, introducing
                    auxiliary variables and invariants. We have successfully shown the applicability of
                    this approach to several concurrent programs from the literature.},
    articleno     = {10},
    numpages      = {11},
    location      = {Chicago, Illinois},
    doi           = {10.1145/1639622.1639632},
    isbn          = {978-1-60558-655-7},
    acmid         = {1639632},
    url           = {http://doi.acm.org/10.1145/1639622.1639632},
    keywords      = {atomicity, concurrent programs, synchronization idioms},
    urlaccessdate = {2017-10-30},
}


@article{redesignOfGit,
    title         = {Purposes, Concepts, Misfits, and a Redesign of Git},
    author        = {Santiago Perez De Rosso and Daniel Jackson},
    journal       = {SIGPLAN Not.},
    volume        = {51},
    number        = {10},
    month         = {10},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    year          = {2016},
    issn          = {0362-1340},
    pages         = {292--310},
    abstract      = {Git is a widely used version control system that is powerful but complicated.
                    Its complexity may not be an inevitable consequence of its power but rather
                    evidence of flaws in its design. To explore this hypothesis, we analyzed the
                    design of Git using a theory that identifies concepts, purposes, and misfits.
                    Some well-known difficulties with Git are described, and explained as misfits in
                    which underlying concepts fail to meet their intended purpose. Based on this
                    analysis, we designed a reworking of Git (called Gitless) that attempts to
                    remedy these flaws. To correlate misfits with issues reported by users, we
                    conducted a study of Stack Overflow questions. And to determine whether users
                    experienced fewer complications using Gitless in place of Git, we conducted a
                    small user study. Results suggest our approach can be profitable in identifying,
                    analyzing, and fixing design problems.},
    keywords      = {Git, concept design, concepts, design, software design, usability, version control},
    numpages      = {19},
    location      = {Massachusetts Institute of Technology, USA},
    doi           = {10.1145/3022671.2984018},
    acmid         = {2984018},
    url           = {https://www.researchgate.net/publication/311477527_Purposes_concepts_misfits_and_a_redesign_of_git},
    urlaccessdate = {2017-10-30},
}


@article{codeClassification,
    title         = {Code Classification as a Learning and Assessment Exercise for Novice Programmers},
    author        = {Errol Thompson and Jacqueline L. Whalley and Raymond Lister and Beth Simon},
    journal       = {Proceedings of the 19th Annual Conference of the National Advisory Committee on Computing Qualifications},
    month         = {07},
    year          = {2006},
    pages         = {291--298},
    publisher     = {Online},
    address       = {Aston University, Computer Science Department},
    abstract      = {When students are given code that is very similar in structure or purpose, how
                    well do they actually recognise the similarities and differences? As part of the
                    BRACElet project, a multi-institutional investigation into reading and
                    comprehension skills of novice programmers, students were asked to classify four
                    code segments that found the minimum or maximum in an array of numbers. This
                    paper reports on the analysis of responses to this question and draws
                    conclusions about the students' ability to recognise the similarities and
                    differences in example code. It then raises questions with respect to an
                    approach to teaching that uses variations in code examples. Received Citrus
                    Award for Collaborative Research and highly recommended in the Best Paper awards
                    },
    location      = {NACCQ, Wellington, New Zealand},
    url           = {https://www.researchgate.net/publication/255948009_Code_Classification_as_a_Learning_and_Assessment_Exercise_for_Novice_Programmers},
    urlaccessdate = {2017-10-31},
}


@article{codeScanningPatterns,
    title         = {Code Scanning Patterns in Program Comprehension},
    author        = {Christoph Aschwanden and Martha Crosby},
    journal       = {Proceedings of the 39th Annual Hawaii International Conference on System Sciences},
    month         = {01},
    year          = {2006},
    publisher     = {Online},
    address       = {Department of Information and Computer Sciences},
    abstract      = {Various publications have identified Beacons to play a key role in program
                    comprehension. Beacons are code fragments that help developers comprehend
                    programs. It has been shown that expert programmers pay more attention to
                    Beacons than novices. Beacons are described as the link between source code and
                    hypothesis verification. Beacons are sets of key features that typically
                    indicate the presence of a particular data structure or operation in source
                    code. However, only little research has been done trying to identify and explain
                    them in greater detail. It has been demonstrated that good variable and
                    procedure names help in program comprehension. Documentation is beneficial as
                    well. The so-called swap operation for variables is a strong indicator for a
                    sorting algorithm. We conducted an eye tracking study using the EventStream
                    software framework as the instrument to investigate programmers' behavior during
                    a code reading exercise. Preliminary results suggest Beacons to be present when
                    the longest fixation duration is thousand milliseconds or higher. Comparing
                    participants with correct understanding versus participants with wrong
                    understanding showed differences in focus of attention. Based on the study
                    conducted, we suggest to consider "int k=(a+b)/2" as Beacons during program
                    comprehension as well as lines of code which exhibit very long fixations above
                    1000 milliseconds.},
    location      = {University of Hawaii at Manoa},
    url           = {https://www.researchgate.net/publication/250718584_Code_Scanning_Patterns_in_Program_Comprehension},
    urlaccessdate = {2017-10-31},
}


@article{programUnderstanding,
    title         = {The Use of Reading Technique and Visualization for Program Understanding},
    author        = {Daniel Porto and Manoel G. Mendonça and Sandra Camargo Pinto Ferraz Fabbri},
    year          = {2009},
    month         = {07},
    pages         = {386--391},
    publisher     = {Online},
    address       = {\url{https://www.researchgate.net/profile/Sarah_Printy/publication/221391313_Enhancing_Property_Specification_Tools_With_Validation_Techniques/links/0deec52af54ccf046e000000.pdf#page=411}},
    abstract      = {Code comprehension is the basis for many other activities in software
                    engineering. It is also time consuming and can greatly profit from tools that decrease the time
                    that it usually consumes. This paper presents a tool named CRISTA that supports code
                    comprehension through the application of Stepwise Abstraction. It uses a visual metaphor to
                    represent the code and supports essential tasks for code reading, inspection and documentation.
                    Three case studies were carried out to evaluate the tool with respect to usability and
                    usefulness. In all of them the experiment participants considered that the tool facilitates code
                    comprehension, inspection and documentation.},
    location      = {Boston, Massachusetts},
    isbn          = {1-891706-24-1},
    url           = {https://www.researchgate.net/publication/221390090_The_Use_of_Reading_Technique_and_Visualization_for_Program_Understanding},
    urlaccessdate = {2017-10-31},
}


@article{theImpactOfIdentifierStyle,
    title         = {The impact of identifier style on effort and comprehension},
    author        = {Binkley, Dave and Davis, Marcia and Lawrie, Dawn and Maletic, Jonathan I. and Morrell, Christopher and Sharif, Bonita},
    journal       = {Empirical Software Engineering},
    year          = {2013},
    month         = {Apr},
    volume        = {18},
    number        = {2},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {219--276},
    abstract      = {A family of studies investigating the impact of program identifier style on
                    human comprehension is presented. Two popular identifier styles are examined,
                    namely camel case and underscore. The underlying hypothesis is that identifier
                    style affects the speed and accuracy of comprehending source code. To
                    investigate this hypothesis, five studies were designed and conducted. The first
                    study, which investigates how well humans read identifiers in the two different
                    styles, focuses on low-level readability issues. The remaining four studies
                    build on the first to focus on the semantic implications of identifier style.
                    The studies involve 150 participants with varied demographics from two different
                    universities. A range of experimental methods is used in the studies including
                    timed testing, read aloud, and eye tracking. These methods produce a broad set
                    of measurements and appropriate statistical methods, such as regression models
                    and Generalized Linear Mixed Models (GLMMs), are applied to analyze the results.
                    While unexpected, the results demonstrate that the tasks of reading and
                    comprehending source code is fundamentally different from those of reading and
                    comprehending natural language. Furthermore, as the task becomes similar to
                    reading prose, the results become similar to work on reading natural language
                    text. For more ``source focused'' tasks, experienced software developers appear
                    to be less affected by identifier style; however, beginners benefit from the use
                    of camel casing with respect to accuracy and effort.},
    location      = {Department of Computer Science, Loyola University Maryland, Baltimore, USA},
    issn          = {1573-7616},
    doi           = {10.1007/s10664-012-9201-4},
    url           = {https://www.researchgate.net/publication/257560017_The_impact_of_identifier_style_on_effort_and_comprehension},
    urlaccessdate = {2017-10-31},
}


@article{womenAndMen,
    title         = {Women and men -- Different but equal: On the impact of identifier style on source code reading},
    author        = {Zohreh Sharafi and Zéphyrin Soh and Yann-Gaël Guéhéneuc},
    year          = {2012},
    month         = {06},
    journal       = {20th IEEE International Conference on Program Comprehension},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    abstract      = {Program comprehension is preliminary to any program evolution task. Researchers
                    agree that identifiers play an important role in code reading and program understanding
                    activities. Yet, to the best of our knowledge, only one work investigated the impact of gender
                    on the memorability of identifiers and thus, ultimately, on program comprehension. This paper
                    reports the results of an experiment involving 15 male subjects and nine female subjects to
                    study the impact of gender on the subjects' visual effort, required time, as well as accuracy to
                    recall Camel Case versus Underscore identifiers in source code reading. We observe no
                    statistically-significant difference in term of accuracy, required time, and effort. However,
                    our data supports the conjecture that male and female subjects follow different comprehension
                    strategies: female subjects seem to carefully weight all options and spend more time to rule out
                    wrong answers while male subjects seem to quickly set their minds on some answers, possibly the
                    wrong ones. Indeed, we found that the effort spent on wrong answers is significantly higher for
                    female subjects and that there is an interaction between the effort that female subjects
                    invested on wrong answers and their higher percentages of correct answers when compared to male
                    subjects.},
    location      = {Passau, Germany},
    issn          = {1092-8138},
    doi           = {10.1109/ICPC.2012.6240505},
    url           = {https://www.researchgate.net/publication/261095133_Women_and_men_-_Different_but_equal_On_the_impact_of_identifier_style_on_source_code_reading},
    urlaccessdate = {2017-10-31},
}


@article{autofoldingForSourceCode,
    title         = {Autofolding for Source Code Summarization},
    author        = {Jaroslav Fowkes and Pankajan Chanthirasegaran and Razvan Ranca},
    journal       = {IEEE Transactions on Software Engineering},
    year          = {2017},
    month         = {02},
    volume        = {PP},
    issue         = {99},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    abstract      = {Developers spend much of their time reading and browsing source code, raising
                    new opportunities for summarization methods. Indeed, modern code editors provide
                    code folding, which allows one to selectively hide blocks of code. However this
                    is impractical to use as folding decisions must be made manually or based on
                    simple rules. We introduce the autofolding problem, which is to automatically
                    create a code summary by folding less informative code regions. We present a
                    novel solution by formulating the problem as a sequence of AST folding
                    decisions, leveraging a scoped topic model for code tokens. On an annotated set
                    of popular open source projects, we show that our summarizer outperforms simpler
                    baselines, yielding a 28\% error reduction. Furthermore, we find through a case
                    study that our summarizer is strongly preferred by experienced developers. More
                    broadly, we hope this work will aid program comprehension by turning code
                    folding into a usable and valuable tool.},
    location      = {School of Informatics, University of Edinburgh, Edinburgh, UK},
    issn          = {0098-5589},
    doi           = {10.1109/TSE.2017.2664836},
    url           = {https://www.researchgate.net/publication/260911164_Autofolding_for_Source_Code_Summarization},
    urlaccessdate = {2017-10-31},
}


@article{quantifyingProgramComprehension,
    title         = {Quantifying Program Comprehension with Interaction Data},
    author        = {Roberto Minelli and Andrea Mocci and Michele Lanza},
    journal       = {14th International Conference on Quality Software},
    year          = {2014},
    month         = {10},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    abstract      = {It is common knowledge that program comprehension takes up a substantial part
                    of software development. This "urban legend" is based on work that dates back
                    decades, which throws up the question whether the advances in software
                    development tools, techniques, and methodologies that have emerged since then
                    may invalidate or confirm the claim. We present an empirical investigation which
                    goal is to confirm or reject the claim, based on interaction data which captures
                    the user interface activities of developers. We use interaction data to
                    empirically quantify the distribution of different developer activities during
                    software development: In particular, we focus on estimating the role of program
                    comprehension. In addition, we investigate if and how different developers and
                    session types influence the duration of such activities. We analyze interaction
                    data from two different contexts: One comes from the ECLIPSE IDE on Java source
                    code development, while the other comes from the PHARO IDE on Smalltalk source
                    code development. We found evidence that code navigation and editing occupies
                    only a small fraction of the time of developers, while the vast majority of the
                    time is spent on reading \& understanding source code. In essence, the importance
                    of program comprehension was significantly underestimated by previous
                    research.},
    location      = {Dallas, TX, USA},
    isbn          = {978-1-4799-7198-5},
    doi           = {10.1109/QSIC.2014.11},
    url           = {https://www.researchgate.net/publication/286487172_Quantifying_Program_Comprehension_with_Interaction_Data},
    urlaccessdate = {2017-10-31},
}


@article{syntaxHighlightingInfluencing,
    title         = {Syntax highlighting as an influencing factor when reading and comprehending source code},
    author        = {Tanya R. Beelders and Jean-Pierre L. du Plessis},
    year          = {2015},
    year          = {01},
    publisher     = {Journal of Eye Movement Research},
    address       = {Bern, Switzerland},
    number        = {1},
    volume        = {9},
    keywords      = {eye tracking; syntax highlighting; code comprehension; reading behaviour},
    abstract      = {Syntax highlighting or syntax colouring, plays a vital role in programming
                    development environments by colour-coding various code elements differently. The
                    supposition is that this syntax highlighting assists programmers when reading
                    and analysing code. However, academic text books are largely only available in
                    black-and-white which could influence the comprehension of novice and beginner
                    programmers. This study investigated whether student programmers experience more
                    difficulty in reading and comprehending source code when it is presented without
                    syntax highlighting. Number of fixations, fixation durations and regressions
                    were all higher for black-and-white code than for colour code but not
                    significantly so. Subjectively students indicated that the colour code snippets
                    were easier to read and more aesthetically pleasing. Based on the analysis it
                    could be concluded that students do not experience significantly more difficulty
                    when reading code in black-and-white as printed in text books.},
    issn          = {1995-8692},
    location      = {University of the Free State, Bloemfontein, Free State, South Africa},
    url           = {https://bop.unibe.ch/index.php/JEMR/article/view/2429},
    urlaccessdate = {2017-11-01},
}


@article{theRoleOfMethodChains,
    title         = {The Role of Method Chains and Comments in Software Readability and Comprehension; An Experiment},
    author        = {J. Börstler and B. Paech},
    journal       = {IEEE Transactions on Software Engineering},
    year          = {2016},
    month         = {09},
    volume        = {42},
    number        = {9},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    pages         = {886-898},
    abstract      = {Software readability and comprehension are important factors in software
                    maintenance. There is a large body of research on software measurement, but the actual factors
                    that make software easier to read or easier to comprehend are not well understood. In the
                    present study, we investigate the role of method chains and code comments in software
                    readability and comprehension. Our analysis comprises data from 104 students with varying
                    programming experience. Readability and comprehension were measured by perceived readability,
                    reading time and performance on a simple cloze test. Regarding perceived readability, our
                    results show statistically significant differences between comment variants, but not between
                    method chain variants. Regarding comprehension, there are no significant differences between
                    method chain or comment variants. Student groups with low and high experience, respectively,
                    show significant differences in perceived readability and performance on the cloze tests. Our
                    results do not show any significant relationships between perceived readability and the other
                    measures taken in the present study. Perceived readability might therefore be insufficient as
                    the sole measure of software readability or comprehension. We also did not find any
                    statistically significant relationships between size and perceived readability, reading time and
                    comprehension.},
    keywords      = {software maintenance;software metrics;cloze tests;code comments;method
                    chains;software comprehension;software maintenance;software measurement;software
                    readability;Complexity theory;Guidelines;Object oriented modeling;Programming;Software;Software
                    engineering;Software measurement;Software readability;comments;experiment;method chains;software
                    comprehension;software measurement},
    ISSN          = {0098-5589},
    location      = {Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden},
    doi           = {10.1109/TSE.2016.2527791},
    url           = {https://www.researchgate.net/publication/294119566_The_Role_of_Method_Chains_and_Comments_in_Software_Readability_and_Comprehension_-_An_Experiment},
    urlaccessdate = {2017-11-02},
}


@article{blindAndSightedProgrammers,
    title         = {A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers},
    journal       = {IEEE Transactions on Software Engineering},
    author        = {A. Armaly and P. Rodeghero and C. McMillan},
    year          = {2017},
    month         = {07},
    volume        = {PP},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    number        = {99},
    pages         = {1-1},
    abstract      = {Programmers who are blind use a screen reader to speak source code one word at
                    a time, as though the code were text. This process of reading is in stark
                    contrast to sighted programmers, who skim source code rapidly with their eyes.
                    At present, it is not known whether the difference in these processes has
                    effects on the program comprehension gained from reading code. These effects are
                    important because they could reduce both the usefulness of accessibility tools
                    and the generalizability of software engineering studies to persons with low
                    vision. In this paper, we present an empirical study comparing the program
                    comprehension of blind and sighted programmers. We found that both blind and
                    sighted programmers prioritize reading method signatures over other areas of
                    code. Both groups obtained an equal and high degree of comprehension, despite
                    the different reading processes.},
    keywords      = {Blindness;Navigation;Programming profession;Software;Software
                    engineering;Tools;Program comprehension;accessibility technology;blindness},
    doi           = {10.1109/TSE.2017.2729548},
    ISSN          = {0098-5589},
    location      = {University of Notre Dame, Notre Dame, Indiana, United States},
    url           = {https://www.researchgate.net/publication/318576489_Blindness_and_Program_Comprehension},
    urlaccessdate = {2017-11-02},
}


@article{usingVersionControlData,
    title         = {Using version control data to evaluate the impact of software tools: a case study of the Version Editor},
    author        = {D. L. Atkins and T. Ball and T. L. Graves and A. Mockus},
    journal       = {IEEE Transactions on Software Engineering},
    volume        = {28},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    number        = {7},
    year          = {2002},
    month         = {07},
    pages         = {625-637},
    abstract      = {Software tools can improve the quality and maintainability of software, but are
                    expensive to acquire, deploy, and maintain, especially in large organizations.
                    We explore how to quantify the effects of a software tool once it has been
                    deployed in a development environment. We present an effort-analysis method that
                    derives tool usage statistics and developer actions from a project's change
                    history (version control system) and uses a novel effort estimation algorithm to
                    quantify the effort savings attributable to tool usage. We apply this method to
                    assess the impact of a software tool called VE, a version-sensitive editor used
                    in Bell Labs. VE aids software developers in coping with the rampant use of
                    certain preprocessor directives (similar to \#if/\#endif in C source files). Our
                    analysis found that developers were approximately 40 percent more productive
                    when using VE than when using standard text editors.},
    keywords      = {configuration management;software metrics;software quality;software tools;text
                    editing;C source files;VE tool;Version Editor;change history;large
                    organizations;preprocessor directives;software effort analysis method;software
                    maintainability;software quality;software tool impact evaluation;text
                    editors;tool usage statistics;version control data;Computer aided software
                    engineering;Control system analysis;Control systems;History;Software
                    maintenance;Software quality;Software tools;Standards development;Statistical
                    analysis;Statistics},
    doi           = {10.1109/TSE.2002.1019478},
    ISSN          = {0098-5589},
    location      = {Oregon Univ., Eugene, OR, USA},
    url           = {http://ieeexplore.ieee.org/document/1019478/},
    urlaccessdate = {2017-11-03},
}


@article{findingRegressionsInProjects,
    title         = {Finding Regressions in Projects under Version Control Systems},
    author        = {Jaroslav Bendik and Nikola Benes and Ivana Cerna},
    journal       = {Computing Research Repository},
    volume        = {abs/1708.06623},
    publisher     = {Cornell University Library},
    address       = {Ithaca, NY, USA},
    year          = {2017},
    month         = {10},
    archivePrefix = {arXiv},
    abstract      = {Version Control Systems (VCS) are frequently used to support development of
                    large-scale software projects. A typical VCS repository of a large project can
                    contain various intertwined branches consisting of a large number of commits. If
                    some kind of unwanted behaviour (e.g. a bug in the code) is found in the
                    project, it is desirable to find the commit that introduced it. Such commit is
                    called a regression point. There are two main issues regarding the regression
                    points. First, detecting whether the project after a certain commit is correct
                    can be very expensive as it may include large-scale testing and/or some other
                    forms of verification. It is thus desirable to minimise the number of such
                    queries. Second, there can be several regression points preceding the actual
                    commit; perhaps a bug was introduced in a certain commit, inadvertently fixed
                    several commits later, and then reintroduced in a yet later commit. In order to
                    fix the actual commit it is usually desirable to find the latest regression
                    point. The currently used distributed VCS contain methods for regression
                    identification, see e.g. the git bisect tool. In this paper, we present a new
                    regression identification algorithm that outperforms the current tools by
                    decreasing the number of validity queries. At the same time, our algorithm tends
                    to find the latest regression points which is a feature that is missing in the
                    state-of-the-art algorithms. The paper provides an experimental evaluation of
                    the proposed algorithm and compares it to the state-of-the-art tool git bisect
                    on a real data set.},
    location      = {Masaryk University, Brno, Czech Republic},
    eprint        = {1708.06623},
    biburl        = {http://dblp.org/rec/bib/journals/corr/abs-1708-06623},
    bibsource     = {dblp computer science bibliography, http://dblp.org},
    url           = {http://arxiv.org/abs/1708.06623},
    urlaccessdate = {2017-11-03},
}

