
%
% Graphical Java application for managing BibTeX and BibLaTeX (.bib) databases (Windows/Linux/MAC)
% https://github.com/JabRef/jabref
%
% BibDesk-like software for Windows
% https://tex.stackexchange.com/questions/9454/bibdesk-like-software-for-windows
%
% LaTeX Bibliography Management
% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#BibTeX
%
%
% $ date "+%Y-%m-%d %H:%M:%S"
% 2017-08-28 09:18:02
%
% YYYY-MM-DD format date in shell script
% https://stackoverflow.com/questions/1401482/yyyy-mm-dd-format-date-in-shell-script
%


@thesis{structuredEditorStudy,
    title         = {Estudo e Criação de um Editor de Código Estruturado},
    author        = {Lucas Boppre Niehues},
    year          = {2013},
    publisher     = {University Library},
    address       = {Florianópolis, Santa Catarina, Brazil},
    type          = {Graduation Thesis submitted to the Computer Science Department},
    institution   = {Federal University of Santa Catarina},
    location      = {Florianópolis, Santa Catarina, Brazil},
    note          = {[Departamento de Informática e Estatística]},
    url           = {https://tcc.inf.ufsc.br},
    urlaccessdate = {2017-03-01},
    abstract      = {Virtually all of the code editors for business use in general\hyp{}purpose
                    languages are oriented to lines and characters. The alternative, editors working
                    directly with the syntactic tree, has received most attention in this area,
                    aiming to have important advantages, such as automatic control of the structure
                    and greater efficiency in the script. The purpose of this paper is to study these
                    alternating active editors, called structured editors or syntax-driven editors,
                    comparing them with traditional editors. Once the advantages and disadvantages
                    have been raised, a structured editor will be developed that best fits in with
                    this medium. This work first provides for an in-depth study of these structured
                    editors, in the context of programming in general or non-general purpose
                    languages. Previous efforts in this area will be sought, assessing their impact,
                    lessons learned and limitations encountered. In a second moment these lessons
                    will be used for the development of a proprietary, fully structured or hybrid
                    editor, in order to increase the efficiency of the programmer. Preferably this
                    editor will be prepared for a standard general purpose language and will be able
                    to edit actual programs. This work aimed to better understand the principles
                    behind structured editors, seeking information on their effectiveness,
                    advantages, disadvantages and factors involved in their lack of adoption. It is
                    also envisaged the construction of a functional structured editor, capable of
                    being used on a daily basis, that makes better use of these advantages.},
}


@thesis{quantumSimulatorChagas,
    title         = {QSystem: simulador quântico para Python},
    author        = {Evandro Chagas Ribeiro da Rosa},
    year          = {2019},
    publisher     = {University Library},
    address       = {Florianópolis, Santa Catarina, Brazil},
    abstract      = {Desenvolvimento de um simulador quântico, inspirado no
    modelo de circuito quântico, para Python.},
    type          = {Graduation Thesis submitted to the Computer Science Department},
    institution   = {Federal University of Santa Catarina},
    location      = {Florianópolis, Santa Catarina, Brazil},
    note          = {[Departamento de Informática e Estatística]},
    url           = {https://tcc.inf.ufsc.br},
    urlaccessdate = {2019-06-28},
}


@thesis{turingMachinesRoyer,
    title         = {MÁQUINAS DE TURING NÃO-DETERMINÍSTICAS COMO COMPUTADORES DE FUNÇÕES},
    author        = {Tiago Royer},
    year          = {2015},
    publisher     = {University Library},
    address       = {Florianópolis, Santa Catarina, Brazil},
    location      = {Florianópolis, Santa Catarina, Brazil},
    note          = {[Departamento de Informática e Estatística]},
    url           = {https://github.com/royertiago/tcc},
    urlaccessdate = {2019-06-20},
    abstract      = {Os conceitos de complexidade de tempo e espaço para
    máquinas de Turing determinísticas e não-determin ísticas, embora
    compartilhem muitos teoremas, costumam ser definidos individualmente. Blum
    (1967, p. 324) define um “recurso computacional” como sendo algo que
    satisfaz dois axiomas; desta forma, podemos tratar da complexidade
    computacional de maneira axiomá- tica e unificar os teoremas que costumam
    ser demonstrados separadamente. Entretanto, Blum (1967, p. 324) não define a
    noção de recurso computacional para decisores, mas sim para computadores de
    funções de inteiros (isto é, máquinas de Turing que computam funções f : N →
    N (HOPCROFT; ULLMAN, 1979, p. 151)). Portanto, para definir as classes P e
    NP em termos dos axiomas de Blum, precisamos interpretar um decisor como um
    computador de uma função de inteiros. Para o caso determinístico é simples,
    pois a máquina determinística possui apenas uma sequência de computação. O
    caso não-determinístico é mais complexo, e é o objeto de estudo deste TCC.
    Pretendo definir o conceito de “função não-determinística” de forma que a
    composição destas funções corresponda, de alguma forma, à noção de
    composição de funções determinísticas.},
}


@thesis{codeComprehensionComparedToOO,
    title         = {An empirical study on code comprehension DCI compared to OO},
    author        = {Héctor Adrián Valdecantos},
    journal       = {Thesis submitted for degree of Master of Science in Software Engineering},
    year          = {2016},
    month         = {08},
    publisher     = {Online},
    address       = {Research Gate},
    location      = {National University of Tucuman, Tucumán Province, Argentina},
    doi           = {10.13140/RG.2.2.29331.48169},
    url           = {https://www.researchgate.net/publication/308314679_An_empirical_study_on_code_comprehension_DCI_compared_to_OO},
    urlaccessdate = {2017-11-02},
    abstract      = {Comprehension of source code affects software development, especially its
                    maintenance where reading code is the most time consuming performed activity. A
                    programming paradigm imposes a style of arranging the source code that is
                    aligned with a way of thinking toward a computable solution. Then, a programming
                    paradigm with a programming language represents an important factor for source
                    code comprehension. Object-Oriented (OO) is the dominant paradigm today.
                    Although, it was criticized from its beginning and recently an alternative has
                    been proposed. In an OO source code, system functions cannot escape outside the
                    definition of classes and their descriptions live inside multiple class
                    declarations. This results in an obfuscated code, a lost sense the run-time, and
                    in a lack of global knowledge that weaken the understandability of the source
                    code at system level. A new paradigm is emerging to address these and other OO
                    issues, this is the Data Context Interaction (DCI) paradigm. We conducted the
                    first human subject related controlled experiment to evaluate the effects of DCI
                    on code comprehension compared to OO. We looked for correctness, time
                    consumption, and focus of attention during comprehension tasks. We also present
                    a novel approach using metrics from Social Network Analysis to analyze what we
                    call the Cognitive Network of Language Elements (CNLE) that is built by
                    programmers while comprehending a system. We consider this approach useful to
                    understand source code properties uncovered from code reading cognitive tasks.
                    The results obtained are preliminary in nature but indicate that DCI approach
                    produces more comprehensible source code and promotes a stronger focus the
                    attention in important files when programmers are reading code during program
                    comprehension. Regarding reading time spent on files, we were not able to
                    indicate with statistical significance which approach allows programmers to
                    consume less time.},
}


@thesis{enhancingLegacySoftwareSystemAnalysis,
    title         = {Enhancing legacy software system analysis by combining
    behavioural and semantic information sources},
    author        = {David Cutting},
    journal       = {Thesis submitted for degree of Doctor of Philosophy},
    year          = {2016},
    month         = {11},
    publisher     = {Online},
    address       = {Research Gate},
    location      = {Queen's University Belfast, Belfast, Northern Ireland},
    doi           = {10.13140/RG.2.2.21231.64160},
    url           = {https://www.researchgate.net/publication/311289366_Enhancing_legacy_software_system_analysis_by_combining_behavioural_and_semantic_information_sources},
    urlaccessdate = {2017-11-02},
    abstract      = {Computer software is, by its very nature highly complex and invisible yet
                    subject to a near-continual pressure to change. Over time the development
                    process has become more mature and less risky. This is in large part due to the
                    concept of software traceability; the ability to relate software components back
                    to their initial requirements and between each other. Such traceability aids
                    tasks such as maintenance by facilitating the prediction of “ripple effects”
                    that may result, and aiding comprehension of software structures in general.
                    Many organisations, however, have large amounts of software for which little or
                    no documentation exists; the original developers are no longer available and yet
                    this software still underpins critical functions. Such “legacy software” can
                    therefore represent a high risk when changes are required. Consequently, large
                    amounts of effort go into attempting to comprehend and understand legacy
                    software. The most common way to accomplish this, given that reading the code
                    directly is hugely time consuming and near-impossible, is to reverse engineer
                    the code, usually to a form of representative projection such as a UML class
                    diagram. Although a wide number of tools and approaches exist, there is no
                    empirical way to compare them or validate new developments. Consequently there
                    was an identified need to define and create the Reverse Engineering to Design
                    Benchmark (RED-BM). This was then applied to a number of industrial tools. The
                    measured performance of these tools varies from 8.8\% to 100\%, demonstrating
                    both the effectiveness of the benchmark and the questionable performance of
                    several tools. In addition to the structural relationships detectable through
                    static reverse engineering, other sources of information are available with the
                    potential to reveal other types of relationships such as semantic links. One
                    such source is the mining of source code repositories which can be analysed to
                    find components within a software system that have, historically, commonly been
                    changed together during the evolution of the system and from the strength of
                    that infer a semantic link. An approach was implemented to mine such semantic
                    relationships from repositories and relationships were found beyond those
                    expressed by static reverse engineering. These included groups of relationships
                    potentially suitable for clustering. To allow for the general use of multiple
                    information sources to build traceability links between software components a
                    uniform approach was defined and illustrated. This includes rules and formulas
                    to allow combination of sources. The uniform approach was implemented in the
                    field of predictive change impact analysis using reverse engineering and
                    repository mining as information sources. This implementation, the Java Code
                    Relationship Anlaysis (jcRA) package, was then evaluated against an industry
                    standard tool, JRipples. Depending on the target, the combined approach is able
                    to outperform JRipples in detecting potential impacts with the risk of
                    over-matching (a high number of false-positives and overall class coverage on
                    some targets).},
}


@book{fundamentalsOfTheoreticalComputerScience,
    author = {Davis, Martin D. and Sigal, Ron and Weyuker, Elaine J.},
    title = {Computability, Complexity, and Languages (2Nd Ed.): Fundamentals of Theoretical Computer Science},
    year = {1994},
    isbn = {0-12-206382-1},
    publisher = {Academic Press Professional, Inc.},
    address = {San Diego, CA, USA},
    abstract = {Computability, Complexity, and Languagesis an introductory text
    that covers the key areas of computer science, including recursive function
    theory, formal languages, and automata. It assumes a minimal background in
    formal mathematics. The book is divided into five parts: Computability,
    Grammars and Automata, Logic, Complexity, and Unsolvability. Computability
    theory is introduced in a manner that makes maximum use of previous
    programming experience, including a "universal" program that takes up less
    than a page. The number of exercises included has more than tripled.
    Automata theory, computational logic, and complexity theory are presented in
    a flexible manner, and can be covered in a variety of different
    arrangements.},
}


@book{antlrBookTerrentParr,
    author = {Parr, Terence},
    title = {The Definitive ANTLR 4 Reference},
    year = {2013},
    isbn = {1934356999, 9781934356999},
    edition = {2nd},
    publisher = {Pragmatic Bookshelf},
    abstract = {Programmers run into parsing problems all the time. Whether it's
    a data format like JSON, a network protocol like SMTP, a server
    configuration file for Apache, a PostScript/PDF file, or a simple
    spreadsheet macro language--ANTLR v4 and this book will demystify the
    process. ANTLR v4 has been rewritten from scratch to make it easier than
    ever to build parsers and the language applications built on top. This
    completely rewritten new edition of the bestselling Definitive ANTLR
    Reference shows you how to take advantage of these new features. Build your
    own languages with ANTLR v4, using ANTLR's new advanced parsing technology.
    In this book, you'll learn how ANTLR automatically builds a data structure
    representing the input (parse tree) and generates code that can walk the
    tree (visitor). You can use that combination to implement data readers,
    language interpreters, and translators. You'll start by learning how to
    identify grammar patterns in language reference manuals and then slowly
    start building increasingly complex grammars. Next, you'll build
    applications based upon those grammars by walking the automatically
    generated parse trees. Then you'll tackle some nasty language problems by
    parsing files containing more than one language (such as XML, Java, and
    Javadoc). You'll also see how to take absolute control over parsing by
    embedding Java actions into the grammar. You'll learn directly from
    well-known parsing expert Terence Parr, the ANTLR creator and project lead.
    You'll master ANTLR grammar construction and learn how to build language
    tools using the built-in parse tree visitor mechanism. The book teaches
    using real-world examples and shows you how to use ANTLR to build such
    things as a data file reader, a JSON to XML translator, an R parser, and a
    Java class-interface extractor. This book is your ticket to becoming a
    parsing guru!What You Need: ANTLR 4.0 and above. Java development tools. Ant
    build system optional (needed for building ANTLR from source)},
}


@book{johnCocke,
    author = {Cocke, John},
    title = {Programming Languages and Their Compilers: Preliminary Notes},
    year = {1969},
    isbn = {B0007F4UOA},
    publisher = {New York University},
    address = {New York, NY, USA},
    abstract = {Our aim in the present volume is to describe the inner
    working of a variety of programming languages, especially from the point of
    view of the compilers which translate these languages from their original
    "source" form into executable machine code. While this aim will of course
    make it necessary for us to describe in somedetail the external form of each
    of the 1angua.ges which we shall study, no more detail will be given than is
    strictly neeessary in order to make it possible for the reader to gain a
    clear view of the machine code forms into which the language will be
    translated and of the problems that a compiler for the language must handle.
    However, interna1 description of the languages studied will be carried
    rather far. Thus the attentive reader of the present work should gain a
    rather good idea of the methods which can be employed to write a compiler
    for a giyen language. On the other hand, he cannot expect to find in this
    book the detailed account af the source conventions for any language whiqh
    he would need to use the language. },
}


@book{ahoTheoryOfParsing,
    author = {Aho, Alfred V. and Ullman, Jeffrey D.},
    title = {The Theory of Parsing, Translation, and Compiling},
    year = {1972},
    isbn = {0-13-914556-7},
    publisher = {Prentice-Hall, Inc.},
    address = {Upper Saddle River, NJ, USA},
    abstract = {This book is intended for a one or two semester course in
    compiling theory at the senior or graduate level. It is a theoretically
    oriented treatment of a practical subject. Our motivation for making it so
    is threefold. (1) In an area as rapidly changing as Computer Science, sound
    pedagogy demands that courses emphasize ideas, rather than implementation
    details. It is our hope that the algorithms and concepts presented in this
    book will survive the next generation of computers and programming
    languages, and that at least some of them will be applicable to fields other
    than compiler writing. (2) Compiler writing has progressed to the point
    where many portions of a compiler can be isolated and subjected to design
    optimization. It is important that appropriate mathematical tools be
    available to the person attempting this optimization. (3) Some of the most
    useful and most efficient compiler algorithms, e.g. LR(k) parsing, require a
    good deal of mathematical background for full understanding. We expect,
    therefore, that a good theoretical background will become essential for the
    compiler designer. While we have not omitted difficult theorems that are
    relevant to compiling, we have tried to make the book as readable as
    possible. Numerous examples are given, each based on a small grammar, rather
    than on the large grammars encountered in practice. It is hoped that these
    examples are sufficient to illustrate the basic ideas, even in cases where
    the theoretical developments are difficult to follow in isolation.
    },
}


@book{computationalComplexityAuroraBarak,
    author = {Arora, Sanjeev and Barak, Boaz},
    title = {Computational Complexity: A Modern Approach},
    year = {2009},
    isbn = {0521424267, 9780521424264},
    edition = {1st},
    publisher = {Cambridge University Press},
    address = {New York, NY, USA},
    abstract = {This beginning graduate textbook describes both recent
    achievements and classical results of computational complexity theory.
    Requiring essentially no background apart from mathematical maturity, the
    book can be used as a reference for self-study for anyone interested in
    complexity, including physicists, mathematicians, and other scientists, as
    well as a textbook for a variety of courses and seminars. More than 300
    exercises are included with a selected hint set.},
}


@book{cormenIntroductionToAlgorithms,
    author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L.
    and Stein, Clifford},
    title = {Introduction to Algorithms, Third Edition},
    year = {2009},
    isbn = {0262033844, 9780262033848},
    edition = {3rd},
    publisher = {The MIT Press},
    abstract = {If you had to buy just one text on algorithms, Introduction to
    Algorithms is a magnificent choice. The book begins by considering the
    mathematical foundations of the analysis of algorithms and maintains this
    mathematical rigor throughout the work. The tools developed in these opening
    sections are then applied to sorting, data structures, graphs, and a variety
    of selected algorithms including computational geometry, string algorithms,
    parallel models of computation, fast Fourier transforms (FFTs), and more.
    This book's strength lies in its encyclopedic range, clear exposition, and
    powerful analysis. Pseudo-code explanation of the algorithms coupled with
    proof of their accuracy makes this book is a great resource on the basic
    tools used to analyze the performance of algorithms.},
}


@book{halliday2013fundamentals,
    title={Fundamentals of Physics Extended, 10th Edition},
    author={Halliday, D. and Resnick, R. and Walker, J.},
    isbn={9781118473818},
    url={https://books.google.com.br/books?id=DTccAAAAQBAJ},
    year={2013},
    publisher={John Wiley \& Sons, Incorporated},
    abstract = {The 10th edition of Halliday's Fundamentals of Physics, Extended
    building upon previous issues by offering several new features and
    additions.  The new edition offers most accurate, extensive and varied set
    of assessment questions of any course management program in addition to all
    questions including some form of question assistance including answer
    specific feedback to facilitate success. The text also offers multimedia
    presentations (videos and animations) of much of the material that provide
    an alternative pathway through the material for those who struggle with
    reading scientific exposition.  Furthermore, the book includes math review
    content in both a self-study module for more in-depth review and also in
    just-in-time math videos for a quick refresher on a specific topic. The
    Halliday content is widely accepted as clear, correct, and complete. The
    end-of-chapters problems are without peer. The new design, which was
    introduced in 9e continues with 10e, making this new edition of Halliday the
    most accessible and reader-friendly book on the market.},
}


@book{dicke1963QuantumPhysicsIntroduction,
    title={Introduction to Quantum Mechanics},
    author={Dicke, R.H. and Wittke, J.P.},
    series={Addison-Wesley world student series},
    url={https://books.google.com.br/books?id=wIhLtAEACAAJ},
    year={1963},
    publisher={Addison-Wesley},
    abstract = {At present, quantum mechanics provides us with the best model we
    have of the physical world and, in particular, of the submicroscopic world of
    the atom. This book is an introduction to the physical concepts and mathematical
    formulations of nonrelativistic quantum mechanics. To get the most profit from
    this book, a familiarity with basic undergraduatelevel physics, including atomic
    physics, electromagnetism, and classical mechanics, is required. A knowledge of
    differential and integral calculus, and some familiarity with differential
    equations, is also needed.},
}


@book{ahoCompilerDragonBook,
    author = {Aho, Alfred V. and Lam, Monica S. and Sethi, Ravi and Ullman,
    Jeffrey D.},
    title = {Compilers: Principles, Techniques, and Tools (2Nd Edition)},
    year = {2006},
    isbn = {0321486811},
    address = {Boston, MA, USA},
    abstract = {Compilers: Principles, Techniques and Tools, known to
    professors, students, and developers worldwide as the "Dragon Book," is
    available in a new edition.  Every chapter has been completely revised to
    reflect developments in software engineering, programming languages, and
    computer architecture that have occurred since 1986, when the last edition
    published.  The authors, recognizing that few readers will ever go on to
    construct a compiler, retain their focus on the broader set of problems
    faced in software design and software development.},
    publisher = {Addison-Wesley Longman Publishing Co., Inc.},
}


@inbook{introductionToContextFreeGrammars,
    author="Paul, Wolfgang J.
    and Baumann, Christoph
    and Lutsyk, Petro
    and Schmaltz, Sabine",
    title="Context-Free Grammars",
    bookTitle="System Architecture: An Ordinary Engineering Discipline",
    year="2016",
    publisher="Springer International Publishing",
    address="Cham",
    pages="159--178",
    abstract="In this chapter the authors introduce context-free grammars, and
    they explain grammars for expressions.",
    isbn="978-3-319-43065-2",
    doi="10.1007/978-3-319-43065-2_10",
    url="https://doi.org/10.1007/978-3-319-43065-2_10"
}


@inbook{sipserBook,
    author = {Sipser, Michael},
    title = {Introduction to the Theory of Computation},
    isbn = {978-1133187790},
    year = {2012},
    url = {http://doi.acm.org/10.1145/230514.571645},
    doi = {10.1145/230514.571645},
    publisher={Pearson Education},
    address = {New York, NY, USA},
    abstract = {Gain a clear understanding of even the most complex, highly
    theoretical computational theory topics in the approachable presentation found
    only in the market-leading INTRODUCTION TO THE THEORY OF COMPUTATION, 3E. The
    number one choice for today's computational theory course, this revision
    continues the book's well-know, approachable style with timely revisions,
    additional practice, and more memorable examples in key areas. A new
    first-of-its-kind theoretical treatment of deterministic context-free languages
    is ideal for a better },
}


@inbook{hopcroftBook,
    author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
    title={Introduction to Automata Theory, Languages, and Computation},
    year = {2001},
    isbn={978-0321455369},
    numpages = {6},
    url = {http://doi.acm.org/10.1145/568438.568455},
    doi = {10.1145/568438.568455},
    year={2006},
    publisher={Pearson Education},
    address = {New York, NY, USA},
    abstract = {This classic book on formal languages, automata theory, and
    computational complexity has been updated to present theoretical concepts in
    a concise and straightforward manner with the increase of hands-on,
    practical applications. This new edition comes with Gradiance, an online
    assessment tool developed for computer science. Gradiance is the most
    advanced online assessment tool developed for the computer science
    discipline. With its innovative underlying technology, Gradiance turns basic
    homework assignments and programming labs into an interactive learning },
}


@inbook{gettingProductive,
    title         = {Getting Productive},
    booktitle     = {Professional Git},
    author        = {Laster, Brent},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {11},
    pages         = {73--97},
    keywords      = {commit message, interactive staging, multiple repositories
    model, partial staging, SHA1, staging area},
    location      = {Cary, North Carolina, USA},
    doi           = {10.1002/9781119285021.ch5},
    isbn          = {9781119285021},
    url           = {https://www.researchgate.net/publication/311666005_Getting_Productive},
    urlaccessdate = {2017-11-02},
    abstract      = {This chapter explains how people can work with help in Git, understand the
                    multiple repositories model, and stage files. Git includes two different forms
                    of help: an abbreviated version and a full version. Partial and interactive
                    staging, committing files into the local repository, and writing good commit
                    messages are also discussed. The chapter also elucidates concepts such as SHA1,
                    options for staging files, and forming good commit messages. Although working
                    with multiple repositories at the same time is common in Git, it is a different
                    way of working for most people. Git commit includes a --verbose option. The
                    first time this option is used on the command line, it results in the diff
                    output between the staging area and the local repository being included. The
                    chapter also talks about ways to amend commits and use some advanced techniques
                    such as commit message template files to improve commit messages.},
}


@inbook{trackingChanges,
    title         = {Tracking Changes},
    booktitle     = {Professional Git},
    author        = {Laster, Brent},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {11},
    pages         = {105-125},
    location      = {Cary, North Carolina, USA},
    doi           = {10.1002/9781119285021.ch6},
    isbn          = {9781119285021},
    url           = {https://www.researchgate.net/publication/311666333_Tracking_Changes},
    urlaccessdate = {2017-11-06},
    abstract      = {This chapter considers different versions of files at the different levels in
                    Git to keep track of where everything is and how the versions at the different
                    levels may differ from each other. It explains ways to keep track of all of the
                    work that's in progress. Git has two commands that can help with this: status
                    and diff. Using these two commands users can quickly understand the state of
                    their changes in the local environment and ensure that the correct changes are
                    tracked and stored in Git. For files that are in the working directory or
                    staging area, the status command answers three questions: whether or not a file
                    is tracked, what is in the staging area, and whether or not a file is modified.
                    Git can report the status of untracked files in a couple of different ways,
                    depending on whether or not something is staged.},
}


@inbook{commandLineInterface,
    title         = {The Command Line Interface},
    author        = {Adam B. Singer},
    booktitle     = {Practical C++ Design},
    year          = {2017},
    pages         = {97-113},
    edition       = {1},
    publisher     = {Apress},
    address       = {Berkeley, CA},
    isbn          = {978-1-4842-3056-5},
    doi           = {10.1007/978-1-4842-3057-2\_5},
    location      = {Online},
    url           = {https://www.researchgate.net/publication/320120365_The_Command_Line_Interface},
    urlaccessdate = {2017-10-10},
    abstract      = {This is a very exciting chapter. While command line interfaces (CLIs) may not
                    have the cachet of modern graphical user interfaces (GUIs), especially those of
                    phones or tablets, the CLI is still a remarkably useful and effective user
                    interface. This chapter details the design and implementation of the command
                    line interface for pdCalc. By the end of this chapter, we will, for the first
                    time, have a functioning (albeit feature incomplete) calculator, which is a
                    significant milestone in our development.},
}


@inbook{automaticSynthesis,
    title         = {Towards Automatic Synthesis of Software Verification Tools},
    author        = {Rybalchenko, Andrey},
    editor        = {Donaldson, Alastair and Parker, David},
    bookTitle     = {Model Checking Software: 19th International Workshop, SPIN
    2012, Oxford, UK, July 23-24, 2012. Proceedings},
    year          = {2012},
    volume        = {7385},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {22--22},
    location      = {Technische Universität München, Germany},
    doi           = {10.1007/978-3-642-31759-0\_3},
    isbn          = {978-3-642-31759-0},
    url           = {https://link.springer.com/chapter/10.1007/978-3-642-31759-0_3},
    urlaccessdate = {2017-10-30},
    abstract      = {Software complexity is growing, so is the demand for software verification. Soon,
                    perhaps within a decade, wide deployment of software verification tools will be
                    indispensable or even mandatory to ensure software reliability in a large number of
                    application domains, including but not restricted to safety and security critical
                    systems. To adequately respond to the demand we need to eliminate tedious aspects of
                    software verifier development, while providing support for the accomplishment of
                    creative aspects.},
}


@inbook{debuggingIntoExamples,
    title         = {Debugging into Examples},
    author        = {Steinert, Bastian and Perscheid, Michael and Beck, Martin
    and Lincke, Jens and Hirschfeld, Robert},
    bookTitle     = {Testing of Software and Communication Systems: 21st IFIP WG
    6.1 International Conference , November 2-4, 2009. Proceedings},
    year          = {2009},
    pages         = {235--240},
    publisher     = {Springer},
    address       = {Berlin, Heidelberg},
    location      = {Eindhoven, The Netherlands},
    isbn          = {978-3-642-05031-2},
    doi           = {10.1007/978-3-642-05031-2_18},
    url           = {https://www.researchgate.net/publication/221047094_Debugging_into_Examples},
    urlaccessdate = {2017-10-31},
    abstract      = {Enhancing and maintaining a complex software system requires detailed
                    understanding of the underlying source code. Gaining this understanding by
                    reading source code is difficult. Since software systems are inherently dynamic,
                    it is complex and time consuming to imagine, for example, the effects of a
                    method's source code at run-time. The inspection of software systems during
                    execution, as encouraged by debugging tools, contributes to source code
                    comprehension. Leveraged by test cases as entry points, we want to make it easy
                    for developers to experience selected execution paths in their code by debugging
                    into examples. We show how links between test cases and application code can be
                    established by means of dynamic analysis while executing regular tests.},
}


@inbook{aPrettyGoodFormatting,
    title         = {A Pretty Good Formatting Pipeline},
    author        = {Bagge, Anya Helee and Hasu, Tero},
    editor        = {Erwig, Martn and Paige, Richard F. and Van Wyk, Eric},
    bookTitle     = {Software Language Engineering: 6th International
    Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013.
    Proceedings},
    year          = {2013},
    month         = {10},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {177--196},
    location      = {Bergen Language Design Laboratory, Dept. of Informatics, University of Bergen, Norway},
    isbn          = {978-3-319-02654-1},
    doi           = {10.1007/978-3-319-02654-1_10},
    url           = {https://www.researchgate.net/publication/300351526_A_Pretty_Good_Formatting_Pipeline},
    urlaccessdate = {2017-10-31},
    abstract      = {Proper formatting makes the structure of a program apparent and aids program
                    comprehension. The need to format code arises in code generation and
                    transformation, as well as in normal reading and editing situations. Commonly
                    used pretty-printing tools in transformation frameworks provide an easy way to
                    produce indented code that is fairly readable for humans, without reaching the
                    level of purpose-built reformatting tools, such as those built into IDEs. This
                    paper presents a library of pluggable components, built to support style-based
                    formatting and reformatting of code, and to enable further experimentation with
                    code formatting.},
}


@inbook{learningSupportSystem,
    title         = {Development of a Learning Support System for Source Code Reading Comprehension},
    author        = {Arai, Tatsuya and Kanamori, Haruki and Tomoto, Takahito and
    Kometani, Yusuke and Akakura, Takako"},
    editor        = {Yamamoto, Sakae},
    bookTitle     = {Human Interface and the Management of Information, 16th International Conference},
    year          = {2014},
    month         = {06},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {12--19},
    location      = {Heraklion, Crete, Greece},
    isbn          = {978-3-319-07863-2},
    doi           = {10.1007/978-3-319-07863-2_2},
    url           = {https://www.researchgate.net/publication/295290682_Development_of_a_Learning_Support_System_for_Source_Code_Reading_Comprehension},
    urlaccessdate = {2017-11-01},
    abstract      = {In this paper, we describe the development of a support system that facilitates
                    the process of learning computer programming through the reading of computer
                    program source code. Reading code consists of two steps: reading comprehension
                    and meaning deduction. In this study, we developed a tool that supports the
                    comprehension of a program's reading. The tool is equipped with an error
                    visualization function that illustrates a learner's mistakes and makes them
                    aware of their errors. We conducted experiments using the learning support tool
                    and confirmed that the system is effective.},
}


@inbook{howNovicesRead,
    title         = {How Novices Read Source Code in Introductory Courses on Programming: An Eye-Tracking Experiment},
    author        = {Yenigalla, Leelakrishna and Sinha, Vinayak and Sharif, Bonita and Crosby, Martha},
    editor        = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
    bookTitle     = {Foundations of Augmented Cognition: Neuroergonomics and Operational Neuroscience: 10th International Conference, Proceedings, Part II},
    year          = {2016},
    month         = {07},
    publisher     = {Springer-Verlag},
    address       = {New York, NY, USA},
    pages         = {120--131},
    location      = {Toronto, ON, Canada},
    keywords      = {Eye tracking study, Novices, Program comprehension},
    isbn          = {978-3-319-39952-2},
    doi           = {10.1007/978-3-319-39952-2_13},
    url           = {https://www.researchgate.net/publication/304190149_How_Novices_Read_Source_Code_in_Introductory_Courses_on_Programming_An_Eye-Tracking_Experiment},
    urlaccessdate = {2017-11-02},
    abstract      = {We present an empirical study using eye tracking equipment to understand how
                    novices read source code in the context of two introductory programming classes.
                    Our main goal is to begin to understand how novices read source code and to
                    determine if we see any improvement in program comprehension as the course
                    progresses. The results indicate that novices put in more effort and had more
                    difficulty reading source code as they progress through the course. However,
                    they are able to partially comprehend code at a later point in the course. The
                    relationship between fixation counts and durations is linear but shows more
                    clusters later in the course, indicating groups of students that learned at the
                    same pace. The results also show that we did not see any significant shift in
                    learning (indicated by the eye tracking metrics) during the course, indicating
                    that there might be more than one course that needs to be taken over the course
                    of a few years to realize the significance of the shift. We call for more
                    studies over a student's undergraduate years to further learn about this
                    shift.},
}


@inbook{usingSourceControl,
    title         = {Using Source Control},
    author        = {Yener, Murat and Dundar, Onur},
    booktitle     = {Expert Android Studio},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {10},
    pages         = {245--279},
    location      = {San Jose, California},
    keywords      = {Android developers, Android Studio, Git repository, GitHub, source control system},
    doi           = {10.1002/9781119419310.ch9},
    isbn          = {9781119419310},
    url           = {https://www.researchgate.net/publication/316657029_Using_Source_Control},
    urlaccessdate = {2017-11-03},
    abstract      = {Git is currently the most widely accepted source control system among Android
                    developers, and has built-in support for Android Studio. This chapter covers how
                    to create a Git repository, add files to it, and perform commits. Unlike other
                    systems that just watch changes in the file system and commit changes to the
                    version control server, Git runs on a client computer and changes need to be
                    committed locally first. This way, a developer can revert any change/branch or
                    version locally through Git. The local Git can push the set of changes committed
                    to network Git servers. Android Studio comes with Git support. However, one may
                    still need to install Git to be able to use it through the command line. GitHub
                    is a popular Git-based project-hosting site that offers free hosting for public
                    repositories. One reason for GitHub's popularity is the available easy-to-use
                    tools for Git.},
}


@inbook{continuousIntegration,
    title         = {Continuous Integration},
    author        = {Yener, Murat and Dundar, Onur},
    booktitle     = {Expert Android Studio},
    publisher     = {John Wiley \& Sons, Inc.},
    address       = {Hoboken, New Jersey, USA},
    year          = {2016},
    month         = {10},
    pages         = {281--307},
    location      = {San Jose, California},
    keywords      = {Android developers, Android Studio, Git repository, GitHub, source control system},
    doi           = {10.1002/9781119419310.ch9},
    isbn          = {9781119419310},
    url           = {https://www.researchgate.net/publication/316657029_Using_Source_Control},
    urlaccessdate = {2017-11-03},
    abstract      = {This chapter focuses on continuous integration (CI) servers that act as the
                    cement between all other processes and convert them into an automated life
                    cycle. It explains how to download and install CI server. The chapter explores
                    how to set up a build job from a Git repository, how to trigger a build cycle on
                    every commit, and how to publish one's app automatically to Google Play. CI
                    servers are very flexible and easy to integrate and can handle Android projects
                    that use make files, Maven, or Gradle. One needs to choose one of those to fully
                    integrate his/her project with a CI server. Version control systems are another
                    crucial part of the CI process. Each code commit triggers a build process that
                    results in compilation, running tests, and packaging the app on the CI server.
                    The chapter also focuses on installing Jenkins.},
}


@online{areContextSensitiveGrammarWithPolynomialTime,
    TITLE = {Are Context Sensitive Grammar with Polynomial Complexity Time?},
    AUTHOR = {Reinier Post},
    HOWPUBLISHED = {Computer Science Stack Exchange},
    URL = {https://cs.stackexchange.com/q/111573},
    urlaccessdate = {2019-07-08},
    abstract = {Assuming a Deterministic Parser is the one which can parse
    unambiguous grammars in something like linear time complexity, while a
    NonDeterministic Parser can parse any grammar in some worst time complexity
    like O(n5) or exponential. Parsing context-sensitive grammars is
    PSPACE-complete, so it takes exponential time in practice; it takes only
    polynomial time for growing context-sensitive grammars.},
}


@online{grammarsAndTheMinimalParsers,
    TITLE = {What about theses grammars and the minimal parser to recognize it?},
    AUTHOR = {Maggie Johnson and Julie Zelenski},
    HOWPUBLISHED = {Stack Overflow Stack Exchange},
    URL = {https://stackoverflow.com/questions/6379937},
    year = {2009},
    urlaccessdate = {2019-07-20},
    abstract = {I'm trying to learn how to make a compiler. In order to do so, I
    read a lot about context-free language. But there are some things I cannot
    get by myself yet. Since it's my first compiler there are some practices
    that I'm not aware of. My questions are asked with in mind to build a parser
    generator, not a compiler neither a lexer. Some questions may be obvious.
    Among my reads are : Bottom-Up Parsing, Top-Down Parsing, Formal Grammars.
    The picture shown comes from : Miscellanous Parsing. All coming from the
    Stanford CS143 class.},
}


@online{deathToTheSpaceInfidels,
    title         = {Death to the Space Infidels!},
    author        = {Jeff Atwood},
    year          = {2007},
    location      = {Online},
    url           = {http://www.codinghorror.com/blog/2009/04/death-to-the-space-infidels.html},
    urlaccessdate = {2017-03-01},
    abstract      = {Ah, spring. What a wonderful time of year. A time when young programmers' minds
                    turn to thoughts of ... never ending last-man-standing filibuster arguments
                    about code formatting. Naturally. And there is no argument more evergreen than
                    the timeless debate between tabs and spaces.},
}


@online{theTidyverseStyleGuide,
    title         = {The tidyverse style guide},
    author        = {Hadley Wickham},
    year          = {2017},
    location      = {Online},
    url           = {https://style.tidyverse.org/},
    urlaccessdate = {2019-07-09},
    abstract      = {Good coding style is like correct punctuation: you can
    manage without it, butitsuremakesthingseasiertoread. Just as with
    punctuation, while there are many code styles to choose from, some are more
    reader-friendly than others. The style presented here, which is used
    throughout the tidyverse, is derived from Google’s R style guide and has
    evolved considerably over the years.},
}


@online{rustSublimeTextSyntaxSyntec,
    title         = {Rust library for syntax highlighting using Sublime Text syntax definitions},
    author        = {Tristan Hume},
    year          = {2016},
    location      = {Online},
    url           = {https://github.com/trishume/syntect},
    urlaccessdate = {2019-09-16},
    abstract      = {Syntect is a syntax highlighting library for Rust that uses
    Sublime Text syntax definitions. It aims to be a good solution for any Rust
    project that needs syntax highlighting, including deep integration with text
    editors written in Rust. It's used in production by at least two companies,
    and by many open source projects. If you are writing a text editor (or
    something else needing highlighting) in Rust and this library doesn't fit
    your needs, I consider that a bug and you should file an issue or email me.
    I consider this project mostly complete, I still maintain it and review PRs,
    but it's not under heavy development.},
}


@online{uncrustifySourceCode,
    title         = {Uncrustify -- A source code beautifier for C, C++, C\#, ObjectiveC, D, Java, Pawn and VALA},
    author        = {André Berg and Ben Gardner and Guy Maurel},
    year          = {2005},
    location      = {Online},
    url           = {https://github.com/uncrustify/uncrustify},
    urlaccessdate = {2019-07-09},
    abstract      = {The goals of this project are simple: Create a highly
    configurable, easily modifiable source code beautifier. Features: 1. Indent
    code, aligning on parens, assignments, etc, 2. Align on '=' and variable
    definitions, 3. Align structure initializers, 4. Align \#define stuff, 5.
    Align backslash-newline stuff, 6. Reformat comments (a little bit), 7. Fix
    inter-character spacing, 8. Add or remove parens on return statements, 9.
    Add or remove braces on single-statement if/do/while/for statements, 10.
    Supports embedded SQL 'EXEC SQL' stuff, 11. Highly configurable - 671
    configurable options as of version 0.69.0.},
}


@online{uncrustifyWebSite,
    title         = {Uncrustify},
    author        = {André Berg and Ben Gardner and Guy Maurel},
    year          = {2005},
    location      = {Online},
    url           = {http://uncrustify.sourceforge.net/},
    urlaccessdate = {2019-07-26},
    abstract      = {The goals of this project are simple: Create a highly
    configurable, easily modifiable source code beautifier. Features: 1. Indent
    code, aligning on parens, assignments, etc, 2. Align on '=' and variable
    definitions, 3. Align structure initializers, 4. Align \#define stuff, 5.
    Align backslash-newline stuff, 6. Reformat comments (a little bit), 7. Fix
    inter-character spacing, 8. Add or remove parens on return statements, 9.
    Add or remove braces on single-statement if/do/while/for statements, 10.
    Supports embedded SQL 'EXEC SQL' stuff, 11. Highly configurable - 671
    configurable options as of version 0.69.0.},
}


@online{obfuscationWikipedia,
    title         = {Obfuscation (software)},
    author        = {Wikipedia contributors},
    year          = {2008},
    location      = {Online},
    url           = {https://en.wikipedia.org/w/index.php?title=Obfuscation_(software)&oldid=905604987},
    urlaccessdate = {2019-07-26},
    abstract      = {In software development, obfuscation is the deliberate act
    of creating source or machine code that is difficult for humans to
    understand. Like obfuscation in natural language, it may use needlessly
    roundabout expressions to compose statements. Programmers may deliberately
    obfuscate code to conceal its purpose (security through obscurity) or its
    logic or implicit values embedded in it, primarily, in order to prevent
    tampering, deter reverse engineering, or even as a puzzle or recreational
    challenge for someone reading the source code. This can be done manually or
    by using an automated tool, the latter being the preferred technique in
    industry.},
}


@online{whoWroteThisCrap,
    title         = {Who Wrote This Crap?},
    author        = {Jeff Atwood},
    year          = {2009},
    location      = {Online},
    url           = {https://blog.codinghorror.com/who-wrote-this-crap/},
    urlaccessdate = {2017-03-01},
    abstract      = {I first read this in the original 1993 edition of Code
    Complete. It's quoted from a much earlier book, Stan Kelley-Bootle's The
    Devil's Dp Dictionary, which was published in 1981. It's still true, more
    than 25 years later. There's a knee-jerk predisposition to look at code you
    didn't write, and for various reasons large and small, proclaim it absolute
    crap. But figuring out who's actually responsible for that crappy code takes
    some detective work.},
}


@online{pep8operatorsBlankLine,
    title         = {Should a Line Break Before or After a Binary Operator?},
    author        = {Guido van Rossum and Barry Warsaw and Nick Coghlan},
    year          = {2001},
    location      = {Online},
    url           = {https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator},
    urlaccessdate = {2019-07-08},
    abstract      = {For decades the recommended style was to break after binary
    operators. But this can hurt readability in two ways: the operators tend to
    get scattered across different columns on the screen, and each operator is
    moved away from its operand and onto the previous line. Here, the eye has to
    do extra work to tell which items are added and which are subtracted.},
}


@online{whyVersionControlIsCritical,
    title         = {Why Version Control Is Critical To Your Success},
    author        = {Ilya Olevsky},
    year          = {2013},
    location      = {Online},
    url           = {https://web.archive.org/web/20180423062356/http://www.codeservedcold.com/version-control-importance/},
    urlaccessdate = {2019-07-04},
    abstract      = {They are known by many names: verison control, source
    control, revision control. Regardless of how you call it, a version control
    tool is critical to the success of your software project. In its most basic
    form, a version control system is a tool that allows you to store a
    modification history of all your code. This concept is used in many other
    systems including Wikis and incremental backup solutions. Version control
    tools for software go far beyond a simple history of code modifications,
    however. They increase productivity, allow for concurrent development of
    large projects, maintain multiple versions (branches) of the same project,
    and much more.},
}


@online{llContainmentInLalr,
    TITLE = {Is there a LL(K) Grammar which is not LALR(K) Grammar?},
    AUTHOR = {Stack Rici},
    year = {2019},
    urlaccessdate = {2019-06-1},
    HOWPUBLISHED = {Computer Science Stack Exchange},
    location = {Online},
    URL = {https://cs.stackexchange.com/q/110775},
    abstract = {It is easy to know that there are LALR(K) grammars which are not
    LL(K) because any grammar with left recursion which is LALR(K), is not LL(K)
    because all LL(K) grammar must be left recursion free. And the opposite? Is
    there a LL(K) Grammar which is not LALR(K) Grammar? Do you have an
    example?},
}


@online{cykParsingAlgorithm,
    TITLE = {Can CYK Parsing algorithm generate the parsing tree in $O(n^3)$?},
    AUTHOR = {Stack Rici},
    location = {Online},
    year = {2019},
    HOWPUBLISHED = {Computer Science Stack Exchange},
    NOTE = {URL:https://cs.stackexchange.com/q/111565},
    EPRINT = {https://cs.stackexchange.com/q/111565},
    URL = {https://cs.stackexchange.com/q/111565},
    urlaccessdate = {2019-07-06},
    abstract = {You can build a parse forest in $O(n^3)$ time and space. The
    forest represents all parse trees, even an infinite number of parse trees,
    because it is a graph, not a tree. From a parse forest, it is possible to
    produce a single parse tree in time linear to the size of the forest, and it
    is possible to iterate through all possible parses. So if you either
    consider that "build a parse tree" means "find one of the possible parses"
    or if you consider that it means "build a datastructure which represents all
    possible parses", then you can certainly do that with a CYK grammar in
    $O(n^3)$. If, on the other hand, you intend "build a parse tree" to mean
    "build all parse trees for a sentence", then it is not limited to
    exponential time; it is easy to write a grammar with an infinite number of
    possible parses for certain sentences.},
}


@online{llVersusLrContainment,
    TITLE = {Language theoretic comparison of LL and LR grammars},
    AUTHOR = {Alex ten Brink},
    year = {2013},
    urlaccessdate = {2019-06-1},
    HOWPUBLISHED = {Computer Science Stack Exchange},
    location = {Online},
    URL = {https://cs.stackexchange.com/q/43},
    abstract = {People often say that LR(k) parsers are more powerful than LL(k)
    parsers. These statements are vague most of the time; in particular, should
    we compare the classes for a fixed k or the union over all k? So how is the
    situation really? In particular, I am interested in how LL(*) fits in.},
}


@online{eclisePeriodSettings,
    TITLE = {How can I get eclipse to wrap lines after a period instead of before},
    AUTHOR = {C Stephen},
    year = {2017},
    urlaccessdate = {2019-06-26},
    HOWPUBLISHED = {Stack Overflow Stack Exchange},
    location = {Online},
    URL = {https://stackoverflow.com/questions/31438377/},
    abstract = {I've looked throughout Preferences -> Java -> Code Style ->
    Formatter and can't find any way to get eclipse to format my code breaking
    lines after a . ... apart from modifying the formatter code itself. (And
    that means you are not running the standard formatter anymore!)},
}


@online{uncrustifyNotMeetingCodingStandards,
    TITLE = {Unable to meet our code style and other issues},
    AUTHOR = {S Maxdax},
    year = {2019},
    urlaccessdate = {2019-07-10},
    HOWPUBLISHED = {GitHub Issue Tracker},
    location = {Online},
    URL = {https://github.com/uncrustify/uncrustify/issues/2338},
    abstract = {We are currently desperate, because we are not able to meet our
    code style by our current uncrustify configuration. We already spent plenty
    hours setup our configuration correctly, but there are still issues we are
    not able to fix. To show our issues I prepared a code example that shows our
    wanted code style where I marked the issues by a comment.},
}


@online{Geukens,
    title         = {Is imposing the same code format for all developers a good idea?},
    author        = {Stijn Geukens},
    year          = {2013},
    location      = {Online},
    url           = {https://softwareengineering.stackexchange.com/questions/189274/is-imposing-the-same-code-format-for-all-developers-a-good-idea},
    urlaccessdate = {2017-03-01},
    abstract      = {We are considering to impose a single standard code format in our project (auto
                    format with save actions in Eclipse). The reason is that currently there is a
                    big difference in the code formats used by several (>10) developers which makes
                    it harder for one developer to work on the code of another developer. The same
                    Java file sometimes uses 3 different formats.},
}


@online{universalIndentGUI,
    title         = {Powerful code indenter front-end, UniversalIndentGUI},
    author        = {Thomas Schweitzer},
    publisher     = {Online Material},
    year          = {2006},
    location      = {Online},
    url           = {https://github.com/danblakemore/universal-indent-gui},
    note          = {\url{http://universalindent.sourceforge.net/index.php}},
    urlaccessdate = {2017-03-01},
    abstract      = {Ever concerned about how your code looks like? Ever heard of different
                    indenting styles, for example K\&R? Ever received code from someone else who
                    didn't care about code formatting? Ever tried to configure a code indenter to
                    convert such code to your coding style? Ever got bored by that tedious "changing
                    a parameter"-"call the indeter"-"try and error" procedure? Help is close to you.
                    UniversalIndentGUI offers a live preview for setting the parameters of nearly
                    any indenter. You change the value of a parameter and directly see how your
                    reformatted code will look like. Save your beauty looking code or create an
                    anywhere usable batch/shell script to reformat whole directories or just one
                    file even out of the editor of your choice that supports external tool calls.},
}


@online{UniversalIndentGUIScreenshot4,
    title         = {UniversalIndentGUI},
    author        = {Thomas Schweitzer},
    publisher     = {Online Material},
    year          = {2007},
    month         = {07},
    day           = {26},
    location      = {Online},
    url           = {http://universalindent.sourceforge.net/images/screenshot4.png},
    note          = {\url{http://universalindent.sourceforge.net/screenshots.php}},
    urlaccessdate = {2019-07-31},
}


@online{editorConfig,
    title         = {Editor Config},
    author        = {Trey Hunner and Hong Xu},
    year          = {2012},
    location      = {Online},
    url           = {https://github.com/editorconfig/editorconfig},
    urlaccessdate = {2019-07-23},
    abstract      = {EditorConfig helps maintain consistent coding styles for
    multiple developers working on the same project across various editors and
    IDEs. The EditorConfig project consists of a file format for defining coding
    styles and a collection of text editor plugins that enable editors to read
    the file format and adhere to defined styles. EditorConfig files are easily
    readable and they work nicely with version control systems.},
}


@online{sublimeTextSyntax,
    title         = {SUBLIME TEXT 3 DOCUMENTATION, Syntax Definitions},
    author        = {Jon Skinner},
    year          = {2015},
    location      = {Online},
    url           = {https://www.sublimetext.com/docs/3/syntax.html},
    urlaccessdate = {2017-03-01},
    abstract      = {Sublime Text can use both .sublime-syntax and .tmLanguage files for syntax
                    highlighting. This document describes .sublime-syntax files. Sublime Syntax
                    files are YAML files with a small header, followed by a list of contexts. Each
                    context has a list of patterns that describe how to highlight text in that
                    context, and how to change the current text.},
}


@online{sublimeTextScopeNaming,
    title         = {SUBLIME TEXT 3 DOCUMENTATION, Scope Naming},
    author        = {Jon Skinner},
    year          = {2015},
    location      = {Online},
    url           = {http://www.sublimetext.com/docs/3/scope_naming.html},
    urlaccessdate = {2019-09-17},
    abstract      = {Syntax definitions and color schemes in Sublime Text
    interact through the use of scope names. Scopes are dotted strings,
    specified from least-to-most specific. For example, the if keyword in PHP
    could be specified via the scope name keyword.control.php. Sublime Text
    supports TextMate language grammars, and inherited its default syntaxes from
    various open-source bundles. The TextMate language grammar documentation
    provided a base set of scope names that have been slowly expanded and
    changed by the community.},
}


@online{vsCodeSyntaxHighlighthing,
    title         = {Optimizations in Syntax Highlighting},
    author        = {Alexandru Dima},
    year          = {2017},
    location      = {Online},
    url           = {https://code.visualstudio.com/blogs/2017/02/08/syntax-highlighting-optimizations},
    urlaccessdate = {2019-09-16},
    abstract      = {Visual Studio Code version 1.9 includes a cool performance
    improvement that we've been working on and I wanted to tell its story. TL;DR
    TextMate themes will look more like their authors intended in VS Code 1.9,
    while being rendered faster and with less memory consumption. Syntax
    Highlighting usually consists of two phases. Tokens are assigned to source
    code, and then they are targeted by a theme, assigned colors, and voilà,
    your source code is rendered with colors. It is the one feature that turns a
    text editor into a code editor. Tokenization in VS Code (and in the Monaco
    Editor) runs line-by-line, from top to bottom, in a single pass. A tokenizer
    can store some state at the end of a tokenized line, which will be passed
    back when tokenizing the next line. This is a technique used by many
    tokenization engines, including TextMate grammars, that allows an editor to
    retokenize only a small subset of the lines when the user makes edits.},
}


@online{prettyPrinter,
    title         = {prettyprinter.de},
    author        = {J.M.},
    year          = {2017},
    location      = {Online},
    url           = {http://prettyprinter.de/},
    urlaccessdate = {2017-03-01},
    abstract      = {This is a source code beautifier (source code formatter), similiar to indent.
                    Please make a backup before you replace your code!},
}


@online{tabsAndSpacesConversion,
    title         = {How to replace spaces with tabs when pasting on a view},
    author        = {Evandro Coan},
    year          = {2017},
    location      = {Online},
    url           = {https://forum.sublimetext.com/t/how-to-replace-spaces-with-tabs-when-pasting-on-a-view-with-translate-tabs-to-spaces-set-to-false/32193},
    urlaccessdate = {2017-09-29},
    abstract      = {The problem is that I will certainly not notice when I paste something indented
                    with spaces instead of tabs. This is problem because for some file types as
                    .sublime-settings files (or a Makefile), which has the setting
                    translate\_tabs\_to\_spaces set to false, so I would expect to all
                    .sublime-settings files to be indented with tabs, not spaces.},
}


@online{synchronizingFolders,
    title         = {Synchronizing folders with rsync},
    author        = {Juan Valencia Escalante},
    year          = {2017},
    location      = {Online},
    url           = {http://www.jveweb.net/en/archives/2010/11/synchronizing-folders-with-rsync.html},
    urlaccessdate = {2017-10-10},
    abstract      = {In this post I cover the basics of rsync, in preparation for a subsequent post
                    that will cover backups and it's use in conjunction with cronjobs to automatize
                    the backup process. From the copying and synchronization of local files and
                    folders, to it's use for transfer information among computers. Its use as a
                    daemon when SSH is unavailable was moved to it's own section.},
}


@online{whyCcannotBeParsedWithALR1Parser,
    title         = {Why can't C++ be parsed with a LR(1) parser?},
    author        = {Ira Baxter},
    year          = {2009},
    location      = {Online},
    url           = {https://stackoverflow.com/questions/243383/why-cant-c-be-parsed-with-a-lr1-parser},
    HOWPUBLISHED  = {StackOverflow Stack Exchange},
    urlaccessdate = {2019-06-06},
    abstract      = {LR parsers can't handle ambiguous grammar rules, by design.
                    (Made the theory easier back in the 1970s when the ideas
                    were being worked out).},
}


@online{turingCompleteRegularLanguages,
    title         = {Can regular languages be Turing complete?},
    author        = {Eugene Kirpichov},
    year          = {2014},
    location      = {Online},
    url           = {https://cs.stackexchange.com/questions/33666},
    HOWPUBLISHED  = {Computer Science Stack Exchange},
    urlaccessdate = {2019-06-13},
    abstract      = {While the set of legal programs in Jot is regular, Jot
    itself is Turing-complete. That means that each computable function can be
    expressed in Jot. We can even come up with a language in which all binary
    strings are legal, but the language itself is Turing complete (exercise).
    You're confusing syntax and semantics.},
}


@online{finiteAutomataTuringComplete,
    TITLE = {Are Finite Automata Turing Complete?},
    AUTHOR = {Curtis Fenner},
    URL = {https://cs.stackexchange.com/q/110998},
    HOWPUBLISHED = {Computer Science Stack Exchange},
    urlaccessdate = {2019-06-22},
    abstract = {Something is Turing Complete if it can be used to simulate any
    Turing Machine. So, can a Finite Automaton simulate a Turing Machine? On the
    question Can regular languages be Turing complete? they say a Regular
    Language can be Turing Complete, but it does not make sense to me. I am not
    talking about the Language being parsed, but the Finite Automaton itself.},
}


@online{larkContextualLexer,
    author = {Erez Shinan},
    url = {https://lark-parser.readthedocs.io/en/latest/parsers/},
    publisher = {Online},
    year = {2014},
    urlaccessdate = {2019-06-30},
    title = {Lark implements the following parsing algorithms: Earley, LALR(1), and CYK},
    abstract = {Lark extends the traditional YACC-based architecture with a
    contextual lexer, which automatically provides feedback from the parser to
    the lexer, making the LALR(1) algorithm stronger than ever. The contextual
    lexer communicates with the parser, and uses the parser's lookahead
    prediction to narrow its choice of tokens. So at each point, the lexer only
    matches the subgroup of terminals that are legal at that parser state,
    instead of all of the terminals. It’s surprisingly effective at resolving
    common terminal collisions, and allows to parse languages that LALR(1) was
    previously incapable of parsing. This is an improvement to LALR(1) that is
    unique to Lark.},
}


@online{familyOfSourceCodeObfuscators,
    title = {Thicket™ Family of Source Code Obfuscators},
    author = {Semantic Designs},
    url = {http://www.semdesigns.com/Products/Obfuscators/index.html},
    publisher = {Online},
    year = {2003},
    urlaccessdate = {2019-07-24},
    abstract = {A source code obfuscator accepts a program source file, and
    generates another functionally equivalent source file which is much harder
    to understand or reverse-engineer. This is useful for technical protection
    of intellectual property when: source code must be delivered for public
    execution purposes (with interpretive languages like ECMAScript in web
    pages)},
}


@inproceedings{codeObfuscationTechniques,
    author = {Ceccato, Mariano and Di Penta, Massimiliano and Nagra, Jasvir and Falcarin, Paolo and Ricca, Filippo and Torchiano, Marco and Tonella, Paolo},
    title = {Towards Experimental Evaluation of Code Obfuscation Techniques},
    booktitle = {Proceedings of the 4th ACM Workshop on Quality of Protection},
    series = {QoP '08},
    year = {2008},
    isbn = {978-1-60558-321-1},
    location = {Alexandria, Virginia, USA},
    pages = {39--46},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/1456362.1456371},
    doi = {10.1145/1456362.1456371},
    acmid = {1456371},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {empirical studies, software obfuscation},
    abstract = {While many obfuscation schemes proposed, none of them satisfy
    any strong definition of obfuscation. Furthermore secure generalpurpose
    obfuscation algorithms have been proven to be impossible. Nevertheless,
    obfuscation schemes which in practice slow down malicious
    reverse-engineering by obstructing code comprehension for even short periods
    of time are considered a useful protection against malicious reverse
    engineering. In previous works, the difficulty of reverse engineering has
    been mainly estimated by means of code metrics, by the computational
    complexity of static analysis or by comparing the output of de-obfuscating
    tools. In this paper we take a different approach and assess the difficulty
    attackers have in understanding and modifying obfuscated code through
    controlled experiments involving human subjects.},
}


@INPROCEEDINGS{growingContextSensitiveLanguages,
    author="Buntrock, Gerhard
    and Lory{\'{s}}, Krzysztof",
    editor="Kuich, W.",
    title="On growing context-sensitive languages",
    booktitle="Automata, Languages and Programming",
    year="1992",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="77--88",
    isbn="978-3-540-47278-0",
    abstract="Growing context-sensitive grammars (GCSG) are investigated. The
    variable membership problem for GCSG is shown to be NP-complete. This solves
    a problem posed in [DW86]. It is also shown that the languages generated by
    GCSG form an abstract family of languages and several implications are
    presented.",
}


@inproceedings{repairingSyntaxErrorsInLR,
    author = {Cerecke, Carl},
    title = {Repairing Syntax Errors in LR-based Parsers},
    booktitle = {Proceedings of the Twenty-fifth Australasian Conference on Computer Science - Volume 4},
    series = {ACSC '02},
    year = {2002},
    isbn = {0-909925-82-8},
    location = {Melbourne, Victoria, Australia},
    pages = {17--22},
    numpages = {6},
    url = {http://dl.acm.org/citation.cfm?id=563801.563804},
    acmid = {563804},
    publisher = {Australian Computer Society, Inc.},
    address = {Darlinghurst, Australia, Australia},
    keywords = {LALR(1), bison, error repair, parsing, syntax errors, yacc},
    abstract = {When a compiler encounters a syntax error, it usually attempts
    to restart parsing to check the remainder of the input for any further
    errors. One common method of recovering from syntax errors is to repair the
    incorrect input string, allowing parsing to continue. This research presents
    a language independent method for repairing the input string to an LALR(1)
    parser. The method results in much faster repairs in general than an
    existing method, enabling some errors to be repaired that were previously
    too costly. Results are based on repairing syntax errors in Java programs
    from first year computer science students.},
}


@inproceedings{allStarAntlr,
    author = {Parr, Terence and Harwell, Sam and Fisher, Kathleen},
    title = {Adaptive LL(*) Parsing: The Power of Dynamic Analysis},
    booktitle = {Proceedings of the 2014 ACM International Conference on Object
    Oriented Programming Systems Languages \& Applications},
    series = {OOPSLA '14},
    year = {2014},
    isbn = {978-1-4503-2585-1},
    location = {Portland, Oregon, USA},
    pages = {579--598},
    numpages = {20},
    url = {http://doi.acm.org/10.1145/2660193.2660202},
    doi = {10.1145/2660193.2660202},
    acmid = {2660202},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {all(*), augmented transition networks, dfa, gll, glr, grammar,
    ll(*), nondeterministic parsing, peg},
    abstract = {Despite the advances made by modern parsing strategies such as
    PEG, LL(*), GLR, and GLL, parsing is not a solved problem. Existing
    approaches suffer from a number of weaknesses, including difficulties
    supporting side-effecting embedded actions, slow and/or unpredictable
    performance, and counter-intuitive matching strategies. This paper
    introduces the ALL(*) parsing strategy that combines the simplicity,
    efficiency, and predictability of conventional top-down LL(k) parsers with
    the power of a GLR-like mechanism to make parsing decisions. The critical
    innovation is to move grammar analysis to parse-time, which lets ALL(*)
    handle any non-left-recursive context-free grammar. ALL(*) is O(n4) in
    theory but consistently performs linearly on grammars used in practice,
    outperforming general strategies such as GLL and GLR by orders of magnitude.
    ANTLR 4 generates ALL(*) parsers and supports direct left-recursion through
    grammar rewriting. Widespread ANTLR 4 use (5000 downloads/month in 2013)
    provides evidence that ALL(*) is effective for a wide variety of
    applications.},
}


@inproceedings{llStarAntlr,
    author = {Parr, Terence and Fisher, Kathleen},
    title = {LL(*): The Foundation of the ANTLR Parser Generator},
    booktitle = {Proceedings of the 32Nd ACM SIGPLAN Conference on Programming
    Language Design and Implementation},
    series = {PLDI '11},
    year = {2011},
    isbn = {978-1-4503-0663-8},
    location = {San Jose, California, USA},
    pages = {425--436},
    numpages = {12},
    url = {http://doi.acm.org/10.1145/1993498.1993548},
    doi = {10.1145/1993498.1993548},
    acmid = {1993548},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {augmented transition networks, backtracking, context-sensitive
    parsing, deterministic finite automata, glr, memoization, nondeterministic
    parsing, peg, semantic predicates, subset construction, syntactic
    predicates},
    abstract = {Despite the power of Parser Expression Grammars (PEGs) and GLR,
    parsing is not a solved problem. Adding nondeterminism (parser speculation)
    to traditional LL and LR parsers can lead to unexpected parse-time behavior
    and introduces practical issues with error handling, single-step debugging,
    and side-effecting embedded grammar actions. This paper introduces the LL(*)
    parsing strategy and an associated grammar analysis algorithm that
    constructs LL(*) parsing decisions from ANTLR grammars. At parse-time,
    decisions gracefully throttle up from conventional fixed k>=1 lookahead to
    arbitrary lookahead and, finally, fail over to backtracking depending on the
    complexity of the parsing decision and the input symbols. LL(*) parsing
    strength reaches into the context-sensitive languages, in some cases beyond
    what GLR and PEGs can express. By statically removing as much speculation as
    possible, LL(*) provides the expressivity of PEGs while retaining LL's good
    error handling and unrestricted grammar actions. Widespread use of ANTLR
    (over 70,000 downloads/year) shows that it is effective for a wide variety
    of applications.},
}


@inproceedings{efficientNonDeterministicParsers,
    author = {Lang, Bernard},
    title = {Deterministic Techniques for Efficient Non-Deterministic Parsers},
    booktitle = {Proceedings of the 2Nd Colloquium on Automata, Languages and Programming},
    year = {1974},
    isbn = {3-540-06841-4},
    pages = {255--269},
    numpages = {15},
    doi = {10.1007/3-540-06841-4_65},
    url = {http://dl.acm.org/citation.cfm?id=646230.681872},
    acmid = {681872},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    abstract = {A general study of parallel non-deterministic parsing and
    translation à la Earley is developped formally, based on non-deterministic
    pushdown acceptor-transducers. Several results (complexity and efficiency)
    are established, same new and other previously proved only in special cases.
    As an application, we show that for every family of deterministic
    context-free pushdown parsers (e.g. precedence, LR(k), LL(k), ...) there is
    a family of general context-free parallel parsers that have the same
    efficiency in most practical cases (e.g. analysis of programming
    languages).},
}


@InProceedings{linearLL1AndLR1Grammars,
    author = "Holzer, Markus and Lange, Klaus -J{\"o}rn",
    editor = "{\'E}sik, Zolt{\'a}n",
    title = "On the complexities of linear LL(1) and LR(1) grammars",
    booktitle = "Fundamentals of Computation Theory",
    year = "1993",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "299--308",
    abstract = "Several notions of deterministic linear languages are considered
    and compared with respect to their complexities and to the families of
    formal languages they generate. We exhibit close relationships between
    simple linear languages and the deterministic linear languages both
    according to Nasu and Honda and to Ibarra, Jiang, and Ravikumar.
    Deterministic linear languages turn out to be special cases of languages
    generated by linear grammars restricted to LL(1) conditions, which have a
    membership problem solvable in NC1. In contrast to that, deterministic
    linear languages defined via automata models turn out to have a
    DSPACE(logn)-complete membership problem. Moreover, they coincide with
    languages generated by linear grammars subject to LR(1) conditions.",
    isbn = "978-3-540-47923-9"
}


@inproceedings{efficientQuantumComputation,
    author = {Purewal,Jr., Tarsem S.},
    title = {Revisiting a Limit on Efficient Quantum Computation},
    booktitle = {Proceedings of the 44th Annual Southeast Regional Conference},
    series = {ACM-SE 44},
    year = {2006},
    isbn = {1-59593-315-8},
    location = {Melbourne, Florida},
    pages = {239--243},
    numpages = {5},
    url = {http://doi.acm.org/10.1145/1185448.1185502},
    doi = {10.1145/1185448.1185502},
    acmid = {1185502},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {computational complexity, quantum computing},
    abstract = {In this paper, we offer an exposition of a theorem originally
    due to Adleman, Demarrais and Huang that shows that the quantum complexity
    class BQP (Bounded-error Quantum Polynomial time) is contained in the
    classical counting class PP (Probabilistic Polynomial time). Our proof
    follows the one given by Fortnow and Rogers that relates quantum computing
    to counting complexity classes by way of GapP functions. The contribution of
    this paper is an exposition of an important result that assumes a minimal
    background in computational complexity theory and no knowledge of quantum
    mechanics.},
}


@inproceedings{sippu1982,
    author = {Sippu, Seppo and Soisalon-Soininen, Eljas},
    title = {Practical Error Recovery in LR Parsing},
    booktitle = {Proceedings of the 9th ACM SIGPLAN-SIGACT Symposium on
    Principles of Programming Languages},
    series = {POPL '82},
    year = {1982},
    isbn = {0-89791-065-6},
    location = {Albuquerque, New Mexico},
    pages = {177--184},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/582153.582173},
    doi = {10.1145/582153.582173},
    acmid = {582173},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {An automatic syntax error handling technique applicable to LR
    parsing is presented and analyzed. The technique includes a "phrase-level"
    error recovery strategy augmented with certain additional features such as
    "local correction". Attention has also been paid to diagnostic aspects, i.e.
    the automatic generation of error message texts. The technique has been
    implemented in the compiler writing system HLP (Helsinki Language
    Processor), and some promising experimental results have been obtained by
    testing the technique with erroneous student-written Algol and Pascal
    programs.},
}


@INPROCEEDINGS{visualizationsInAFunctionalProgramming,
    author={J. A. {Velazquez-Iturbide} and A. {Presa-Vazquez}},
    booktitle={FIE'99 Frontiers in Education. 29th Annual Frontiers in Education
    Conference. Designing the Future of Science and Engineering Education.
    Conference Proceedings (IEEE Cat. No.99CH37011},
    title={Customization of visualizations in a functional programming environment},
    year={1999},
    volume={2},
    number={},
    pages={12B3/22-12B3/28 vol.2},
    doi={10.1109/FIE.1999.841580},
    ISSN={0190-5848},
    month={Nov},
    abstract={CS first-year students expect the user interface of programming
    environments to be similar to that of common PC applications. A natural
    evolution of educational programming environments consists in incorporating
    many of their user-friendly facilities. The authors concentrate in this
    paper on the facilities that WinHIPE, an environment for functional
    programming, provides to students for customizing the visualization of
    expressions. Expressions can be either pretty-printed as text or displayed
    graphically, showing drawings of lists and binary trees. Besides, fonts,
    sizes, colors, lines and distances are parameters that can be customized for
    any visualization. Finally, the visualization of large expressions can be
    simplified to show only their most relevant parts. Students obtain several
    benefits from customization facilities. They feel more comfortable with the
    programming environment WinHIPE, because they can develop more readable
    programs written "in their personal style". Students can also experiment at
    small effort with different formats, becoming profident in style issues.
    Finally customization facilities allow making clear in a course on
    programming languages the relevant role of visualization in programming
    tools and their relative independence from language syntax.},
    keywords={computer science education;educational courses;functional
    programming;programming environments;user interfaces;functional programming
    environment;visualisations customisation;first-year computer science
    students;PC applications;user interface;educational programming
    environments;WinHIPE;programming languages course;programming
    tools;Visualization;Functional programming;Programming environments;User
    interfaces;Programming profession;Application software;Binary trees;Computer
    languages;Cognitive science;Computer errors},
}


@INPROCEEDINGS{transformationForDomainSpecificOptimisation,
    author={O. S. {Bagge} and K. T. {Kalleberg} and M. {Haveraaen} and E. {Visser}},
    booktitle={Proceedings Third IEEE International Workshop on Source Code Analysis and Manipulation},
    title={Design of the CodeBoost transformation system for domain-specific optimisation of C++ programs},
    year={2003},
    volume={},
    number={},
    pages={65-74},
    doi={10.1109/SCAM.2003.1238032},
    month={Sep.},
    abstract={The use of a high-level, abstract coding style can greatly
    increase developer productivity. For numerical software, this can result in
    drastically reduced run-time performance. High-level, domain-specific
    optimisations can eliminate much of the overhead caused by an abstract
    coding style, but current compilers have poor support for domain-specific
    optimisation. We present CodeBoost, a source-to-source transformation tool
    for domain-specific optimisation of C++ programs. CodeBoost performs
    parsing, semantic analysis and pretty-printing, and transformations can be
    implemented either in the Stratego program transformation language, or as
    user-defined rewrite rules embedded within the C++ program. CodeBoost has
    been used with great success to optimise numerical applications written in
    the Sophus high-level coding style. We discuss the overall design of the
    CodeBoost transformation framework, and take a closer look at two important
    features of CodeBoost: user-defined rules and totem annotations. We also
    show briefly how CodeBoost is used to optimise Sophus code, resulting in
    applications that run twice as fast, or more.},
    keywords={optimising compilers;C++ language;software tools;abstract coding
    style;domain-specific optimization;program compiler;source-to-source
    transformation tool;C++ program optimisation;parsing;semantic
    analysis;Stratego program transformation language;user-defined rewrite
    rules;CodeBoost transformation framework;totem annotation;Sophus code;Design
    optimization;Runtime;Optimizing compilers;Informatics;Productivity;Software
    performance;Performance analysis;Bridges;Software engineering;Software
    libraries},
}


@inproceedings{yamlSpecificModelChecking,
    author = {Zervoudakis, Fokion and Rosenblum, David S. and Elbaum, Sebastian and Finkelstein, Anthony},
    title = {Cascading Verification: An Integrated Method for Domain-specific Model Checking},
    booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
    series = {ESEC/FSE 2013},
    year = {2013},
    isbn = {978-1-4503-2237-9},
    location = {Saint Petersburg, Russia},
    pages = {400--410},
    numpages = {11},
    url = {http://doi.acm.org/10.1145/2491411.2491454},
    doi = {10.1145/2491411.2491454},
    acmid = {2491454},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {Composite reasoning, OWL, PRISM, Prolog, SWRL, UAVs, domain model, model checking},
    abstract = {Model checking is an established method for verifying behavioral
    properties of system models. But model checkers tend to support low-level
    modeling languages that require intricate models to represent even the
    simplest systems. Modeling complexity arises in part from the need to encode
    domain knowledge at relatively low levels of abstraction. In this paper, we
    demonstrate that formalized domain knowledge can be reused to raise the
    abstraction level of model and property specifications, and hence the
    effectiveness of model checking. We describe a novel method for
    domain-specific model checking called cascading verification that uses
    composite reasoning over high-level system specifications and formalized
    domain knowledge to synthesize both low-level system models and their
    behavioral properties for verification. In particular, model builders use a
    high-level domain-specific language (DSL) based on YAML to express system
    specifications that can be verified with probabilistic model checking.
    Domain knowledge is encoded in the Web Ontology Language (OWL), the Semantic
    Web Rule Language (SWRL) and Prolog, which are used in combination to
    overcome their individual limitations. A compiler then synthesizes models
    and properties for verification by the probabilistic model checker PRISM. We
    illustrate cascading verification for the domain of uninhabited aerial
    vehicles (UAVs), for which we have constructed a prototype implementation.
    An evaluation of this prototype reveals nontrivial reductions in the size
    and complexity of input specifications compared to the artifacts synthesized
    for PRISM.},
}


@INPROCEEDINGS{improvingRefactoringSpeed,
    author={J. {Kim} and D. {Batory} and D. {Dig} and M. {Azanza}},
    booktitle={2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)},
    title={Improving Refactoring Speed by 10X},
    year={2016},
    pages={1145-1156},
    doi={10.1145/2884781.2884802},
    ISSN={1558-1225},
    month={May},
    abstract={Refactoring engines are standard tools in today's Integrated
    Development Environments (IDEs). They allow programmers to perform one
    refactoring at a time, but programmers need more. Most design patterns in
    the Gang-of-Four text can be written as a refactoring script - a
    programmatic sequence of refactorings. In this paper, we present R3, a new
    Java refactoring engine that supports refactoring scripts. It builds a
    main-memory, non-persistent database to encode Java entity declarations
    (e.g., packages, classes, methods), their containment relationships, and
    language features such as inheritance and modifiers. Unlike classical
    refactoring engines that modify Abstract Syntax Trees (ASTs), R3
    refactorings modify only the database; refactored code is produced only when
    pretty-printing ASTs that reference database changes. R3 performs comparable
    precondition checks to those of the Eclipse Java Development Tools (JDT) but
    R3's codebase is about half the size of the JDT refactoring engine and runs
    an order of magnitude faster. Further, a user study shows that R3 improved
    the success rate of retrofitting design patterns by 25\% up to 50\%.},
    keywords={Java;software maintenance;refactoring speed;integrated development
    environment;IDE;Gang-of-Four text;refactoring script;refactoring
    sequence;Java refactoring engine;Java entity declarations;abstract syntax
    trees;AST;Eclipse Java development tools;JDT;retrofitting design
    patterns;Java;Engines;Databases;Graphics;Computer bugs;Graphical user
    interfaces;Maintenance engineering},
}


@INPROCEEDINGS{prettyPrintingOfVisualSentences,
    author={T. B. {Dinesh} and S. M. {Uskudarh}},
    booktitle={Proceedings. 1997 IEEE Symposium on Visual Languages (Cat. No.97TB100180)},
    title={Pretty-printing of visual sentences},
    year={1997},
    volume={},
    number={},
    pages={242-243},
    doi={10.1109/VL.1997.626589},
    ISSN={1049-2615},
    month={Sep.},
    abstract={When input sentences are processed in some manner, say evaluated
    with some set of rules, the resulting sentence must be pretty-printed in
    order to be presented to the user. We introduce a technique called
    "Share-Where maintenance" which is used to preserve layout information by
    annotating abstract representations of visual sentences. The annotations in
    the abstract representation point to the presentation where sub-terms
    originated which were created either by the user (initial term) or by the
    language specifier (equations, as in introduced terms). Both kinds are
    visual presentations which are used for presenting the new term.},
    keywords={visual programming;grammars;visual languages;algebraic
    specification;formal specification;visual sentences;input
    sentences;pretty-printed;Share-Where maintenance;layout information;abstract
    representation annotation;language specifier;visual
    presentations;Equations;Labeling;Logic programming;Computer
    science;Usability;Information analysis;Calculus;Design
    engineering;Prototypes;Application software},
}


@INPROCEEDINGS{prettyPrintingForSoftware,
    author={M. {de Jonge}},
    booktitle={International Conference on Software Maintenance, 2002. Proceedings.},
    title={Pretty-printing for software reengineering},
    year={2002},
    volume={},
    number={},
    pages={550-559},
    doi={10.1109/ICSM.2002.1167816},
    ISSN={1063-6773},
    month={Oct},
    abstract={Automatic software reengineering changes or repairs existing
    software systems. They are usually tailor-made for a specific customer and
    language dependent. Maintaining similar reengineering for multiple customers
    and different language dialects may, therefore, soon become problematic
    unless advanced language technology is used. Generic pretty-printing is part
    of such technology and is the subject of this paper. We discuss specific
    pretty-print aspects of software reengineering such as fulfilling
    customer-specific format conventions, preserving existing layout, and
    producing multiple output formats. In addition, we describe pretty-print
    techniques that help to reduce maintenance effort of tailor-made
    reengineering supporting multiple language dialects. Applications such as
    COBOL reengineering and SDL documentation generation show that our
    techniques, implemented in the generic pretty-printer GPP, are feasible.},
    keywords={systems re-engineering;software maintenance;system
    documentation;COBOL;automatic software reengineering;generic
    pretty-printing;customer-specific format conventions;layout
    preservation;multiple output formats;maintenance;multiple language
    dialects;COBOL reengineering;SDL documentation
    generation;GPP;Documentation;Software systems;Network address
    translation;Application software;Time to
    market;Humans;Pipelines;Inspection;Software maintenance;Computer languages},
}


@INPROCEEDINGS{translatorGenerationCompilier,
    author={ {Ai Hua Wu} and J. {Paquet}},
    booktitle={8th International Conference on Computer Supported Cooperative Work in Design},
    title={The translator generation in the general intensional programming compilier},
    year={2004},
    volume={2},
    number={},
    pages={668-672 Vol.2},
    doi={10.1109/CACWD.2004.1349274},
    ISSN={},
    month={May},
    abstract={General intensional programming compiler (GIPC) is one component
    of the general intensional programming system (GIPSY) that aims at the
    development of a programming system that would allow dynamic investigations
    on the possibilities of intensional programming and all its widely different
    flavors and domains of application. To cope with the constant evolution of
    intensional programming languages, we design the system in a very flexible
    infrastructure for the generation of compiler components upon the creation
    of a new version. This work focuses on one and most important component that
    is about the generation of the translator between generic and specific
    intensional programming languages.},
    keywords={program interpreters;parallel languages;program
    compilers;configuration management;translator generation;general intensional
    programming complier;GIPC;general intensional programming system;GIPSY
    system;compiler components;version creation;specific intensional programming
    languages;Computer languages;Dynamic programming;Program
    processors;Filters;Tensile stress;Genetic programming;Runtime
    environment;Pipelines;Multidimensional systems;Differential equations},
}


@inproceedings{documentingAndSharingKnowledge,
    author        = {Guzzi, Anja},
    title         = {Documenting and Sharing Knowledge About Code},
    booktitle     = {Proceedings of the 34th International Conference on Software Engineering},
    series        = {ICSE '12},
    year          = {2012},
    isbn          = {978-1-4673-1067-3},
    pages         = {1535--1538},
    numpages      = {4},
    publisher     = {IEEE Press},
    address       = {Piscataway, NJ, USA},
    url           = {http://dl.acm.org/citation.cfm?id=2337223.2337476},
    location      = {Zurich, Switzerland},
    acmid         = {2337476},
    urlaccessdate = {2017-10-31},
    abstract      = {Software engineers spend a considerable amount of time on program
                    comprehension. Current research has primarily focused on assisting the developer
                    trying to build up his understanding of the code. This knowledge remains only in
                    the mind of the developer and, as time elapses, often “disappears”. In this
                    research, we shift the focus to the developer who is using her Integrated
                    Development Environment (IDE) for writing, modifying, or reading the code, and
                    who actually understands the code she is working with. The objective of this PhD
                    research is to seek ways to support this developer to document and share her
                    knowledge with the rest of the team. In particular, we investigate the full
                    potential of micro-blogging integrated into the IDE for addressing the program
                    comprehension problem.},
}


@inproceedings{analysisOfCodeReading,
    title         = {Analysis of Code Reading to Gain More Insight in Program Comprehension},
    author        = {Busjahn, Teresa and Schulte, Carsten and Busjahn, Andreas},
    booktitle     = {Proceedings of the 11th Koli Calling International
    Conference on Computing Education Research},
    series        = {Koli Calling '11},
    year          = {2011},
    isbn          = {978-1-4503-1052-9},
    location      = {Koli, Finland},
    pages         = {1--9},
    numpages      = {9},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    keywords      = {CS Ed research, code comprehension, code reading,
    educational research, eye tracking, program comprehension},
    doi           = {10.1145/2094131.2094133},
    acmid         = {2094133},
    location      = {Freie Universität Berlin, Germany},
    url           = {https://www.researchgate.net/publication/254004382_Analysis_of_code_reading_to_gain_more_insight_in_program_comprehension},
    urlaccessdate = {2017-10-31},
    abstract      = {Code reading, although an integral part of program comprehension, is rarely
                    reflected. In this paper, we want to argue for a research approach and direction
                    exploiting the potential that lies in the analysis of reading processes. Based
                    on the vast experience compiled in psychology and some studies in computing, eye
                    tracking and think aloud were elaborated as a viable research instrument for
                    code reading studies. We conducted a feasibility study, designed to examine the
                    actual process of code reading as the sensory starting point of comprehension.
                    Computational and statistical tools were developed to facilitate data capture
                    and analysis for eye tracking experiments. Results do not just provide proof of
                    concept but already emphasize differences between reading natural language text
                    and source code, as well as a distinct attention allocation within different
                    code elements like keywords and operators. In conclusion we suggest a
                    combination of theory-driven selected stimuli material, a carefully designed
                    procedure of eye tracking, complemented with suitable post-tests on
                    comprehension as well as retrospective think aloud in order to obtain additional
                    information on the linking process between perception and comprehension. As an
                    addition to other research approaches this should most certainly help us to
                    improve our knowledge of comprehension within an educational research
                    framework.},
}


@inproceedings{howProgrammersRead,
    author        = {A. Jbara and D. G. Feitelson},
    booktitle     = {2015 IEEE 23rd International Conference on Program Comprehension},
    title         = {How Programmers Read Regular Code: A Controlled Experiment Using Eye Tracking},
    year          = {2015},
    month         = {05},
    pages         = {244-254},
    doi           = {10.1109/ICPC.2015.35},
    ISSN          = {1092-8138},
    location      = {Florence, Italy},
    url           = {https://www.researchgate.net/publication/281579264_How_Programmers_Read_Regular_Code_A_Controlled_Experiment_Using_Eye_Tracking},
    urlaccessdate = {2017-10-31},
    keywords      = {ergonomics;gaze tracking;software metrics;source code (software);LOC;McCabe
                    cyclomatic complexity;cubic model;exponential model;eye tracking;lines of
                    code;read regular code;regular function;source code complexity;syntactic code
                    complexity metrics;Complexity theory;Correlation;Diamonds;Time
                    measurement;Tracking;Visualization;Code complexity metrics;Code
                    regularity;Controlled experiment;Eye tracking},
    abstract      = {Regular code, which includes repetitions of the same basic pattern, has been
                    shown to have an effect on code comprehension: a regular function can be just as
                    easy to comprehend as an irregular one with the same functionality, despite
                    being longer and including more control constructs. It has been speculated that
                    this effect is due to leveraging the understanding of the first instances to
                    ease the understanding of repeated instances of the pattern. To verify and
                    quantify this effect, we use eye tracking to measure the time and effort spent
                    reading and understanding regular code. The results are that time and effort
                    invested in the initial code segments are indeed much larger than those spent on
                    the later ones, and the decay in effort can be modeled by an exponential or
                    cubic model. This shows that syntactic code complexity metrics (such as LOC and
                    MCC) need to be made context-sensitive, e.g. By giving reduced weight to
                    repeated segments according to their place in the sequence.},
}


@inproceedings{improvingCodeReadability,
    title         = {Improving code readability models with textual features},
    author        = {S. Scalabrino and M. Linares-Vásquez and D. Poshyvanyk and R. Oliveto},
    booktitle     = {2016 IEEE 24th International Conference on Program Comprehension (ICPC)},
    year          = {2016},
    month         = {05},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    pages         = {1-10},
    location      = {Austin, TX, USA},
    isbn          = {978-1-5090-1428-6},
    doi           = {10.1109/ICPC.2016.7503707},
    url           = {https://www.researchgate.net/publication/301685380_Improving_Code_Readability_Models_with_Textual_Features},
    urlaccessdate = {2017-11-01},
    keywords      = {software maintenance;source code (software);text analysis;code readability
                    models;code snippets;line length;program comprehension effort;software
                    maintenance;software readability;source code;textual features;Computational
                    modeling;Feature extraction;Semantics;Software
                    quality;Syntactics;Visualization},
    abstract      = {Code reading is one of the most frequent activities in software maintenance;
                    before implementing changes, it is necessary to fully understand source code
                    often written by other developers. Thus, readability is a crucial aspect of
                    source code that may significantly influence program comprehension effort. In
                    general, models used to estimate software readability take into account only
                    structural aspects of source code, e.g., line length and a number of comments.
                    However, source code is a particular form of text; therefore, a code readability
                    model should not ignore the textual aspects of source code encapsulated in
                    identifiers and comments. In this paper, we propose a set of textual features
                    aimed at measuring code readability. We evaluated the proposed textual features
                    on 600 code snippets manually evaluated (in terms of readability) by 5K+ people.
                    The results demonstrate that the proposed features complement classic structural
                    features when predicting code readability judgments. Consequently, a code
                    readability model based on a richer set of features, including the ones proposed
                    in this paper, achieves a significantly higher accuracy as compared to all of
                    the state-of-the-art readability models.},
}


@inproceedings{moldableCodeEditor,
    title         = {Towards a Live, Moldable Code Editor},
    booktitle     = {Companion to the First International Conference on the Art, Science and Engineering of Programming},
    series        = {Programming '17},
    author        = {Syrel, Aliaksei},
    year          = {2017},
    month         = {04},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    pages         = {43:1--43:3},
    articleno     = {43},
    numpages      = {3},
    keywords      = {Code editors, Program comprehension, User interfaces},
    location      = {Brussels, Belgium},
    doi           = {10.1145/3079368.3079376},
    isbn          = {978-1-4503-4836-2},
    acmid         = {3079376},
    url           = {https://www.researchgate.net/publication/318873843_Towards_a_live_moldable_code_editor},
    urlaccessdate = {2017-11-02},
    abstract      = {Creating and evolving object-oriented applications requires developers to
                    reason about source code and run-time state. Integrated development environments
                    (IDEs) are tools that support developers in this activity. Many mainstream IDEs,
                    however, focus on code editors that promote reading of static text even in the
                    debugger. This affects program comprehension, as developers have to manually
                    link the static code with the run-time objects. In this work we explore how to
                    address this problem through a moldable code editor that enables developers to
                    select executable snippets of code and replace them with graphical views of the
                    resulting objects. Each object can define multiple views that developers can
                    select. This way objects and code coexist in the same editor.},
}


@inproceedings{quitDiffCalculating,
    title         = {Quit Diff: Calculating the Delta Between RDF Datasets Under Version Control},
    author        = {Arndt, Natanael and Radtke, Norman},
    booktitle     = {Proceedings of the 12th International Conference on Semantic Systems},
    series        = {SEMANTiCS 2016},
    year          = {2016},
    month         = {09},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    isbn          = {978-1-4503-4752-5},
    pages         = {185--188},
    numpages      = {4},
    location      = {Leipzig, Germany},
    doi           = {10.1145/2993318.2993349},
    acmid         = {2993349},
    url           = {https://www.researchgate.net/publication/309430151_Quit_Diff_Calculating_the_Delta_Between_RDF_Datasets_Under_Version_Control},
    urlaccessdate = {2017-11-03},
    abstract      = {Distributed actors working on a common RDF dataset regularly encounter the
                    issue to compare the status of one graph with another or generally to
                    synchronize copies of a dataset. A versioning system helps to synchronize the
                    copies of a dataset, combined with a difference calculation system it is also
                    possible to compare versions in a log and to determine, in which version a
                    certain statement was introduced or removed. In this demo we present Quit Diff1,
                    a tool to compare versions of a Git versioned quad store, while it is also
                    applicable to simple unversioned RDF datasets. We are following an approach to
                    abstract from differences on a syntactical level to differences on the level of
                    the RDF data model, while we leave further semantic interpretation on the schema
                    and instance level to specialized applications. Quit Diff can generate patches
                    in various output formats and can be directly integrated in the distributed
                    version control system Git which provides a foundation for a comprehensive
                    co-evolution work flow on RDF datasets.},
}


@inproceedings{areThereDomainSpecificLanguages,
    author = {Michaelson, Greg},
    title = {Are There Domain Specific Languages?},
    booktitle = {Proceedings of the 1st International Workshop on Real World Domain Specific Languages},
    series = {RWDSL '16},
    year = {2016},
    isbn = {978-1-4503-4051-9},
    location = {Barcelona, Spain},
    pages = {1:1--1:3},
    articleno = {1},
    numpages = {3},
    url = {http://doi.acm.org/10.1145/2889420.2892271},
    doi = {10.1145/2889420.2892271},
    acmid = {2892271},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {Domain Specific Languages, expressiveness},
    abstract = {Turing complete languages can express unbounded computations
    over unbounded structures, either directly or by a suitable encoding. In
    contrast, Domain Specific Languages (DSLs) are intended to simplify the
    expression of computations over structures in restricted contexts. However,
    such simplification often proves irksome, especially for constructing more
    elaborate programs where the domain, though central, is one of many
    considerations. Thus, it is often tempting to extend a DSL with more general
    abstractions, typically to encompass common programming tropes, typically
    from favourite languages. The question then arises: once a DSL becomes
    Turing complete, then in what sense is it still domain specific?
    },
}


@inproceedings{efficientBreadthFirstSearch,
    author = {Shang, Haichuan and Kitsuregawa, Masaru},
    title = {Efficient Breadth-first Search on Large Graphs with Skewed Degree Distributions},
    booktitle = {Proceedings of the 16th International Conference on Extending Database Technology},
    series = {EDBT '13},
    year = {2013},
    isbn = {978-1-4503-1597-5},
    location = {Genoa, Italy},
    pages = {311--322},
    numpages = {12},
    url = {http://doi.acm.org/10.1145/2452376.2452413},
    doi = {10.1145/2452376.2452413},
    acmid = {2452413},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {breadth-first search, graph database, graph indexing},
    abstract = {Many recent large-scale data intensive applications are
    increasingly demanding efficient graph databases. Distributed graph
    algorithms, as a core part of practical graph databases, have a wide range
    of important applications, but have been rarely studied in sufficient
    detail. These problems are challenging as real graphs are usually extremely
    large and the intrinsic character of graph data, lacking locality, causes
    unbalanced computation and communication workloads. In this paper, we
    explore distributed breadth-first search algorithms with regards to
    large-scale applications. We propose DPC (Degree-based Partitioning and
    Communication), a scalable and efficient distributed BFS algorithm which
    achieves high scalability and performance through novel balancing techniques
    between computation and communication. In experimental study, we compare our
    algorithm with two state-of-the-art algorithms under the Graph500 benchmark
    with a variety of settings. The result shows our algorithm significantly
    outperforms the existing algorithms under all the settings.},
}


@article{compilersCompilerMetaLanguage,
    author = {Book, Erwin and Shorre, Dewey Val and Sherman, Steven J.},
    title = {The CWIC/36O System, a Compiler for Writing and Implementing Compilers},
    journal = {SIGPLAN Not.},
    issue_date = {June 1970},
    volume = {5},
    number = {6},
    month = jun,
    year = {1970},
    issn = {0362-1340},
    pages = {11--29},
    numpages = {19},
    url = {http://doi.acm.org/10.1145/954344.954345},
    doi = {10.1145/954344.954345},
    acmid = {954345},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {Cwic/360 (Compiler for Writing and Implementing Compilers) is a
    metacompiler system. It is composed of compilers for three special-purpose
    languages, each intended to permit the description of certain aspects of
    translation in a straightforward, natural manner. The Syntax language is
    used to describe the recognition of source text and the construction from it
    of an intermediate tree structure. The Generator language is used to
    describe the transformation of the tree into appropriate object language.
    The MOL/360 language is used to provide an interface with the machine and
    its operating system.This paper describes each of these languages, presents
    examples of their use, and discusses the philosophy underlying their design
    and implementation.},
}


@article{teachingEbnf,
    author = {Pattis, Richard E.},
    title = {Teaching EBNF First in CS 1},
    journal = {SIGCSE Bull.},
    issue_date = {March 1994},
    volume = {26},
    number = {1},
    month = mar,
    year = {1994},
    issn = {0097-8418},
    pages = {300--303},
    numpages = {4},
    url = {http://doi.acm.org/10.1145/191033.191155},
    doi = {10.1145/191033.191155},
    acmid = {191155},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {This paper is a guided tour through the first day of a CS 1
    course. It discusses teaching Extended Backus-Naur Form (EBNF) as the first
    topic—not to facilitate presenting the syntax of a programming language, but
    because EBNF is a microcosm of programming. With no prerequisites, students
    are introduced to a variety of fundamental concepts in programming: formal
    systems, abstraction, control structures, equivalence of descriptions, the
    difference between syntax and semantics, and the relative power of recursion
    versus iteration. As a non-numeric formal system, EBNF provides a small but
    concrete context in which to study all these topics.EBNF descriptions
    include abstraction (named rules) and the four fundamental control
    structures (sequence, decision, repetition, and recursion). Because there
    are no data or parameters in EBNF, it is easy to sidestep tricky issues
    surrounding variables, scope, assignment statements, and parameter modes.
    Describing entitles in EBNF is similar to describing computations in a
    programming language. Students learn to read a description and analyze
    whether it generates/matches candidate symbols; then they learn to
    synthesize descriptions from English specifications, augmented by legal and
    illegal exemplars of symbols.All these concepts can be covered in one
    lecture, establishing a high level of formality early in the course, while
    foreshadowing actual programming language features and techniques to be
    covered later. Of course, learning EBNF also facilitates presenting the
    syntax of a programming language concisely during the rest of the course.},
}


@article{errorsInLRParsers,
    title = {Repairing syntax errors in LR parsers},
    author = {Corchuelo, Rafael and Antonio Pérez, José and Ruiz-Cortés, Antonio and Toro, Miguel},
    year = {2002},
    month = {11},
    pages = {698-710},
    volume = {24},
    journal = {ACM Trans. Program. Lang. Syst.},
    doi = {10.1145/586088.586092},
    url = {https://www.researchgate.net/publication/220404285_Repairing_syntax_errors_in_LR_parsers},
    abstract = {This article reports on an error-repair algorithm for LR
    parsers. It locally inserts, deletes or shifts symbols at the positions
    where errors are detected, thus modifying the right context in order to
    resume parsing on a valid piece of input. This method improves on others in
    that it does not require the user to provide additional information about
    the repair process, it does not require precalculation of auxiliary tables,
    and it can be easily integrated into existing LR parser generators. A
    Yacc-based implementation is presented along with some experimental results
    and comparisons with other well-known methods.},
}


@article{generalContextFreeParsingAlgorithm,
    title = "A general context-free parsing algorithm running in linear time on
    every LR(k) grammar without using lookahead",
    journal = "Theoretical Computer Science",
    volume = "82",
    number = "1",
    pages = "165 - 176",
    year = "1991",
    issn = "0304-3975",
    doi = "https://doi.org/10.1016/0304-3975(91)90180-A",
    url = "http://www.sciencedirect.com/science/article/pii/030439759190180A",
    author = "Joop M.I.M. Leo",
    abstract = "A new general context-free parsing algorithm is presented which
    runs in linear time and space on every LR(k) grammar without using any
    lookahead and without making use of the LR property. Most of the existing
    implementations of tabular parsing algorithms, including those using
    lookahead, can easily be adapted to this new algorithm without a noteworthy
    loss of efficiency. For some natural right recursive grammars both the time
    and space complexity will be improved from O(n2) to O(n). This makes this
    algorithm not only of theoretical but probably of practical interest as
    well.",
}


@article{complexityOfLRKTesting,
    author = {Hunt,III, Harry B. and Szymanski, Thomas G. and Ullman, Jeffrey D.},
    title = {On the Complexity of LR(K) Testing},
    journal = {Commun. ACM},
    issue_date = {Dec. 1975},
    volume = {18},
    number = {12},
    month = dec,
    year = {1975},
    issn = {0001-0782},
    pages = {707--716},
    numpages = {10},
    url = {http://doi.acm.org/10.1145/361227.361232},
    doi = {10.1145/361227.361232},
    acmid = {361232},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {LR(k) grammars, NP-complete problems, computational complexity,
    context-free grammars, parsing},
    abstract = {The problem of determining whether an arbitrary context-free
    grammar is a member of some easily parsed subclass of grammars such as the
    LR(k) grammars is considered. The time complexity of this problem is
    analyzed both when k is considered to be a fixed integer and when k is
    considered to be a parameter of the test. In the first case, it is shown
    that for every k there exists an O(nk+2) algorithm for testing the LR(k)
    property, where n is the size of the grammar in question. On the other hand,
    if both k and the subject grammar are problem parameters, then the
    complexity of the problem depends very strongly on the representation chosen
    for k. More specifically, it is shown that this problem is NP-complete when
    k is expressed in unary. When k is expressed in binary the problem is
    complete for nondeterministic exponential time. These results carry over to
    many other parameterized classes of grammars, such as the LL(k), strong
    LL(k), SLR(k), LC(k), and strong LC(k) grammars.},
}


@article{quantumComputingForNonPhysicists,
    author = {Rieffel, Eleanor and Polak, Wolfgang},
    title = {An Introduction to Quantum Computing for Non-physicists},
    journal = {ACM Comput. Surv.},
    volume = {32},
    number = {3},
    month = {sep},
    year = {2000},
    issn = {0360-0300},
    pages = {300--335},
    numpages = {36},
    url = {http://doi.acm.org/10.1145/367701.367709},
    doi = {10.1145/367701.367709},
    acmid = {367709},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {complexity, parallelism, quantum computing},
    abstract = {Richard Feynman's observation that certain quantum mechanical
    effects cannot be simulated efficiently on a computer led to speculation
    that computation in general could be done more efficiently if it used these
    quantum effects. This speculation proved justified when Peter Shor described
    a polynomial time quantum algorithm for factoring intergers.In quantum
    systems, the computational space increases exponentially with the size of
    the system, which enables exponential parallelism. This parallelism could
    lead to exponentially faster quantum algorithms than possible classically.
    The catch is that accessing the results, which requires measurement, proves
    tricky and requires new nontraditional programming techniques.The aim of
    this paper is to guide computer scientists through the barriers that
    separate quantum computing from conventional computing. We introduce basic
    principles of quantum mechanics to explain where the power of quantum
    computers comes from and why it is difficult to harness. We describe quantum
    cryptography, teleportation, and dense coding. Various approaches to
    exploiting the power of quantum parallelism are explained. We conclude with
    a discussion of quantum error correction.},
}


@article{quantumComputerSurvey,
    author = {Yoshito Kanamori and Seong-Moo Yoo and W. D. Pan and F. T. Sheldon},
    title = {A short survey on quantum computers},
    journal = {International Journal of Computers and Applications},
    volume = {28},
    issue = {3},
    year = {2006},
    issn = {1925-7074},
    url = {https://www.researchgate.net/publication/228613051_A_short_survey_on_quantum_computers},
    doi = {10.2316/Journal.202.2006.3.202-1700},
    publisher = {ACTA Press},
    address = {2451 Dieppe Ave SW, Calgary, AB, Canada},
    keywords = {Classical computers, quantum computers, quantum computer
    systems, quantum simulators, Shor's algorithm},
    abstract = {Quantum computing is an emerging technology. The clock frequency
    of current computer processor systems may reach about 40 GHz within the next
    10 years. By then, one atom may represent one bit. Electrons under such
    conditions are no longer described by classical physics, and a new model of
    the computer may be necessary by that time. The quantum computer is one
    proposal that may have merit in dealing with the problems presented.
    Currently, there exist some algorithms utilizing the advantage of quantum
    computers. For example, Shor's algorithm performs factoring of a large
    integer in polynomial time, whereas classical factoring algorithms can do it
    in exponential time. In this paper we briey survey the current status of
    quantum computers, quantum computer systems, and quantum simulators.},
}


@article{complexityClasses,
    author = {Tor\'{a}n, Jacobo},
    title = {Complexity Classes Defined by Counting Quantifiers},
    journal = {J. ACM},
    volume = {38},
    number = {3},
    month = jul,
    year = {1991},
    issn = {0004-5411},
    pages = {752--773},
    numpages = {22},
    url = {http://doi.acm.org/10.1145/116825.116858},
    doi = {10.1145/116825.116858},
    acmid = {116858},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {counting complexity classes},
    abstract = {The polynomial-time counting hierarchy, a hierarchy of
    complexity classes related to the notion of counting is studied. Some of their
    structural properties are investigated, settling many open questions dealing
    with oracle characterizations, closure under Boolean operations, and relations
    with other complexity classes. A new combinatorial technique to obtain
    relativized separations for some of the studied classes, which imply absolute
    separations for some logarithmic time bounded complexity classes, is
    developed.},
}


@article{probabilisticQuantumComputation,
    title = {Probabilistic instantaneous quantum computation},
    author = {Brukner, \ifmmode \check{C}\else \v{C}\fi{}aslav and Pan, Jian-Wei
    and Simon, Christoph and Weihs, Gregor and Zeilinger, Anton},
    journal = {Phys. Rev. A},
    volume = {67},
    issue = {3},
    pages = {034304},
    numpages = {4},
    year = {2003},
    month = {Mar},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevA.67.034304},
    url = {https://link.aps.org/doi/10.1103/PhysRevA.67.034304},
    abstract = {The principle of teleportation can be used to perform a quantum
    computation even before its quantum input is defined. The basic idea is to
    perform the quantum computation at some earlier time with qubits that are
    part of an entangled state. At a later time a generalized Bell-state
    measurement is performed jointly on the then defined actual input qubits and
    the rest of the entangled state. This projects the output state onto the
    correct one with a certain exponentially small probability. The sufficient
    conditions are found under which the scheme is of benefit.},
}


@article{theGoodAndBadQuantumComputing,
    title = {Seeking Quantum Speedup Through Spin Glasses: The Good, the Bad, and the Ugly},
    author = {Katzgraber, Helmut G. and Hamze, Firas and Zhu, Zheng and Ochoa, Andrew J. and Munoz-Bauza, H.},
    journal = {Phys. Rev. X},
    volume = {5},
    issue = {3},
    pages = {031026},
    numpages = {15},
    year = {2015},
    month = {Sep},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevX.5.031026},
    url = {https://link.aps.org/doi/10.1103/PhysRevX.5.031026},
    abstract = {There has been considerable progress in the design and
    construction of quantum annealing devices. However, a conclusive detection
    of quantum speedup over traditional silicon-based machines remains elusive,
    despite multiple careful studies. In this work we outline strategies to
    design hard tunable benchmark instances based on insights from the study of
    spin glasses—the archetypal random benchmark problem for novel algorithms
    and optimization devices. We propose to complement head-to-head scaling
    studies that compare quantum annealing machines to state-of-the-art
    classical codes with an approach that compares the performance of different
    algorithms and/or computing architectures on different classes of
    computationally hard tunable spin-glass instances. The advantage of such an
    approach lies in having to compare only the performance hit felt by a given
    algorithm and/or architecture when the instance complexity is increased.
    Furthermore, we propose a methodology that might not directly translate into
    the detection of quantum speedup but might elucidate whether quantum
    annealing has a “quantum advantage” over corresponding classical algorithms,
    such as simulated annealing. Our results on a 496-qubit D-Wave Two quantum
    annealing device are compared to recently used state-of-the-art thermal
    simulated annealing codes.},
}


@article{larkJosefGrosch,
    author = {Grosch, Josef},
    year = {2005},
    month = {06},
    url = {http://www.cocolab.com/products/cocktail/doc.pdf/lark.pdf},
    publisher = {Online},
    urlaccessdate = {2019-06-06},
    title = {Lark-An LALR(2) parser generator with backtracking},
    abstract = {Lark is a parser generator for LALR(2) and LR(1) grammars. With
    its backtracking facility it is ev en able to generate parsers for non-LR(k)
    languages. It is compatible with its predecessor, the parser generator Lalr
    [GrV]. The parser generator Lark offers the following features: generates
    highly efficient parsers provides automatic error reporting, error recovery,
    and error repair},
}


@article{contextSensitiveParsing,
    author = {Woods, William A.},
    title = {Context-sensitive Parsing},
    journal = {Commun. ACM},
    volume = {13},
    number = {7},
    month = jul,
    year = {1970},
    issn = {0001-0782},
    pages = {437--445},
    numpages = {9},
    url = {http://doi.acm.org/10.1145/362686.362695},
    doi = {10.1145/362686.362695},
    acmid = {362695},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {context-sensitive grammars, context-sensitive parsing, formal
    grammars, formal language theory, parsing, parsing algorithms, recognition
    algorithms},
    abstract = {This paper presents a canonical form for context-sensitive
    derivations and a parsing algorithm which finds each context-sensitive
    analysis once and only once. The amount of memory required by the algorithm
    is essentially no more than that required to store a single complete
    derivation. In addition, a modified version of the basic algorithm is
    presented which blocks infinite analyses for grammars which contain loops.
    The algorithm is also compared with several previous parsers for
    context-sensitive grammars and general rewriting systems, and the difference
    between the two types of analyses is discussed. The algorithm appears to be
    complementary to an algorithm by S. Kuno in several respects, including the
    space-time trade-off and the degree of context dependence involved.
    },
}


@article{knuthLrParser1965,
    title = {On the translation of languages from left to right},
    journal = {Information and Control},
    volume = {8},
    number = {6},
    pages = {607 - 639},
    year = {1965},
    issn = {0019-9958},
    doi = {https://doi.org/10.1016/S0019-9958(65)90426-2},
    url = {http://www.sciencedirect.com/science/article/pii/S0019995865904262},
    author = {Donald E. Knuth},
    abstract = {There has been much recent interest in languages whose grammar
    is sufficiently simple that an efficient left-to-right parsing algorithm can
    be mechanically produced from the grammar. In this paper, we define LR(k)
    grammars, which are perhaps the most general ones of this type, and they
    provide the basis for understanding all of the special tricks which have
    been used in the construction of parsing algorithms for languages with
    simple structure, e.g. algebraic languages. We give algorithms for deciding
    if a given grammar satisfies the LR(k) condition, for given k, and also give
    methods for generating recognizes for LR(k) grammars. It is shown that the
    problem of whether or not a grammar is LR(k) for some k is undecidable, and
    the paper concludes by establishing various connections between LR(k)
    grammars and deterministic languages. In particular, the LR(k) condition is
    a natural analogue, for grammars, of the deterministic condition, for
    languages.},
}


@article{polynomialQuantumComputers,
    author = {Shor, P.},
    title = {Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer},
    journal = {SIAM Review},
    volume = {41},
    number = {2},
    pages = {303-332},
    year = {1999},
    doi = {10.1137/S0036144598347011},
    URL = {https://doi.org/10.1137/S0036144598347011},
    eprint = {https://doi.org/10.1137/S0036144598347011},
    abstratc={A digital computer is generally believed to be an efficient
    universal computing device; that is, it is believed to be able to simulate
    any physical computing device with an increase in computation time by at
    most a polynomial factor. This may not be true when quantum mechanics is
    taken into consideration. This paper considers factoring integers and
    finding discrete logarithms, two problems that are generally thought to be
    hard on classical computers and that have been used as the basis of several
    proposed cryptosystems. Efficient randomized algorithms are given for these
    two problems on a hypothetical quantum computer. These algorithms take a
    number of steps polynomial in the input size, for example, the number of
    digits of the integer to be factored.},
}


@article{churchTuringQuantumComputer,
    author = {David Deutsch  and Roger Penrose },
    title = {Quantum theory, the Church-Turing principle and the universal quantum computer},
    journal = {Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences},
    volume = {400},
    number = {1818},
    pages = {97-117},
    year = {1985},
    doi = {10.1098/rspa.1985.0070},
    URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1985.0070},
    eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1985.0070},
    abstract = { It is argued that underlying the Church–Turing hypothesis there
    is an implicit physical assertion. Here, this assertion is presented
    explicitly as a physical principle: ‘every finitely realizible physical
    system can be perfectly simulated by a universal model computing machine
    operating by finite means'. Classical physics and the universal Turing
    machine, because the former is continuous and the latter discrete, do not
    obey the principle, at least in the strong form above. A class of model
    computing machines that is the quantum generalization of the class of Turing
    machines is described, and it is shown that quantum theory and the
    'universal quantum computer' are compatible with the principle. Computing
    machines resembling the universal quantum computer could, in principle, be
    built and would have many remarkable properties not reproducible by any
    Turing machine. These do not include the computation of non-recursive
    functions, but they do include ‘quantum parallelism', a method by which
    certain probabilistic tasks can be performed faster by a universal quantum
    computer than by any classical restriction of it. The intuitive explanation
    of these properties places an intolerable strain on all interpretations of
    quantum theory other than Everett's. Some of the numerous connections
    between the quantum theory of computation and the rest of physics are
    explored. Quantum complexity theory allows a physically more reasonable
    definition of the ‘complexity' or ‘knowledge' in a physical system than does
    classical complexity theory. },
}


@article{nonlinearQuantumComputers,
    title = {Nonlinear Quantum Mechanics Implies Polynomial-Time Solution for
    NP~=Complete and \#P Problems},
    author = {Abrams, Daniel S. and Lloyd, Seth},
    journal = {Phys. Rev. Lett.},
    volume = {81},
    issue = {18},
    pages = {3992--3995},
    numpages = {0},
    year = {1998},
    month = {Nov},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevLett.81.3992},
    url = {https://link.aps.org/doi/10.1103/PhysRevLett.81.3992},
    abstract = {If quantum states exhibit small nonlinearities during time
    evolution, then quantum computers can be used to solve NP~=complete and \#P
    problems in polynomial time. We provide algorithms that solve NP~=complete
    and \#P oracle problems by exploiting nonlinear quantum logic gates. Using
    the Weinberg model as a simple example, the explicit construction of these
    gates is derived from the underlying physics. Nonlinear quantum algorithms
    are also presented using Polchinski type nonlinearities which do not allow
    for superluminal communication.},
}


@ARTICLE{chomskyGrammars1956,
    author={N. {Chomsky}},
    journal={IRE Transactions on Information Theory},
    title={Three models for the description of language},
    year={1956},
    volume={2},
    number={3},
    pages={113-124},
    keywords={Languages;Markov processes;Natural
    languages;Testing;Laboratories;Markov processes;Impedance
    matching;Kernel;Research and development},
    doi={10.1109/TIT.1956.1056813},
    ISSN={0096-1000},
    month={Sep.},
    abstract={We investigate several conceptions of linguistic structure to
    determine whether or not they can provide simple and "revealing" grammars
    that generate all of the sentences of English and only these. We find that
    no finite-state Markov process that produces symbols with transition from
    state to state can serve as an English grammar. Furthermore, the particular
    subclass of such processes that producen-order statistical approximations to
    English do not come closer, with increasingn, to matching the output of an
    English grammar. We formalize-the notions of "phrase structure" and show
    that this gives us a method for describing language which is essentially
    more powerful, though still representable as a rather elementary type of
    finite-state process. Nevertheless, it is successful only when limited to a
    small subset of simple sentences. We study the formal properties of a set of
    grammatical transformations that carry sentences with phrase structure into
    new sentences with derived phrase structure, showing that transformational
    grammars are processes of the same elementary type as phrase-structure
    grammars; that the grammar of English is materially simplified if phrase
    structure description is limited to a kernel of simple sentences from which
    all other sentences are constructed by repeated transformations; and that
    this view of linguistic structure gives a certain insight into the use and
    understanding of language.},
}


@article{generalizedVennDiagrams,
    author = {Kestler, Hans A. and Müller, André and Gress, Thomas M. and Buchholz, Malte},
    title = "{Generalized Venn diagrams: a new method of visualizing complex genetic set relations}",
    journal = {Bioinformatics},
    volume = {21},
    number = {8},
    pages = {1592-1595},
    year = {2004},
    month = {11},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bti169},
    url = {https://doi.org/10.1093/bioinformatics/bti169},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/21/8/1592/691813/bti169.pdf},
    abstract = "{Motivation: Microarray experiments generate vast amounts of
    data. The unknown or only partially known functional context of
    differentially expressed genes may be assessed by querying the Gene Ontology
    database via GOMiner. Resulting tree representations are difficult to
    interpret and are not suited for visualization of this type of data. Methods
    are needed to effectively visualize these complex set relationships.Results:
    We present a visualization approach for set relationships based on Venn
    diagrams. The proposed extension enhances the usual notion of Venn diagrams
    by incorporating set size information. The cardinality of the sets and
    intersection sets is represented by their corresponding circle (polygon)
    sizes. To avoid local minima, solutions to this problem are sought by
    evolutionary optimization. This generalized Venn diagram approach has been
    implemented as an interactive Java application (VennMaster) specifically
    designed for use with GOMiner in the context of the Gene Ontology
    database.Availability: VennMaster is platform-independent (Java 1.4.2) and
    has been tested on Windows (XP, 2000), Mac OS X, and Linux. Supplementary
    information and the software (free for non-commercial use) are available at
    http://www.informatik.uni-ulm.de/ni/mitarbeiter/HKestler/vennm together with
    a user documentation.Contact:hans.kestler@medizin.uni-ulm.de}",
}


@article{parikh1966,
    author = {Parikh, Rohit J.},
    title = {On Context-Free Languages},
    journal = {J. ACM},
    volume = {13},
    number = {4},
    month = oct,
    year = {1966},
    issn = {0004-5411},
    pages = {570--581},
    numpages = {12},
    url = {http://doi.acm.org/10.1145/321356.321364},
    doi = {10.1145/321356.321364},
    acmid = {321364},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {In this report, certain properties of context-free (CF or type
    2) grammars are investigated, like that of Chomsky. In particular, questions
    regarding structure, possible ambiguity and relationship to finite automata
    are considered. The following results are presented: The language generated
    by a context-free grammmar is linear in a sense that is defined precisely.
    The requirement of unambiguity—that every sentence has a unique phrase
    structure—weakens the grammar in the sense that there exists a CF language
    that cannot be generated unambiguously by a CF grammar. The result that not
    every CF language is a finite automaton (FA) language is improved in the
    following way. There exists a CF language L such that for any L $\subset$ L, if L
    is FA, an L $\subset$ L can be found such that L is also FA, L $\subset$ L and L
    contains infinitely many sentences not in L. A type of grammar is defined
    that is intermediate between type 1 and type 2 grammars. It is shown that
    this type of grammar is essentially stronger than type 2 grammars and has
    the advantage over type 1 grammars that the phrase structure of a
    grammatical sentence is unique, once the derivation is given.},
}


@article{lalrDeRemer1982,
    author = {DeRemer, Frank and Pennello, Thomas},
    title = {Efficient Computation of LALR(1) Look-Ahead Sets},
    journal = {ACM Trans. Program. Lang. Syst.},
    issue_date = {Oct. 1982},
    volume = {4},
    number = {4},
    month = oct,
    year = {1982},
    issn = {0164-0925},
    pages = {615--649},
    numpages = {35},
    url = {http://doi.acm.org/10.1145/69622.357187},
    doi = {10.1145/69622.357187},
    acmid = {357187},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {Two relations that capture the essential structure of the
    problem of computing LALR(1) look-ahead sets are defined, and an efficient
    algorithm is presented to compute the sets in time linear in the size of the
    relations. In particular, for a PASCAL grammar, the algorithm performs fewer
    than 15 percent of the set unions performed by the popular compiler-compiler
    YACC. When a grammar is not LALR(1), the relations, represented explicitly,
    provide for printing useroriented error messages that specifically indicate
    how the look-ahead problem arose. In addition, certain loops in the digraphs
    induced by these relations indicate that the grammar is not LR(k) for any k.
    Finally, an oft-discovered and used but incorrect look-ahead set algorithm
    is similarly based on two other relations defined for the fwst time here.
    The formal presentation of this algorithm should help prevent its
    rediscovery.},
}


@article{beatty1982,
    author = {Beatty, John C.},
    title = {On the Relationship Between LL(1) and LR(1) Grammars},
    journal = {J. ACM},
    volume = {29},
    number = {4},
    month = {oct},
    year = {1982},
    issn = {0004-5411},
    pages = {1007--1022},
    numpages = {16},
    url = {http://doi.acm.org/10.1145/322344.322350},
    doi = {10.1145/322344.322350},
    acmid = {322350},
    publisher = {ACM},
    address = {New York, NY, USA},
    abstract = {It is shown that every p-reduced LL(1) grammar is LALR(1) and,
    as a corollary, that every A-free LL(1) grammar is SLR(1) A partial converse to
    this result is also demonstrated: If there is at most one marked rule m the
    basis of every state set in the canonical collection of sets of LR(k) items for
    a grammar G in which S =+> Sy impossible, then G is LL(k).},
}


@article{errorRecoveryForYaccParsers,
    number = {Number 73},
    month = {October},
    author = {Julia Anne Dain},
    series = {Department of Computer Science Research Report},
    address = {Coventry, UK},
    title = {Error recovery for YACC parsers},
    publisher = {University of Warwick. Department of Computer Science},
    year = {1985},
    keywords = {technical report},
    url = {http://wrap.warwick.ac.uk/60772/},
    abstract = {The aim to improve error recovery in parsers generated by the
    LALR parser-generator Yacc. We describe an error recovery scheme which a new
    version of Yacc automatically builds into its parsers. The scheme uses state
    information to attempt to repair input which is syntactically incorrect.
    Repair by alteration of a single token is attempted first, followed by
    replacement of a phrase of the input. A parser for the C language is
    generated from existing specifications and tested on a collection of student
    programs. The quality of error recovery and diagnostic messages is found to
    be higher than that of the existing portable C compiler. The new version of
    Yacc may be used by any current user Yacc, with minor modifications to their
    existing specifications, to produce systems with enhanced syntax error
    recovery.}
}


@article{lr1ErrorRecovery,
    author = {Sorkin, Arthur and Donovan, Peter},
    title = {LR(1) Parser Generation System: LR(1) Error Recovery, Oracles, and Generic Tokens},
    journal = {SIGSOFT Softw. Eng. Notes},
    volume = {36},
    number = {2},
    month = {mar},
    year = {2011},
    issn = {0163-5948},
    pages = {1--5},
    numpages = {5},
    url = {http://doi.acm.org/10.1145/1943371.1943391},
    doi = {10.1145/1943371.1943391},
    acmid = {1943391},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {LR(1), error recovery, generic tokens, oracles, pager, parser construction, parsing},
    abstract = {The LR(1) Parser Generation System generates full LR(1) parsers
    that are comparable in speed and size to those generated by LALR(1) parser
    generators, such as yacc [5]. In addition to the inherent advantages of full
    LR(1) parsing, it contains a number of novel features. This paper discusses
    three of them in detail: an LR(1) grammar specified automatic error recovery
    algorithm, oracles, and generic tokens. The error recovery algorithm depends
    on the fact that full LR(1) parse tables preserve context. Oracles are
    pieces of code that are defined in a grammar and that are executed between
    the scanner and parser. They are used to resolve token ambiguities,
    including semantic ones. Generic tokens are used to replace syntactically
    identical tokens with a single token, which is, in effect, a variable
    representing a set of tokens.},
}


@article{jourdan2017,
    title = {{A Simple, Possibly Correct LR Parser for C11}},
    author = {Jourdan, Jacques-Henri and Pottier, Fran{\c c}ois},
    url = {https://hal.archives-ouvertes.fr/hal-01633123},
    journal = {{ACM Transactions on Programming Languages and Systems (TOPLAS)}},
    publisher = {{ACM}},
    volume = {39},
    number = {4},
    pages = {1 - 36},
    year = {2017},
    month = {Sep},
    doi = {10.1145/3064848},
    keywords = {C language ; parsing ; ambiguity ; lexical feedback ; C89 ; C99 ; C11},
    pdf = {https://hal.archives-ouvertes.fr/hal-01633123/file/jourdan2017simple.pdf},
    abstract = {The syntax of the C programming language is described in the C11
    standard by an ambiguous context-free grammar, accompanied with English
    prose that describes the concept of " scope " and indicates how certain
    ambiguous code fragments should be interpreted. Based on these elements, the
    problem of implementing a compliant C11 parser is not entirely trivial. We
    review the main sources of difficulty and describe a relatively simple
    solution to the problem. Our solution employs the well-known technique of
    combining an LALR(1) parser with a " lexical feedback " mechanism. It draws
    on folklore knowledge and adds several original aspects , including: a twist
    on lexical feedback that allows a smooth interaction with lookahead; a
    simplified and powerful treatment of scopes; and a few amendments in the
    grammar. Although not formally verified, our parser avoids several pitfalls
    that other implementations have fallen prey to. We believe that its
    simplicity, its mostly-declarative nature, and its high similarity with the
    C11 grammar are strong informal arguments in favor of its correctness. Our
    parser is accompanied with a small suite of " tricky " C11 programs. We hope
    that it may serve as a reference or a starting point in the implementation
    of compilers and analysis tools.},
}


@Article{generatingInterpretiveTranslators,
    author="Murphree, E. L.
    and Fenves, S. J.",
    title="A technique for generating interpretive translators for problem-oriented languages",
    journal="BIT Numerical Mathematics",
    year="1970",
    month="Sep",
    day="01",
    volume="10",
    number="3",
    pages="310--323",
    issn="1572-9125",
    doi="10.1007/BF01934200",
    url="https://doi.org/10.1007/BF01934200",
    abstract="The paper presents a technique for generating translators for
    problem-oriented and other command- or data-oriented languages which can be
    interpretively executed. The system consists of: (a) a stored grammar or
    table, representing in a modified graph form the syntactically valid
    statements and the corresponding semantic actions; (b) a set of application
    procedures, written in a procedural language; (c) a universal translator,
    which performs reading, input conversion, matching input items against the
    grammar, and calling the procedures specified by the grammar. The generator
    consists of the translator and a specific grammar and set of procedures,
    which together convert the problem-oriented description of the source
    grammar into the table used by the translator for execution.",
}


@ARTICLE{anAbstractPrettyPrinter,
    author={R. D. {Cameron}},
    journal={IEEE Software},
    title={An abstract pretty printer},
    year={1988},
    volume={5},
    number={6},
    pages={61-67},
    doi={10.1109/52.10004},
    ISSN={0740-7459},
    month={Nov},
    keywords={programming environments;utility programs;programming
    environments;abstract pretty printer;low-level printing;pretty-printing
    utilities;Printers;Printing;Reactive power;Algorithms},
    abstract={The author has distilled the basic operations of the pretty
    printer into an abstract pretty printer that uses procedural parameters to
    perform low-level printing actions. By encapsulating the algorithm in one
    place, all the pretty-printing utilities will use the same algorithm, and
    the algorithm itself can be changed easily. The author describes how the
    abstract pretty printer can be used for basic design, printing to files and
    screens, setting the cursor, identifying a node, formatting text, and
    lexical changes.<>},
}


@article{universalCodeFormatter,
    title         = {Towards a Universal Code Formatter through Machine Learning},
    author        = {Terence Parr and J.J. Vinju},
    month         = {10},
    year          = {2016},
    journal       = {SLE 2016 Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    pages         = {137-151},
    doi           = {10.1145/2997364.2997383},
    isbn          = {978-1-4503-4447-0},
    location      = {Amsterdam, The Netherlands},
    url           = {https://www.researchgate.net/publication/309363024_Towards_a_universal_code_formatter_through_machine_learning},
    urlaccessdate = {2017-03-01},
    abstract      = {There are many declarative frameworks that allow us to implement code
                    formatters relatively easily for any specific language, but constructing them is
                    cumbersome. The first problem is that “everybody” wants to format their code
                    differently, leading to either many formatter variants or a ridiculous number of
                    configuration options. Second, the size of each implementation scales with a
                    language's grammar size, leading to hundreds of rules. In this paper, we solve
                    the formatter construction problem using a novel approach, one that
                    automatically derives formatters for any given language without intervention
                    from a language expert. We introduce a code formatter called CODEB UFF that uses
                    machine learning to abstract formatting rules from a representative corpus,
                    using a carefully designed feature set. Our experiments on Java, SQL, and ANTLR
                    grammars show that CODEB UFF is efficient, has excellent accuracy, and is
                    grammar invariant for a given language. It also generalizes to a 4th language
                    tested during manuscript preparation.},
}


@article{industrialApplication,
    title         = {An industrial application of context-sensitive formatting},
    author        = {M.G.J. van den Brand and A.T. Kooiker and N.P. Veerman and J.J. Vinju},
    year          = {2005},
    publisher     = {Online},
    url           = {https://www.researchgate.net/publication/228540036_An_industrial_application_of_context-sensitive_formatting},
    urlaccessdate = {2017-09-07},
    abstract      = {Automated formatting is an important technique for the software maintainer. It
                    is either applied separately to improve the readability of source code, or as
                    part of a source code transformation tool chain. In this paper we report on the
                    application of generic tools for constructing formatters. In an industrial
                    setting automated formatters need to be tailored to the requirements of the
                    customer. The (legacy) programming language or dialect and the corporate
                    formatting conventions are specific and non-negotiable. Can generic formatting
                    tools deal with such unexpected requirements? Driven by an industrial case of 78
                    thousand lines of Cobol code, several limitations in existing formatting
                    technology have been addressed. We improved its flexibility by replacing a
                    generative phase by a generic tool, and we added a little expressiveness to the
                    formatting backend. Most importantly, we employed a multi-stage formatting
                    architecture that can cope with any kind of formatting convention using more
                    computational power.},
}


@article{programIndentation,
    title         = {Program indentation and comprehensibility},
    author        = {Richard J. Miara and Joyce A. Musselman and Juan A. Navarro and Ben Shneiderman},
    month         = {11},
    year          = {1983},
    journal       = {Communications of the ACM},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    volume        = {26},
    issue         = {11},
    pages         = {861-867},
    location      = {Online},
    doi           = {10.1145/182.358437},
    url           = {https://www.researchgate.net/publication/234809222_Program_indentation_and_comprehensibility},
    urlaccessdate = {2017-09-07},
    abstract      = {The consensus in the programming community is that indentation aids program
                    comprehension, although many studies do not back this up. We tested program
                    comprehension on a Pascal program. Two styles of indentation were used --
                    blocked and non-blocked -- in addition to four passible levels of indentation
                    (0, 2, 4, 6 spaces). Both experienced and novice subjects were used. Although
                    the blocking style made no difference, the level of indentation had a
                    significant effect on program comprehension. (2--4 spaces had the highest mean
                    score for program comprehension.) We recommend that a moderate level of
                    indentation be used to increase program comprehension and user satisfaction.},
}


@article{independentFramework,
    title         = {A Language Independent Framework for Context-sensitive Formatting},
    author        = {M.G.J. Van den Brand and A.T. Kooiker and J.J. Vinju},
    month         = {03},
    year          = {2006},
    journal       = {CSMR '06 Proceedings of the Conference on Software Maintenance and Reengineering},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    volume        = {26},
    issue         = {1},
    pages         = {103-112},
    doi           = {10.1109/CSMR.2006.4},
    isbn          = {0-7695-2536-9},
    location      = {Bari, Italy},
    url           = {https://www.researchgate.net/publication/4226896_A_language_independent_framework_for_context-sensitive_formatting},
    urlaccessdate = {2017-09-07},
    abstract      = {Automated formatting is an important technique for the software maintainer. It
                    is either applied separately to improve the readability of source code, or as
                    part of a source code transformation tool chain. In this paper we report on the
                    application of generic tools for constructing formatters. In an industrial
                    setting automated formatters need to be tailored to the requirements of the
                    customer. The (legacy) programming language or dialect and the corporate
                    formatting conventions are specific and non-negotiable. Can generic formatting
                    tools deal with such unexpected requirements? Driven by an industrial case of
                    nearly 80 thousand lines of Cobol code, several limitations in existing
                    formatting technology have been addressed. We improved its flexibility by
                    replacing a generative phase by a generic tool, and we added a little
                    expressiveness to the formatting back end. Most importantly, we employed a
                    multi-stage formatting framework that can cope with any kind of formatting
                    convention using more computational power.},
}


@article{architectureFormatting,
    title         = {An architecture for context-sensitive formatting},
    author        = {M.G.J. van den Brand and A.T. Kooiker and J.J. Vinju and N.P. Veerman},
    month         = {09},
    year          = {2005},
    journal       = {21st IEEE International Conference on Software Maintenance (ICSM'05)},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    doi           = {10.1109/ICSM.2005.17},
    isbn          = {0-7695-2368-4},
    location      = {Budapest, Hungary, Hungary},
    url           = {https://www.researchgate.net/publication/4175894_An_architecture_for_context-sensitive_formatting},
    urlaccessdate = {2017-09-07},
    abstract      = {We developed an architecture for context-sensitive formatting of source code.
                    The architecture was implemented and applied in an industrial formatting case.},
}


@article{prettyPrinting,
    title         = {Pretty-printing for software reengineering},
    author        = {Merijn De Jonge},
    month         = {10},
    year          = {2002},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    journal       = {International Conference on Software Maintenance, 2002. Proceedings},
    doi           = {10.1109/ICSM.2002.1167816},
    isbn          = {0-7695-1819-2},
    location      = {Montreal, Quebec, Canada},
    url           = {https://www.researchgate.net/publication/3998748_Pretty-Printing_for_Software_Reengineering},
    urlaccessdate = {2017-09-07},
    abstract      = {Automatic software reengineerings change or repair existing software systems.
                    They are usually tailor-made for a specific customer and language dependent.
                    Maintaining similar reengineerings for multiple customers and different language
                    dialects might therefore soon become problematic unless advanced language
                    technology is being used. Generic pretty-printing is part of such technology and
                    is the subject of this paper. We discuss specific pretty-print aspects of
                    software reengineering such as fulfilling customer-specific format conventions,
                    preserving existing layout, and producing multiple output formats. In addition,
                    we describe pretty-print techniques that help to reduce maintenance effort of
                    tailor-made reengineerings supporting multiple language dialects. Applications,
                    such as COBOL reengineering and SDL documentation generation show that our
                    techniques, implemented in the generic pretty-printer GPP, are feasible.},
}


@article{massMaintenance,
    title         = {Automated Mass Maintenance of Software Assets},
    author        = {Niels Veerman},
    month         = {03},
    year          = {2007},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    journal       = {11th European Conference on Software Maintenance and Reengineering (CSMR'07)},
    doi           = {10.1109/CSMR.2007.15},
    isbn          = {0-7695-2802-3},
    location      = {Amsterdam, The Netherlands},
    url           = {https://www.researchgate.net/publication/221569579_Automated_Mass_Maintenance_of_Software_Assets},
    urlaccessdate = {2017-09-11},
    abstract      = {This is a research summary of a PhD project in the area of massive software
                    maintenance automation. We explain the context, approach, and contributions.},
}


@article{legacyAssets,
    title         = {Towards automated modification of legacy assets},
    author        = {Alex Sellink and Chris Verhoef},
    month         = {01},
    year          = {2000},
    journal       = {Annals of Software Engineering},
    publisher     = {Kluwer Academic Publishers},
    address       = {Dordrecht, The Netherlands},
    volume        = {9},
    issue         = {1-4},
    pages         = {315-336},
    location      = {Red Bank, NJ, USA},
    doi           = {10.1023/A:1018941228255},
    url           = {https://www.researchgate.net/publication/2825635_Towards_Automated_Modification_of_Legacy_Assets},
    urlaccessdate = {2017-09-11},
    abstract      = {In this paper we argue that there is a necessity for automating modifications
                    to legacy assets. We propose a five layered process for the introduction and
                    employment of tool support that enables automated modification to entire legacy
                    systems. Furthermore, we elaborately discuss each layer on a conceptual level,
                    and we make appropriate references to sources where technical contributions
                    supporting that particular layer can be found. We sketch the perspective that
                    more and more people working in the software engineering area will be
                    contributing to working on existing systems and/or tools to support such work.},
}


@article{softwarePortfolio,
    title         = {Automated maintenance of a software portfolio},
    author        = {Niels Veerman},
    month         = {10},
    year          = {2006},
    journal       = {Science of Computer Programming - Special issue on source code analysis and manipulation (SCAM 2005)},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {62},
    issue         = {3},
    pages         = {287-317},
    doi           = {10.1016/j.scico.2006.04.006},
    location      = {Amsterdam, The Netherlands},
    url           = {https://www.researchgate.net/publication/222831264_Automated_maintenance_of_a_software_portfolio},
    urlaccessdate = {2017-09-12},
    abstract      = {This is an experience report on automated mass maintenance of a large Cobol
                    software portfolio. A company in the financial services and insurance industry
                    upgraded their database system to a new version, affecting their entire software
                    portfolio. The database system was accessed by the portfolio of 45 systems,
                    totalling nearly 3000 programs and covering over 4 million lines of Cobol code.
                    We upgraded the programs to the new database version using several automatic
                    tools, and we performed an automated analysis supporting further manual
                    modifications by the system experts. The automatic tools were built using a
                    combination of lexical and syntactic technology, and they were deployed in a
                    mass update factory to allow large-scale application to the software portfolio.
                    The updated portfolio has been accepted and taken into production by the
                    company, serving over 600 employees with the new database version. In this
                    paper, we discuss the automated upgrade from problem statement to project
                    costs.},
}


@article{pushdownAutomata,
    title         = {2-Head Pushdown Automata},
    author        = {Awe Ayodeji Samson},
    month         = {06},
    year          = {2015},
    journal       = {Procedia - Social and Behavioral Sciences},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {195},
    issue         = {1},
    pages         = {2037-2046},
    doi           = {10.1016/j.sbspro.2015.06.225},
    location      = {Department of Mathematics, Eastern Mediterranean University, Mersin 10 Turkey, Famagusta, North Cyprus},
    url           = {https://www.researchgate.net/publication/282556609_2-Head_Pushdown_Automata},
    urlaccessdate = {2017-09-12},
    abstract      = {Finite state automata recognize regular languages which can be used in text
                    processing, compilers, and hardware design. Two head finite automata accept
                    linear context free languages. In addition, pushdown automata are able to
                    recognize context free languages which can be used in programming languages and
                    artificial intelligence. The finite automaton has deterministic and
                    non-deterministic version likewise the two head finite automata and the pushdown
                    automata. The deterministic version of these machines is such that there is no
                    choice of move in any situation while the non-deterministic version has a choice
                    of move. In this research the 2-head pushdown automata are described which is
                    more powerful than the pushdown automata and it is able to recognize some
                    non-context free languages as well. During this work, the main task is to
                    characterize these machines.},
}


@article{aspectOriented,
    title         = {Aspect-oriented model-driven code generation: A systematic mapping study},
    author        = {Abid Mehmood and Dayang Norhayati Abang Jawawi},
    month         = {09},
    year          = {2012},
    journal       = {Information and Software Technology},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {55},
    issue         = {2},
    pages         = {395-411},
    location      = {Department of Software Engineering, Faculty of Computer
    Science and Information Systems, Universiti Teknologi Malaysia, 81310
    Skudai, Johor, Malaysia},
    doi           = {10.1016/j.infsof.2012.09.003},
    url           = {https://www.researchgate.net/publication/257391227_Aspect-oriented_model-driven_code_generation_A_systematic_mapping_study},
    urlaccessdate = {2017-09-12},
    abstract      = {Context: Model-driven code generation is being increasingly applied to enhance
                    software development from perspectives of maintainability, extensibility and
                    reusability. However, aspect-oriented code generation from models is an area
                    that is currently underdeveloped. Objective: In this study we provide a survey
                    of existing research on aspect-oriented modeling and code generation to discover
                    current work and identify needs for future research. Method: A systematic
                    mapping study was performed to find relevant studies. Classification schemes
                    have been defined and the 65 selected primary studies have been classified on
                    the basis of research focus, contribution type and research type. Results: The
                    papers of solution proposal research type are in a majority. All together
                    aspect-oriented modeling appears being the most focused area divided into
                    modeling notations and process (36\%) and model composition and interaction
                    management (26\%). The majority of contributions are methods. Conclusion:
                    Aspect-oriented modeling and composition mechanisms have been significantly
                    discussed in existing literature while more research is needed in the area of
                    model-driven code generation. Furthermore, we have observed that previous
                    research has frequently focused on proposing solutions and thus there is need
                    for research that validates and evaluates the existing proposals in order to
                    provide firm foundations for aspect-oriented model-driven code generation.},
}


@article{aspectOrientationReview,
    title         = {A Review: Analysis of Aspect Orientation and Model Driven Engineering for Code Generation},
    author        = {Dhiraj Gurunule and Madhu Nashipudimath},
    month         = {03},
    year          = {2015},
    journal       = {Procedia Computer Science},
    publisher     = {Elsevier North-Holland, Inc.},
    address       = {Amsterdam, The Netherlands},
    volume        = {45},
    issue         = {1},
    pages         = {852-861},
    location      = {Information Technology Department and Computer Department PIIT, New Panvel, India},
    doi           = {10.1016/j.procs.2015.03.171},
    url           = {https://www.researchgate.net/publication/276899518_A_Review_Analysis_of_Aspect_Orientation_and_Model_Driven_Engineering_for_Code_Generation},
    urlaccessdate = {2017-09-12},
    abstract      = {In the development of large and complex software application software engineers
                    has to focuses on many requirements other than desired application's requirement
                    at the coding and design level. Code Generation is a technique which is use to
                    automatically generates lower level executable code from higher level design
                    artifact. Code generation provides design of the code at higher abstract level
                    so that software developers can focuses on higher level design problem
                    simultaneously meeting goals of desired application. Aspect Orientation (AO) is
                    characterizing by identification and separation of different concerns and
                    encapsulates them in modules. Concern is an interest which pertains to system
                    operation, function, development or any other things which is important to one
                    of the stakeholder. Model Driven Engineering (MDE) is a development paradigm
                    which is characterize by model transformation and uses models to support various
                    stages of the development life cycle. Model is primary artifact in MDE. In this
                    paper we analyze both techniques i.e. AO and MDE, and how they can be used for
                    code generation.},
}


@article{quantificationOfInterface,
    title         = {Quantification of interface visual complexity},
    author        = {Aliaksei Miniukovich and Antonella De Angeli},
    month         = {03},
    year          = {2014},
    journal       = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    pages         = {153-160},
    location      = {University of Trento, Como, Italy},
    doi           = {10.1145/2598153.2598173},
    isbn          = {978-1-4503-2775-6},
    url           = {https://www.researchgate.net/publication/266657991_Quantification_of_interface_visual_complexity},
    urlaccessdate = {2017-10-10},
    abstract      = {Designers strive for enjoyable user experience (UX) and put a significant
                    effort into making graphical user interfaces (GUI) both usable and beautiful.
                    Our goal is to minimize their effort: with this purpose in mind, we have been
                    studying automatic metrics of GUI qualities. These metrics could enable
                    designers to iterate their designs more quickly. We started from the
                    psychological findings that people tend to prefer simpler things. We then
                    assumed visual complexity determinants also determine visual aesthetics and
                    outlined eight of them as belonging to three dimensions: information amount
                    (visual clutter and color variability), information organization (symmetry,
                    grid, ease-of-grouping and prototypicality), and information discriminability
                    (contour density and figure-ground contrast). We investigated five determinants
                    (visual clutter, symmetry, contour density, figure-ground contrast and color
                    variability) and proposed six associated automatic metrics. These metrics take
                    screenshots of GUI as input and can thus be applied to any type of GUI. We
                    validated the metrics through a user study: we gathered the ratings of immediate
                    impressions of GUI visual complexity and aesthetics, and correlated them with
                    the output of the metrics. The output explained up to 51\% of aesthetics ratings
                    and 50\% of complexity ratings. This promising result could be further extended
                    towards the creation of tLight, our automatic GUI evaluation tool.},
}


@article{toolsForProjectManagement,
    title         = {Comparison of open source tools for project management},
    author        = {André Marques Pereira and Rafael Queiroz Gonçalves and Christiane Gresse von Wangenheim and Luigi Buglione},
    month         = {03},
    year          = {2013},
    journal       = {International Journal of Software Engineering and Knowledge Engineering},
    publisher     = {World Scientific Publishing, },
    address       = {Singapore},
    volume        = {23},
    issue         = {02},
    pages         = {189-209},
    location      = {Federal University of Santa Catarina (UFSC), Florianópolis, Santa Catarina, Brazil},
    doi           = {10.1142/S0218194013500046},
    url           = {https://www.researchgate.net/publication/273569026_Comparison_of_open_source_tools_for_project_management},
    urlaccessdate = {2017-10-27},
    abstract      = {Software projects often fail, because they are not adequately managed. The
                    establishment of effective and efficient project management practices still
                    remains a key challenge to software organizations. Striving to address these
                    needs, "best practice" models, such as, the Capability Maturity Model
                    Integration (CMMI) or the Project Management Body of Knowledge (PMBOK), are
                    being developed to assist organizations in improving project management.
                    Although not required, software tools can help implement the project management
                    process in practice. In order to provide comprehensive, low-cost tool support
                    for project management, specifically, for small and medium enterprises (SMEs),
                    in this paper we compare the most popular free/open-source web-based project
                    management tools with respect to their compliance to PMBOK and CMMI for
                    Development (CMMI-DEV). The results of this research can be used by
                    organizations to make decisions on tool adoptions as well as a basis for
                    evolving software tools in alignment with best practices models.},
}


@article{naturalCodingConventions,
    title         = {Learning Natural Coding Conventions},
    author        = {Miltiadis Allamanis and Earl T. Barr and Charles Sutton},
    month         = {11},
    year          = {2014},
    journal       = {Proceeding FSE 2014 Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    volume        = {23},
    issue         = {02},
    pages         = {281-293},
    location      = {Hong Kong, China},
    doi           = {10.1145/2635868.2635883},
    url           = {https://www.researchgate.net/publication/260250447_Learning_Natural_Coding_Conventions},
    urlaccessdate = {2017-10-27},
    abstract      = {Every programmer has a characteristic style, ranging from preferences about
                    identifier naming to preferences about object relationships and design patterns.
                    Coding conventions define a consistent syntactic style, fostering readability
                    and hence maintainability. When collaborating, programmers strive to obey a
                    project's coding conventions. However, one third of reviews of changes contain
                    feedback about coding conventions, indicating that programmers do not always
                    follow them and that project members care deeply about adherence. Unfortunately,
                    programmers are often unaware of coding conventions because inferring them
                    requires a global view, one that aggregates the many local decisions programmers
                    make and identifies emergent consensus on style. We present NATURALIZE, a
                    framework that learns the style of a codebase, and suggests revisions to improve
                    stylistic consistency. NATURALIZE builds on recent work in applying statistical
                    natural language processing to source code. We apply NATURALIZE to suggest
                    natural identifier names and formatting conventions. We present four tools
                    focused on ensuring natural code during development and release management,
                    including code review. NATURALIZE achieves 94\% accuracy in its top suggestions
                    for identifier names. We used NATURALIZE to generate 18 patches for 5 open
                    source projects: 14 were accepted.},
}


@article{codePlagiarismDetection,
    title         = {Style Analysis for Source Code Plagiarism Detection},
    author        = {Olfat Mirza and Mike Joy},
    month         = {06},
    year          = {2015},
    journal       = {International Conference Plagiarism across Europe and Beyond 2015},
    publisher     = {Online},
    address       = {\url{https://plagiarism.pefka.mendelu.cz/?sec=cf15#proc}},
    pages         = {53–61},
    location      = {Brno, Czech Republic},
    url           = {https://www.researchgate.net/publication/303932091_Style_Analysis_for_Source_Code_Plagiarism_Detection},
    urlaccessdate = {2017-10-27},
    abstract      = {Plagiarism has become an increasing problem in higher education in recent
                    years. A number of research papers have discussed the problem of plagiarism in
                    terms of text and source code and the techniques to detect it in various
                    contexts. There is a variety of easy ways of copying others' work because the
                    source code can be obtained from online source code banks and textbooks, which
                    makes plagiarism easy for students. Source code plagiarism has a very specific
                    definition, and Parker and Hamblen define plagiarism on software as “A program
                    that has been produced from another program with a small number of routine
                    transformations”. The transformations can range from very simple changes to very
                    difficult ones, which can be one of the six levels of program modifications that
                    are given by Faidhi and Robinson. Coding style is a way to detect source code
                    plagiarism because it relates to programmer personality without affecting the
                    logic of a program, and can be used to differentiate between different code
                    authors. This paper reviews a number of publications which report style
                    comparison to detect source code plagiarism in order to determine research gaps
                    and explore areas where this approach can be improved. A summary of the
                    plagiarism techniques in which style analysis can help identify plagiarism is
                    presented.},
}


@article{annotationAssistant,
    title         = {An Annotation Assistant for Interactive Debugging of Programs with Common Synchronization Idioms},
    author        = {Elmas, Tayfun and Sezgin, Ali and Tasiran, Serdar and Qadeer, Shaz},
    booktitle     = {Proceedings of the 7th Workshop on Parallel and Distributed Systems: Testing, Analysis, and Debugging},
    series        = {PADTAD '09},
    year          = {2009},
    pages         = {10:1--10:11},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    articleno     = {10},
    numpages      = {11},
    location      = {Chicago, Illinois},
    doi           = {10.1145/1639622.1639632},
    isbn          = {978-1-60558-655-7},
    acmid         = {1639632},
    url           = {http://doi.acm.org/10.1145/1639622.1639632},
    keywords      = {atomicity, concurrent programs, synchronization idioms},
    urlaccessdate = {2017-10-30},
    abstract      = {This paper explores an approach to improving the practical usability of static
                    verification tools for debugging synchronization idioms. Synchronization idioms such
                    as mutual exclusion and readers/writer locks are widely-used to ensure atomicity of
                    critical regions. We present an annotation assistant that automatically generates
                    program annotations. These annotations express noninterference between program
                    statements, ensured by the synchronization idioms, and are used to identify atomic
                    code regions. This allows the programmer to debug the use of the idioms in the
                    program. We start by formalizing several well-known idioms by providing an abstract
                    semantics for each idiom. For programs that use these idioms, we require the
                    programmer to provide a few predicates linking the idiom with its realization in
                    terms of program variables. From these, we automatically generate a proof script
                    that is mechanically checked. These scripts include steps such as automatically
                    generating assertions and annotating program actions with them, introducing
                    auxiliary variables and invariants. We have successfully shown the applicability of
                    this approach to several concurrent programs from the literature.},
}


@article{redesignOfGit,
    title         = {Purposes, Concepts, Misfits, and a Redesign of Git},
    author        = {Santiago Perez De Rosso and Daniel Jackson},
    journal       = {SIGPLAN Not.},
    volume        = {51},
    number        = {10},
    month         = {10},
    publisher     = {ACM},
    address       = {New York, NY, USA},
    year          = {2016},
    issn          = {0362-1340},
    pages         = {292--310},
    keywords      = {Git, concept design, concepts, design, software design, usability, version control},
    numpages      = {19},
    location      = {Massachusetts Institute of Technology, USA},
    doi           = {10.1145/3022671.2984018},
    acmid         = {2984018},
    url           = {https://www.researchgate.net/publication/311477527_Purposes_concepts_misfits_and_a_redesign_of_git},
    urlaccessdate = {2017-10-30},
    abstract      = {Git is a widely used version control system that is powerful but complicated.
                    Its complexity may not be an inevitable consequence of its power but rather
                    evidence of flaws in its design. To explore this hypothesis, we analyzed the
                    design of Git using a theory that identifies concepts, purposes, and misfits.
                    Some well-known difficulties with Git are described, and explained as misfits in
                    which underlying concepts fail to meet their intended purpose. Based on this
                    analysis, we designed a reworking of Git (called Gitless) that attempts to
                    remedy these flaws. To correlate misfits with issues reported by users, we
                    conducted a study of Stack Overflow questions. And to determine whether users
                    experienced fewer complications using Gitless in place of Git, we conducted a
                    small user study. Results suggest our approach can be profitable in identifying,
                    analyzing, and fixing design problems.},
}


@article{codeClassification,
    title         = {Code Classification as a Learning and Assessment Exercise for Novice Programmers},
    author        = {Errol Thompson and Jacqueline L. Whalley and Raymond Lister and Beth Simon},
    journal       = {Proceedings of the 19th Annual Conference of the National Advisory Committee on Computing Qualifications},
    month         = {07},
    year          = {2006},
    pages         = {291--298},
    publisher     = {Online},
    address       = {Aston University, Computer Science Department},
    location      = {NACCQ, Wellington, New Zealand},
    url           = {https://www.researchgate.net/publication/255948009_Code_Classification_as_a_Learning_and_Assessment_Exercise_for_Novice_Programmers},
    urlaccessdate = {2017-10-31},
    abstract      = {When students are given code that is very similar in structure or purpose, how
                    well do they actually recognise the similarities and differences? As part of the
                    BRACElet project, a multi-institutional investigation into reading and
                    comprehension skills of novice programmers, students were asked to classify four
                    code segments that found the minimum or maximum in an array of numbers. This
                    paper reports on the analysis of responses to this question and draws
                    conclusions about the students' ability to recognise the similarities and
                    differences in example code. It then raises questions with respect to an
                    approach to teaching that uses variations in code examples. Received Citrus
                    Award for Collaborative Research and highly recommended in the Best Paper awards
                    },
}


@article{codeScanningPatterns,
    title         = {Code Scanning Patterns in Program Comprehension},
    author        = {Christoph Aschwanden and Martha Crosby},
    journal       = {Proceedings of the 39th Annual Hawaii International Conference on System Sciences},
    month         = {01},
    year          = {2006},
    publisher     = {Online},
    address       = {Department of Information and Computer Sciences},
    location      = {University of Hawaii at Manoa},
    url           = {https://www.researchgate.net/publication/250718584_Code_Scanning_Patterns_in_Program_Comprehension},
    urlaccessdate = {2017-10-31},
    abstract      = {Various publications have identified Beacons to play a key role in program
                    comprehension. Beacons are code fragments that help developers comprehend
                    programs. It has been shown that expert programmers pay more attention to
                    Beacons than novices. Beacons are described as the link between source code and
                    hypothesis verification. Beacons are sets of key features that typically
                    indicate the presence of a particular data structure or operation in source
                    code. However, only little research has been done trying to identify and explain
                    them in greater detail. It has been demonstrated that good variable and
                    procedure names help in program comprehension. Documentation is beneficial as
                    well. The so-called swap operation for variables is a strong indicator for a
                    sorting algorithm. We conducted an eye tracking study using the EventStream
                    software framework as the instrument to investigate programmers' behavior during
                    a code reading exercise. Preliminary results suggest Beacons to be present when
                    the longest fixation duration is thousand milliseconds or higher. Comparing
                    participants with correct understanding versus participants with wrong
                    understanding showed differences in focus of attention. Based on the study
                    conducted, we suggest to consider "int k=(a+b)/2" as Beacons during program
                    comprehension as well as lines of code which exhibit very long fixations above
                    1000 milliseconds.},
}


@article{programUnderstanding,
    title         = {The Use of Reading Technique and Visualization for Program Understanding},
    author        = {Daniel Porto and Manoel G. Mendonça and Sandra Camargo Pinto Ferraz Fabbri},
    year          = {2009},
    month         = {07},
    pages         = {386--391},
    publisher     = {Online},
    address       = {\url{https://www.researchgate.net/profile/Sarah_Printy/publication/221391313_Enhancing_Property_Specification_Tools_With_Validation_Techniques/links/0deec52af54ccf046e000000.pdf#page=411}},
    location      = {Boston, Massachusetts},
    isbn          = {1-891706-24-1},
    url           = {https://www.researchgate.net/publication/221390090_The_Use_of_Reading_Technique_and_Visualization_for_Program_Understanding},
    urlaccessdate = {2017-10-31},
    abstract      = {Code comprehension is the basis for many other activities in software
                    engineering. It is also time consuming and can greatly profit from tools that decrease the time
                    that it usually consumes. This paper presents a tool named CRISTA that supports code
                    comprehension through the application of Stepwise Abstraction. It uses a visual metaphor to
                    represent the code and supports essential tasks for code reading, inspection and documentation.
                    Three case studies were carried out to evaluate the tool with respect to usability and
                    usefulness. In all of them the experiment participants considered that the tool facilitates code
                    comprehension, inspection and documentation.},
}


@article{theImpactOfIdentifierStyle,
    title         = {The impact of identifier style on effort and comprehension},
    author        = {Binkley, Dave and Davis, Marcia and Lawrie, Dawn and Maletic, Jonathan I. and Morrell, Christopher and Sharif, Bonita},
    journal       = {Empirical Software Engineering},
    year          = {2013},
    month         = {Apr},
    volume        = {18},
    number        = {2},
    publisher     = {Springer},
    address       = {Berlin, Germany},
    pages         = {219--276},
    location      = {Department of Computer Science, Loyola University Maryland, Baltimore, USA},
    issn          = {1573-7616},
    doi           = {10.1007/s10664-012-9201-4},
    url           = {https://www.researchgate.net/publication/257560017_The_impact_of_identifier_style_on_effort_and_comprehension},
    urlaccessdate = {2017-10-31},
    abstract      = {A family of studies investigating the impact of program identifier style on
                    human comprehension is presented. Two popular identifier styles are examined,
                    namely camel case and underscore. The underlying hypothesis is that identifier
                    style affects the speed and accuracy of comprehending source code. To
                    investigate this hypothesis, five studies were designed and conducted. The first
                    study, which investigates how well humans read identifiers in the two different
                    styles, focuses on low-level readability issues. The remaining four studies
                    build on the first to focus on the semantic implications of identifier style.
                    The studies involve 150 participants with varied demographics from two different
                    universities. A range of experimental methods is used in the studies including
                    timed testing, read aloud, and eye tracking. These methods produce a broad set
                    of measurements and appropriate statistical methods, such as regression models
                    and Generalized Linear Mixed Models (GLMMs), are applied to analyze the results.
                    While unexpected, the results demonstrate that the tasks of reading and
                    comprehending source code is fundamentally different from those of reading and
                    comprehending natural language. Furthermore, as the task becomes similar to
                    reading prose, the results become similar to work on reading natural language
                    text. For more ``source focused'' tasks, experienced software developers appear
                    to be less affected by identifier style; however, beginners benefit from the use
                    of camel casing with respect to accuracy and effort.},
}


@article{womenAndMen,
    title         = {Women and men -- Different but equal: On the impact of identifier style on source code reading},
    author        = {Zohreh Sharafi and Zéphyrin Soh and Yann-Gaël Guéhéneuc},
    year          = {2012},
    month         = {06},
    journal       = {20th IEEE International Conference on Program Comprehension},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    location      = {Passau, Germany},
    issn          = {1092-8138},
    doi           = {10.1109/ICPC.2012.6240505},
    url           = {https://www.researchgate.net/publication/261095133_Women_and_men_-_Different_but_equal_On_the_impact_of_identifier_style_on_source_code_reading},
    urlaccessdate = {2017-10-31},
    abstract      = {Program comprehension is preliminary to any program evolution task. Researchers
                    agree that identifiers play an important role in code reading and program understanding
                    activities. Yet, to the best of our knowledge, only one work investigated the impact of gender
                    on the memorability of identifiers and thus, ultimately, on program comprehension. This paper
                    reports the results of an experiment involving 15 male subjects and nine female subjects to
                    study the impact of gender on the subjects' visual effort, required time, as well as accuracy to
                    recall Camel Case versus Underscore identifiers in source code reading. We observe no
                    statistically-significant difference in term of accuracy, required time, and effort. However,
                    our data supports the conjecture that male and female subjects follow different comprehension
                    strategies: female subjects seem to carefully weight all options and spend more time to rule out
                    wrong answers while male subjects seem to quickly set their minds on some answers, possibly the
                    wrong ones. Indeed, we found that the effort spent on wrong answers is significantly higher for
                    female subjects and that there is an interaction between the effort that female subjects
                    invested on wrong answers and their higher percentages of correct answers when compared to male
                    subjects.},
}


@article{autofoldingForSourceCode,
    title         = {Autofolding for Source Code Summarization},
    author        = {Jaroslav Fowkes and Pankajan Chanthirasegaran and Razvan Ranca},
    journal       = {IEEE Transactions on Software Engineering},
    year          = {2017},
    month         = {02},
    volume        = {PP},
    issue         = {99},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    location      = {School of Informatics, University of Edinburgh, Edinburgh, UK},
    issn          = {0098-5589},
    doi           = {10.1109/TSE.2017.2664836},
    url           = {https://www.researchgate.net/publication/260911164_Autofolding_for_Source_Code_Summarization},
    urlaccessdate = {2017-10-31},
    abstract      = {Developers spend much of their time reading and browsing source code, raising
                    new opportunities for summarization methods. Indeed, modern code editors provide
                    code folding, which allows one to selectively hide blocks of code. However this
                    is impractical to use as folding decisions must be made manually or based on
                    simple rules. We introduce the autofolding problem, which is to automatically
                    create a code summary by folding less informative code regions. We present a
                    novel solution by formulating the problem as a sequence of AST folding
                    decisions, leveraging a scoped topic model for code tokens. On an annotated set
                    of popular open source projects, we show that our summarizer outperforms simpler
                    baselines, yielding a 28\% error reduction. Furthermore, we find through a case
                    study that our summarizer is strongly preferred by experienced developers. More
                    broadly, we hope this work will aid program comprehension by turning code
                    folding into a usable and valuable tool.},
}


@article{quantifyingProgramComprehension,
    title         = {Quantifying Program Comprehension with Interaction Data},
    author        = {Roberto Minelli and Andrea Mocci and Michele Lanza},
    journal       = {14th International Conference on Quality Software},
    year          = {2014},
    month         = {10},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    location      = {Dallas, TX, USA},
    isbn          = {978-1-4799-7198-5},
    doi           = {10.1109/QSIC.2014.11},
    url           = {https://www.researchgate.net/publication/286487172_Quantifying_Program_Comprehension_with_Interaction_Data},
    urlaccessdate = {2017-10-31},
    abstract      = {It is common knowledge that program comprehension takes up a substantial part
                    of software development. This "urban legend" is based on work that dates back
                    decades, which throws up the question whether the advances in software
                    development tools, techniques, and methodologies that have emerged since then
                    may invalidate or confirm the claim. We present an empirical investigation which
                    goal is to confirm or reject the claim, based on interaction data which captures
                    the user interface activities of developers. We use interaction data to
                    empirically quantify the distribution of different developer activities during
                    software development: In particular, we focus on estimating the role of program
                    comprehension. In addition, we investigate if and how different developers and
                    session types influence the duration of such activities. We analyze interaction
                    data from two different contexts: One comes from the ECLIPSE IDE on Java source
                    code development, while the other comes from the PHARO IDE on Smalltalk source
                    code development. We found evidence that code navigation and editing occupies
                    only a small fraction of the time of developers, while the vast majority of the
                    time is spent on reading \& understanding source code. In essence, the importance
                    of program comprehension was significantly underestimated by previous
                    research.},
}


@article{syntaxHighlightingInfluencing,
    title         = {Syntax highlighting as an influencing factor when reading and comprehending source code},
    author        = {Tanya R. Beelders and Jean-Pierre L. du Plessis},
    year          = {2015},
    year          = {01},
    publisher     = {Journal of Eye Movement Research},
    address       = {Bern, Switzerland},
    number        = {1},
    volume        = {9},
    issn          = {1995-8692},
    location      = {University of the Free State, Bloemfontein, Free State, South Africa},
    url           = {https://bop.unibe.ch/index.php/JEMR/article/view/2429},
    urlaccessdate = {2017-11-01},
    keywords      = {eye tracking; syntax highlighting; code comprehension; reading behaviour},
    abstract      = {Syntax highlighting or syntax colouring, plays a vital role in programming
                    development environments by colour-coding various code elements differently. The
                    supposition is that this syntax highlighting assists programmers when reading
                    and analysing code. However, academic text books are largely only available in
                    black-and-white which could influence the comprehension of novice and beginner
                    programmers. This study investigated whether student programmers experience more
                    difficulty in reading and comprehending source code when it is presented without
                    syntax highlighting. Number of fixations, fixation durations and regressions
                    were all higher for black-and-white code than for colour code but not
                    significantly so. Subjectively students indicated that the colour code snippets
                    were easier to read and more aesthetically pleasing. Based on the analysis it
                    could be concluded that students do not experience significantly more difficulty
                    when reading code in black-and-white as printed in text books.},
}


@article{theRoleOfMethodChains,
    title         = {The Role of Method Chains and Comments in Software Readability and Comprehension; An Experiment},
    author        = {J. Börstler and B. Paech},
    journal       = {IEEE Transactions on Software Engineering},
    year          = {2016},
    month         = {09},
    volume        = {42},
    number        = {9},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    pages         = {886-898},
    keywords      = {software maintenance;software metrics;cloze tests;code comments;method
                    chains;software comprehension;software maintenance;software measurement;software
                    readability;Complexity theory;Guidelines;Object oriented modeling;Programming;Software;Software
                    engineering;Software measurement;Software readability;comments;experiment;method chains;software
                    comprehension;software measurement},
    ISSN          = {0098-5589},
    location      = {Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden},
    doi           = {10.1109/TSE.2016.2527791},
    url           = {https://www.researchgate.net/publication/294119566_The_Role_of_Method_Chains_and_Comments_in_Software_Readability_and_Comprehension_-_An_Experiment},
    urlaccessdate = {2017-11-02},
    abstract      = {Software readability and comprehension are important factors in software
                    maintenance. There is a large body of research on software measurement, but the actual factors
                    that make software easier to read or easier to comprehend are not well understood. In the
                    present study, we investigate the role of method chains and code comments in software
                    readability and comprehension. Our analysis comprises data from 104 students with varying
                    programming experience. Readability and comprehension were measured by perceived readability,
                    reading time and performance on a simple cloze test. Regarding perceived readability, our
                    results show statistically significant differences between comment variants, but not between
                    method chain variants. Regarding comprehension, there are no significant differences between
                    method chain or comment variants. Student groups with low and high experience, respectively,
                    show significant differences in perceived readability and performance on the cloze tests. Our
                    results do not show any significant relationships between perceived readability and the other
                    measures taken in the present study. Perceived readability might therefore be insufficient as
                    the sole measure of software readability or comprehension. We also did not find any
                    statistically significant relationships between size and perceived readability, reading time and
                    comprehension.},
}


@article{blindAndSightedProgrammers,
    title         = {A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers},
    journal       = {IEEE Transactions on Software Engineering},
    author        = {A. Armaly and P. Rodeghero and C. McMillan},
    year          = {2017},
    month         = {07},
    volume        = {PP},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    number        = {99},
    pages         = {1-1},
    keywords      = {Blindness;Navigation;Programming profession;Software;Software
                    engineering;Tools;Program comprehension;accessibility technology;blindness},
    doi           = {10.1109/TSE.2017.2729548},
    ISSN          = {0098-5589},
    location      = {University of Notre Dame, Notre Dame, Indiana, United States},
    url           = {https://www.researchgate.net/publication/318576489_Blindness_and_Program_Comprehension},
    urlaccessdate = {2017-11-02},
    abstract      = {Programmers who are blind use a screen reader to speak source code one word at
                    a time, as though the code were text. This process of reading is in stark
                    contrast to sighted programmers, who skim source code rapidly with their eyes.
                    At present, it is not known whether the difference in these processes has
                    effects on the program comprehension gained from reading code. These effects are
                    important because they could reduce both the usefulness of accessibility tools
                    and the generalizability of software engineering studies to persons with low
                    vision. In this paper, we present an empirical study comparing the program
                    comprehension of blind and sighted programmers. We found that both blind and
                    sighted programmers prioritize reading method signatures over other areas of
                    code. Both groups obtained an equal and high degree of comprehension, despite
                    the different reading processes.},
}


@article{usingVersionControlData,
    title         = {Using version control data to evaluate the impact of software tools: a case study of the Version Editor},
    author        = {D. L. Atkins and T. Ball and T. L. Graves and A. Mockus},
    journal       = {IEEE Transactions on Software Engineering},
    volume        = {28},
    publisher     = {IEEE Computer Society},
    address       = {New York, NY, USA},
    number        = {7},
    year          = {2002},
    month         = {07},
    pages         = {625-637},
    doi           = {10.1109/TSE.2002.1019478},
    ISSN          = {0098-5589},
    location      = {Oregon Univ., Eugene, OR, USA},
    url           = {http://ieeexplore.ieee.org/document/1019478/},
    urlaccessdate = {2017-11-03},
    keywords      = {configuration management;software metrics;software quality;software tools;text
                    editing;C source files;VE tool;Version Editor;change history;large
                    organizations;preprocessor directives;software effort analysis method;software
                    maintainability;software quality;software tool impact evaluation;text
                    editors;tool usage statistics;version control data;Computer aided software
                    engineering;Control system analysis;Control systems;History;Software
                    maintenance;Software quality;Software tools;Standards development;Statistical
                    analysis;Statistics},
    abstract      = {Software tools can improve the quality and maintainability of software, but are
                    expensive to acquire, deploy, and maintain, especially in large organizations.
                    We explore how to quantify the effects of a software tool once it has been
                    deployed in a development environment. We present an effort-analysis method that
                    derives tool usage statistics and developer actions from a project's change
                    history (version control system) and uses a novel effort estimation algorithm to
                    quantify the effort savings attributable to tool usage. We apply this method to
                    assess the impact of a software tool called VE, a version-sensitive editor used
                    in Bell Labs. VE aids software developers in coping with the rampant use of
                    certain preprocessor directives (similar to \#if/\#endif in C source files). Our
                    analysis found that developers were approximately 40 percent more productive
                    when using VE than when using standard text editors.},
}


@article{findingRegressionsInProjects,
    title         = {Finding Regressions in Projects under Version Control Systems},
    author        = {Jaroslav Bendik and Nikola Benes and Ivana Cerna},
    journal       = {Computing Research Repository},
    volume        = {abs/1708.06623},
    publisher     = {Cornell University Library},
    address       = {Ithaca, NY, USA},
    year          = {2017},
    month         = {10},
    archivePrefix = {arXiv},
    location      = {Masaryk University, Brno, Czech Republic},
    eprint        = {1708.06623},
    biburl        = {http://dblp.org/rec/bib/journals/corr/abs-1708-06623},
    bibsource     = {dblp computer science bibliography, http://dblp.org},
    url           = {http://arxiv.org/abs/1708.06623},
    urlaccessdate = {2017-11-03},
    abstract      = {Version Control Systems (VCS) are frequently used to support development of
                    large-scale software projects. A typical VCS repository of a large project can
                    contain various intertwined branches consisting of a large number of commits. If
                    some kind of unwanted behaviour (e.g. a bug in the code) is found in the
                    project, it is desirable to find the commit that introduced it. Such commit is
                    called a regression point. There are two main issues regarding the regression
                    points. First, detecting whether the project after a certain commit is correct
                    can be very expensive as it may include large-scale testing and/or some other
                    forms of verification. It is thus desirable to minimise the number of such
                    queries. Second, there can be several regression points preceding the actual
                    commit; perhaps a bug was introduced in a certain commit, inadvertently fixed
                    several commits later, and then reintroduced in a yet later commit. In order to
                    fix the actual commit it is usually desirable to find the latest regression
                    point. The currently used distributed VCS contain methods for regression
                    identification, see e.g. the git bisect tool. In this paper, we present a new
                    regression identification algorithm that outperforms the current tools by
                    decreasing the number of validity queries. At the same time, our algorithm tends
                    to find the latest regression points which is a feature that is missing in the
                    state-of-the-art algorithms. The paper provides an experimental evaluation of
                    the proposed algorithm and compares it to the state-of-the-art tool git bisect
                    on a real data set.},
}

