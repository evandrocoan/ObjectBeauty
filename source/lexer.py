
"[@0,0:3='\r\n\r\n'<_NL>,1:1]"
"[@1,4:18='language_syntax'<RULE>,3:1]"
"[@2,19:19=':'<_COLON>,3:16]"
"[@3,21:28='_NEWLINE'<TERMINAL>,3:18]"
"[@4,29:29='?'<OP>,3:26]"
"[@5,31:49='preamble_statements'<RULE>,3:28]"
"[@6,51:58='_NEWLINE'<TERMINAL>,3:48]"
"[@7,59:59='?'<OP>,3:56]"
"[@8,61:84='language_construct_rules'<RULE>,3:58]"
"[@9,86:93='_NEWLINE'<TERMINAL>,3:83]"
"[@10,94:94='?'<OP>,3:91]"
"[@11,96:96='('<_LPAR>,3:93]"
"[@12,98:125='miscellaneous_language_rules'<RULE>,3:95]"
"[@13,127:134='_NEWLINE'<TERMINAL>,3:124]"
"[@14,135:135='?'<OP>,3:132]"
"[@15,137:137=')'<_RPAR>,3:134]"
"[@16,138:138='*'<OP>,3:135]"
"[@17,140:147='_NEWLINE'<TERMINAL>,3:137]"
"[@18,148:148='?'<OP>,3:145]"
"[@19,149:152='\r\n\r\n'<_NL>,3:146]"
"[@20,153:171='preamble_statements'<RULE>,5:1]"
"[@21,172:172=':'<_COLON>,5:20]"
"[@22,174:174='('<_LPAR>,5:22]"
"[@23,176:176='('<_LPAR>,5:24]"
"[@24,178:207='target_language_name_statement'<RULE>,5:26]"
"[@25,209:209='|'<_OR>,5:57]"
"[@26,211:237='master_scope_name_statement'<RULE>,5:59]"
"[@27,239:239=')'<_RPAR>,5:87]"
"[@28,241:248='_NEWLINE'<TERMINAL>,5:89]"
"[@29,250:250=')'<_RPAR>,5:98]"
"[@30,251:251='+'<OP>,5:99]"
"[@31,252:253='\r\n'<_NL>,5:100]"
"[@32,254:277='language_construct_rules'<RULE>,6:1]"
"[@33,278:278=':'<_COLON>,6:25]"
'[@34,280:289=\'"contexts"\'<STRING>,6:27]'
'[@35,291:293=\'":"\'<STRING>,6:38]'
"[@36,295:311='indentation_block'<RULE>,6:42]"
"[@37,312:313='\r\n'<_NL>,6:59]"
"[@38,314:341='miscellaneous_language_rules'<RULE>,7:1]"
"[@39,342:342=':'<_COLON>,7:29]"
"[@40,344:352='/[^:\\n]+/'<REGEXP>,7:31]"
'[@41,354:356=\'":"\'<STRING>,7:41]'
"[@42,358:374='indentation_block'<RULE>,7:45]"
"[@43,375:378='\r\n\r\n'<_NL>,7:62]"
"[@44,379:408='target_language_name_statement'<RULE>,9:1]"
"[@45,409:409=':'<_COLON>,9:31]"
'[@46,411:416=\'"name"\'<STRING>,9:33]'
'[@47,418:420=\'":"\'<STRING>,9:40]'
"[@48,422:438='free_input_string'<RULE>,9:44]"
"[@49,439:440='\r\n'<_NL>,9:61]"
"[@50,441:467='master_scope_name_statement'<RULE>,10:1]"
"[@51,468:468=':'<_COLON>,10:28]"
'[@52,470:476=\'"scope"\'<STRING>,10:30]'
'[@53,478:480=\'":"\'<STRING>,10:38]'
"[@54,482:498='free_input_string'<RULE>,10:42]"
"[@55,499:502='\r\n\r\n'<_NL>,10:59]"
"[@56,595:595='\n'<_NL>,12:93]"
"[@57,596:612='indentation_block'<RULE>,13:1]"
"[@58,613:613=':'<_COLON>,13:18]"
'[@59,615:617=\'"{"\'<STRING>,13:20]'
"[@60,619:626='_NEWLINE'<TERMINAL>,13:24]"
"[@61,628:628='('<_LPAR>,13:33]"
"[@62,630:644='statements_list'<RULE>,13:35]"
"[@63,646:653='_NEWLINE'<TERMINAL>,13:51]"
"[@64,655:655=')'<_RPAR>,13:60]"
"[@65,656:656='+'<OP>,13:61]"
'[@66,658:660=\'"}"\'<STRING>,13:63]'
"[@67,661:662='\r\n'<_NL>,13:66]"
"[@68,663:677='statements_list'<RULE>,14:1]"
"[@69,678:678=':'<_COLON>,14:16]"
"[@70,680:694='match_statement'<RULE>,14:18]"
"[@71,696:696='|'<_OR>,14:34]"
"[@72,698:714='include_statement'<RULE>,14:36]"
"[@73,716:716='|'<_OR>,14:54]"
"[@74,718:731='push_statement'<RULE>,14:56]"
"[@75,733:733='|'<_OR>,14:71]"
"[@76,735:747='pop_statement'<RULE>,14:73]"
"[@77,749:749='|'<_OR>,14:87]"
"[@78,751:770='meta_scope_statement'<RULE>,14:89]"
"[@79,771:774='\r\n\r\n'<_NL>,14:109]"
"[@80,775:788='push_statement'<RULE>,16:1]"
"[@81,789:789=':'<_COLON>,16:15]"
'[@82,792:797=\'"push"\'<STRING>,16:18]'
'[@83,799:801=\'":"\'<STRING>,16:25]'
"[@84,803:819='indentation_block'<RULE>,16:29]"
"[@85,820:821='\r\n'<_NL>,16:46]"
"[@86,822:838='include_statement'<RULE>,17:1]"
"[@87,839:839=':'<_COLON>,17:18]"
'[@88,841:849=\'"include"\'<STRING>,17:20]'
'[@89,851:853=\'":"\'<STRING>,17:30]'
"[@90,855:871='free_input_string'<RULE>,17:34]"
"[@91,872:875='\r\n\r\n'<_NL>,17:51]"
"[@92,876:891='match_statements'<RULE>,19:1]"
"[@93,892:892=':'<_COLON>,19:17]"
"[@94,894:903='scope_name'<RULE>,19:19]"
"[@95,905:905='|'<_OR>,19:30]"
"[@96,907:921='capturing_block'<RULE>,19:32]"
"[@97,923:923='|'<_OR>,19:48]"
"[@98,925:939='statements_list'<RULE>,19:50]"
"[@99,940:941='\r\n'<_NL>,19:65]"
"[@100,942:956='match_statement'<RULE>,20:1]"
"[@101,957:957=':'<_COLON>,20:16]"
'[@102,959:965=\'"match"\'<STRING>,20:18]'
'[@103,967:969=\'":"\'<STRING>,20:26]'
"[@104,971:987='free_input_string'<RULE>,20:30]"
'[@105,989:991=\'"{"\'<STRING>,20:48]'
"[@106,993:993='('<_LPAR>,20:52]"
"[@107,995:1002='_NEWLINE'<TERMINAL>,20:54]"
"[@108,1004:1019='match_statements'<RULE>,20:63]"
"[@109,1021:1021=')'<_RPAR>,20:80]"
"[@110,1022:1022='*'<OP>,20:81]"
"[@111,1024:1031='_NEWLINE'<TERMINAL>,20:83]"
'[@112,1033:1035=\'"}"\'<STRING>,20:92]'
"[@113,1036:1037='\r\n'<_NL>,20:95]"
"[@114,1038:1047='scope_name'<RULE>,21:1]"
"[@115,1048:1048=':'<_COLON>,21:11]"
'[@116,1050:1056=\'"scope"\'<STRING>,21:13]'
'[@117,1058:1060=\'":"\'<STRING>,21:21]'
"[@118,1062:1078='free_input_string'<RULE>,21:25]"
"[@119,1079:1082='\r\n\r\n'<_NL>,21:42]"
"[@120,1083:1097='capturing_block'<RULE>,23:1]"
"[@121,1098:1098=':'<_COLON>,23:16]"
'[@122,1100:1109=\'"captures"\'<STRING>,23:18]'
'[@123,1111:1113=\'":"\'<STRING>,23:29]'
'[@124,1115:1117=\'"{"\'<STRING>,23:33]'
"[@125,1119:1119='('<_LPAR>,23:37]"
"[@126,1121:1128='_NEWLINE'<TERMINAL>,23:39]"
"[@127,1130:1144='capturing_lines'<RULE>,23:48]"
"[@128,1146:1146=')'<_RPAR>,23:64]"
"[@129,1147:1147='+'<OP>,23:65]"
"[@130,1149:1156='_NEWLINE'<TERMINAL>,23:67]"
'[@131,1158:1160=\'"}"\'<STRING>,23:76]'
"[@132,1161:1162='\r\n'<_NL>,23:79]"
"[@133,1163:1177='capturing_lines'<RULE>,24:1]"
"[@134,1178:1178=':'<_COLON>,24:16]"
"[@135,1180:1186='INTEGER'<TERMINAL>,24:18]"
"[@136,1187:1187='+'<OP>,24:25]"
'[@137,1189:1191=\'":"\'<STRING>,24:27]'
"[@138,1193:1209='free_input_string'<RULE>,24:31]"
"[@139,1210:1213='\r\n\r\n'<_NL>,24:48]"
"[@140,1214:1226='pop_statement'<RULE>,26:1]"
"[@141,1227:1227=':'<_COLON>,26:14]"
'[@142,1229:1233=\'"pop"\'<STRING>,26:16]'
'[@143,1235:1237=\'":"\'<STRING>,26:22]'
"[@144,1239:1255='free_input_string'<RULE>,26:26]"
"[@145,1256:1257='\r\n'<_NL>,26:43]"
"[@146,1258:1277='meta_scope_statement'<RULE>,27:1]"
"[@147,1278:1278=':'<_COLON>,27:21]"
'[@148,1280:1291=\'"meta_scope"\'<STRING>,27:23]'
'[@149,1293:1295=\'":"\'<STRING>,27:36]'
"[@150,1297:1313='free_input_string'<RULE>,27:40]"
"[@151,1314:1317='\r\n\r\n'<_NL>,27:57]"
"[@152,1349:1349='\n'<_NL>,29:32]"
"[@153,1350:1351='CR'<TERMINAL>,30:1]"
"[@154,1352:1352=':'<_COLON>,30:3]"
'[@155,1354:1357=\'"/r"\'<STRING>,30:5]'
"[@156,1358:1359='\r\n'<_NL>,30:9]"
"[@157,1360:1361='LF'<TERMINAL>,31:1]"
"[@158,1362:1362=':'<_COLON>,31:3]"
'[@159,1364:1367=\'"/n"\'<STRING>,31:5]'
"[@160,1368:1369='\r\n'<_NL>,31:9]"
"[@161,1370:1375='SPACES'<TERMINAL>,32:1]"
"[@162,1376:1376=':'<_COLON>,32:7]"
"[@163,1378:1387='/[\\t \\f]+/'<REGEXP>,32:9]"
"[@164,1388:1391='\r\n\r\n'<_NL>,32:19]"
"[@165,1427:1427='\n'<_NL>,34:36]"
"[@166,1428:1444='free_input_string'<RULE>,35:1]"
"[@167,1445:1445=':'<_COLON>,35:18]"
"[@168,1447:1466='/(\\\\{|\\\\}|[^\\n{}])+/'<REGEXP>,35:20]"
"[@169,1467:1470='\r\n\r\n'<_NL>,35:40]"
"[@170,1471:1489='SINGLE_LINE_COMMENT'<TERMINAL>,37:1]"
"[@171,1490:1490=':'<_COLON>,37:20]"
"[@172,1492:1508='/(\\#|\\/\\/)[^\\n]*/'<REGEXP>,37:22]"
"[@173,1509:1510='\r\n'<_NL>,37:39]"
"[@174,1511:1528='MULTI_LINE_COMMENT'<TERMINAL>,38:1]"
"[@175,1529:1529=':'<_COLON>,38:19]"
"[@176,1531:1550='/\\/\\*([\\s\\S]*?)\\*\\//'<REGEXP>,38:21]"
"[@177,1551:1554='\r\n\r\n'<_NL>,38:41]"
"[@178,1555:1561='NEWLINE'<TERMINAL>,40:1]"
"[@179,1562:1562=':'<_COLON>,40:8]"
"[@180,1564:1564='('<_LPAR>,40:10]"
"[@181,1565:1566='CR'<TERMINAL>,40:11]"
"[@182,1567:1567='?'<OP>,40:13]"
"[@183,1569:1570='LF'<TERMINAL>,40:15]"
"[@184,1571:1571=')'<_RPAR>,40:17]"
"[@185,1572:1572='+'<OP>,40:18]"
"[@186,1573:1574='\r\n'<_NL>,40:19]"
"[@187,1575:1582='_NEWLINE'<TERMINAL>,41:1]"
"[@188,1583:1583=':'<_COLON>,41:9]"
"[@189,1585:1585='('<_LPAR>,41:11]"
"[@190,1587:1599='/\\r?\\n[\\t ]*/'<REGEXP>,41:13]"
"[@191,1601:1601='|'<_OR>,41:27]"
"[@192,1603:1621='SINGLE_LINE_COMMENT'<TERMINAL>,41:29]"
"[@193,1623:1623=')'<_RPAR>,41:49]"
"[@194,1624:1624='+'<OP>,41:50]"
"[@195,1625:1626='\r\n'<_NL>,41:51]"
"[@196,1627:1633='INTEGER'<TERMINAL>,42:1]"
"[@197,1634:1634=':'<_COLON>,42:8]"
'[@198,1636:1638=\'"0"\'<STRING>,42:10]'
"[@199,1639:1639='.'<_DOT>,42:13]"
"[@200,1640:1640='.'<_DOT>,42:14]"
'[@201,1641:1643=\'"9"\'<STRING>,42:15]'
"[@202,1644:1647='\r\n\r\n'<_NL>,42:18]"
"[@203,1648:1654='%ignore'<_IGNORE>,44:1]"
"[@204,1656:1661='SPACES'<TERMINAL>,44:9]"
"[@205,1662:1663='\r\n'<_NL>,44:15]"
"[@206,1664:1670='%ignore'<_IGNORE>,45:1]"
"[@207,1672:1689='MULTI_LINE_COMMENT'<TERMINAL>,45:9]"
"[@208,1690:1691='\r\n'<_NL>,45:27]"
"[@209,1692:1698='%ignore'<_IGNORE>,46:1]"
"[@210,1700:1718='SINGLE_LINE_COMMENT'<TERMINAL>,46:9]"
"[@211,1719:1722='\r\n\r\n'<_NL>,46:28]"
"[@212,1723:1730='%declare'<_DECLARE>,48:1]"
"[@213,1732:1738='_INDENT'<TERMINAL>,48:10]"
"[@214,1740:1746='_DEDENT'<TERMINAL>,48:18]"
"[@215,1747:1748='\r\n'<_NL>,48:25]"
"[@216,1766:1766='\n'<_NL>,49:18]"
"[@217,1784:1785='\n\n'<_NL>,50:18]"
# The file was automatically generated by Lark v0.6.4
#
#
#   Lark Stand-alone Generator Tool
# ----------------------------------
# Generates a stand-alone LALR(1) parser with a standard lexer
#
# Git:    https://github.com/erezsh/lark
# Author: Erez Shinan (erezshin@gmail.com)
#
#
#    >>> LICENSE
#
#    This tool and its generated code use a separate license from Lark.
#
#    It is licensed under GPLv2 or above.
#
#    If you wish to purchase a commercial license for this tool and its
#    generated code, contact me via email.
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 2 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    See <http://www.gnu.org/licenses/>.
#
#

class LarkError(Exception):
    pass

class GrammarError(LarkError):
    pass

class ParseError(LarkError):
    pass

class LexError(LarkError):
    pass

class UnexpectedInput(LarkError):
    pos_in_stream = None

    def get_context(self, text, span=40):
        pos = self.pos_in_stream
        start = max(pos - span, 0)
        end = pos + span
        before = text[start:pos].rsplit('\n', 1)[-1]
        after = text[pos:end].split('\n', 1)[0]
        return before + after + '\n' + ' ' * len(before) + '^\n'

    def match_examples(self, parse_fn, examples):
        """ Given a parser instance and a dictionary mapping some label with
            some malformed syntax examples, it'll return the label for the
            example that bests matches the current error.
        """
        assert self.state is not None, "Not supported for this exception"

        candidate = None
        for label, example in examples.items():
            assert not isinstance(example, STRING_TYPE)

            for malformed in example:
                try:
                    parse_fn(malformed)
                except UnexpectedInput as ut:
                    if ut.state == self.state:
                        try:
                            if ut.token == self.token:  # Try exact match first
                                return label
                        except AttributeError:
                            pass
                        if not candidate:
                            candidate = label

        return candidate


class UnexpectedCharacters(LexError, UnexpectedInput):
    def __init__(self, seq, lex_pos, line, column, allowed=None, considered_tokens=None, state=None):
        message = "No terminal defined for '%s' at line %d col %d" % (seq[lex_pos], line, column)

        self.line = line
        self.column = column
        self.allowed = allowed
        self.considered_tokens = considered_tokens
        self.pos_in_stream = lex_pos
        self.state = state

        message += '\n\n' + self.get_context(seq)
        if allowed:
            message += '\nExpecting: %s\n' % allowed

        super(UnexpectedCharacters, self).__init__(message)



class UnexpectedToken(ParseError, UnexpectedInput):
    def __init__(self, token, expected, considered_rules=None, state=None):
        self.token = token
        self.expected = expected     # XXX str shouldn't necessary
        self.line = getattr(token, 'line', '?')
        self.column = getattr(token, 'column', '?')
        self.considered_rules = considered_rules
        self.state = state
        self.pos_in_stream = getattr(token, 'pos_in_stream', None)

        message = ("Unexpected token %r at line %s, column %s.\n"
                   "Expected: %s\n"
                   % (token, self.line, self.column, ', '.join(self.expected)))

        super(UnexpectedToken, self).__init__(message)


try:
    STRING_TYPE = basestring
except NameError:   # Python 3
    STRING_TYPE = str


import types
from functools import wraps, partial
from contextlib import contextmanager

Str = type(u'')

def smart_decorator(f, create_decorator):
    if isinstance(f, types.FunctionType):
        return wraps(f)(create_decorator(f, True))

    elif isinstance(f, (type, types.BuiltinFunctionType)):
        return wraps(f)(create_decorator(f, False))

    elif isinstance(f, types.MethodType):
        return wraps(f)(create_decorator(f.__func__, True))

    elif isinstance(f, partial):
        # wraps does not work for partials in 2.7: https://bugs.python.org/issue3445
        return create_decorator(f.__func__, True)

    else:
        return create_decorator(f.__func__.__call__, True)




try:
    from contextlib import suppress     # Python 3
except ImportError:
    @contextmanager
    def suppress(*excs):
        '''Catch and dismiss the provided exception

        >>> x = 'hello'
        >>> with suppress(IndexError):
        ...     x = x[10]
        >>> x
        'hello'
        '''
        try:
            yield
        except excs:
            pass



class Meta:
    pass

class Tree(object):
    def __init__(self, data, children, meta=None):
        self.data = data
        self.children = children
        self._meta = meta

    @property
    def meta(self):
        if self._meta is None:
            self._meta = Meta()
        return self._meta

    def __repr__(self):
        return 'Tree(%s, %s)' % (self.data, self.children)

    def _pretty_label(self):
        return self.data

    def _pretty(self, level, indent_str):
        if len(self.children) == 1 and not isinstance(self.children[0], Tree):
            return [ indent_str*level, self._pretty_label(), '\t', '%s' % (self.children[0],), '\n']

        l = [ indent_str*level, self._pretty_label(), '\n' ]
        for n in self.children:
            if isinstance(n, Tree):
                l += n._pretty(level+1, indent_str)
            else:
                l += [ indent_str*(level+1), '%s' % (n,), '\n' ]

        return l

    def pretty(self, indent_str='  '):
        return ''.join(self._pretty(0, indent_str))
    def __eq__(self, other):
        try:
            return self.data == other.data and self.children == other.children
        except AttributeError:
            return False

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash((self.data, tuple(self.children)))

class Indenter:
    def __init__(self):
        self.paren_level = 0
        self.indent_level = [0]

    def handle_NL(self, token):
        if self.paren_level > 0:
            return

        yield token

        indent_str = token.rsplit('\n', 1)[1] # Tabs and spaces
        indent = indent_str.count(' ') + indent_str.count('\t') * self.tab_len

        if indent > self.indent_level[-1]:
            self.indent_level.append(indent)
            yield Token.new_borrow_pos(self.INDENT_type, indent_str, token)
        else:
            while indent < self.indent_level[-1]:
                self.indent_level.pop()
                yield Token.new_borrow_pos(self.DEDENT_type, indent_str, token)

            assert indent == self.indent_level[-1], '%s != %s' % (indent, self.indent_level[-1])

    def process(self, stream):
        for token in stream:
            if token.type == self.NL_type:
                for t in self.handle_NL(token):
                    yield t
            else:
                yield token

            if token.type in self.OPEN_PAREN_types:
                self.paren_level += 1
            elif token.type in self.CLOSE_PAREN_types:
                self.paren_level -= 1
                assert self.paren_level >= 0

        while len(self.indent_level) > 1:
            self.indent_level.pop()
            yield Token(self.DEDENT_type, '')

        assert self.indent_level == [0], self.indent_level

    # XXX Hack for ContextualLexer. Maybe there's a more elegant solution?
    @property
    def always_accept(self):
        return (self.NL_type,)


class Token(Str):
    __slots__ = ('type', 'pos_in_stream', 'value', 'line', 'column', 'end_line', 'end_column')

    def __new__(cls, type_, value, pos_in_stream=None, line=None, column=None):
        self = super(Token, cls).__new__(cls, value)
        self.type = type_
        self.pos_in_stream = pos_in_stream
        self.value = value
        self.line = line
        self.column = column
        self.end_line = None
        self.end_column = None
        return self

    @classmethod
    def new_borrow_pos(cls, type_, value, borrow_t):
        return cls(type_, value, borrow_t.pos_in_stream, line=borrow_t.line, column=borrow_t.column)

    def __reduce__(self):
        return (self.__class__, (self.type, self.value, self.pos_in_stream, self.line, self.column, ))

    def __repr__(self):
        return 'Token(%s, %r)' % (self.type, self.value)

    def __deepcopy__(self, memo):
        return Token(self.type, self.value, self.pos_in_stream, self.line, self.column)

    def __eq__(self, other):
        if isinstance(other, Token) and self.type != other.type:
            return False

        return Str.__eq__(self, other)

    __hash__ = Str.__hash__


class LineCounter:
    def __init__(self):
        self.newline_char = '\n'
        self.char_pos = 0
        self.line = 1
        self.column = 1
        self.line_start_pos = 0

    def feed(self, token, test_newline=True):
        """Consume a token and calculate the new line & column.

        As an optional optimization, set test_newline=False is token doesn't contain a newline.
        """
        if test_newline:
            newlines = token.count(self.newline_char)
            if newlines:
                self.line += newlines
                self.line_start_pos = self.char_pos + token.rindex(self.newline_char) + 1

        self.char_pos += len(token)
        self.column = self.char_pos - self.line_start_pos + 1

class _Lex:
    "Built to serve both Lexer and ContextualLexer"
    def __init__(self, lexer, state=None):
        self.lexer = lexer
        self.state = state

    def lex(self, stream, newline_types, ignore_types):
        newline_types = list(newline_types)
        ignore_types = list(ignore_types)
        line_ctr = LineCounter()

        t = None
        while True:
            lexer = self.lexer
            for mre, type_from_index in lexer.mres:
                m = mre.match(stream, line_ctr.char_pos)
                if m:
                    value = m.group(0)
                    type_ = type_from_index[m.lastindex]
                    if type_ not in ignore_types:
                        t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                        if t.type in lexer.callback:
                            t = lexer.callback[t.type](t)
                        yield t
                    else:
                        if type_ in lexer.callback:
                            t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                            lexer.callback[type_](t)

                    line_ctr.feed(value, type_ in newline_types)
                    if t:
                        t.end_line = line_ctr.line
                        t.end_column = line_ctr.column

                    break
            else:
                if line_ctr.char_pos < len(stream):
                    raise UnexpectedCharacters(stream, line_ctr.char_pos, line_ctr.line, line_ctr.column, state=self.state)
                break

        if t:
            t.end_line = line_ctr.line
            t.end_column = line_ctr.column

class UnlessCallback:
    def __init__(self, mres):
        self.mres = mres

    def __call__(self, t):
        for mre, type_from_index in self.mres:
            m = mre.match(t.value)
            if m:
                t.type = type_from_index[m.lastindex]
                break
        return t


from functools import partial, wraps


class ExpandSingleChild:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        if len(children) == 1:
            return children[0]
        else:
            return self.node_builder(children)


class PropagatePositions:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        res = self.node_builder(children)

        if children and isinstance(res, Tree):
            for a in children:
                if isinstance(a, Tree):
                    res.meta.line = a.meta.line
                    res.meta.column = a.meta.column
                elif isinstance(a, Token):
                    res.meta.line = a.line
                    res.meta.column = a.column
                break

            for a in reversed(children):
                # with suppress(AttributeError):
                if isinstance(a, Tree):
                    res.meta.end_line = a.meta.end_line
                    res.meta.end_column = a.meta.end_column
                elif isinstance(a, Token):
                    res.meta.end_line = a.end_line
                    res.meta.end_column = a.end_column

                break

        return res


class ChildFilter:
    def __init__(self, to_include, node_builder):
        self.node_builder = node_builder
        self.to_include = to_include

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                filtered += children[i].children
            else:
                filtered.append(children[i])

        return self.node_builder(filtered)

class ChildFilterLALR(ChildFilter):
    "Optimized childfilter for LALR (assumes no duplication in parse tree, so it's safe to change it)"

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                if filtered:
                    filtered += children[i].children
                else:   # Optimize for left-recursion
                    filtered = children[i].children
            else:
                filtered.append(children[i])

        return self.node_builder(filtered)

def _should_expand(sym):
    return not sym.is_term and sym.name.startswith('_')

def maybe_create_child_filter(expansion, keep_all_tokens, ambiguous):
    to_include = [(i, _should_expand(sym)) for i, sym in enumerate(expansion)
                  if keep_all_tokens or not (sym.is_term and sym.filter_out)]

    if len(to_include) < len(expansion) or any(to_expand for i, to_expand in to_include):
        return partial(ChildFilter if ambiguous else ChildFilterLALR, to_include)


class Callback(object):
    pass


def inline_args(func):
    @wraps(func)
    def f(children):
        return func(*children)
    return f



class ParseTreeBuilder:
    def __init__(self, rules, tree_class, propagate_positions=False, keep_all_tokens=False, ambiguous=False):
        self.tree_class = tree_class
        self.propagate_positions = propagate_positions
        self.always_keep_all_tokens = keep_all_tokens
        self.ambiguous = ambiguous

        self.rule_builders = list(self._init_builders(rules))

        self.user_aliases = {}

    def _init_builders(self, rules):
        for rule in rules:
            options = rule.options
            keep_all_tokens = self.always_keep_all_tokens or (options.keep_all_tokens if options else False)
            expand_single_child = options.expand1 if options else False

            wrapper_chain = filter(None, [
                (expand_single_child and not rule.alias) and ExpandSingleChild,
                maybe_create_child_filter(rule.expansion, keep_all_tokens, self.ambiguous),
                self.propagate_positions and PropagatePositions,
            ])

            yield rule, wrapper_chain


    def create_callback(self, transformer=None):
        callback = Callback()

        i = 0
        for rule, wrapper_chain in self.rule_builders:
            internal_callback_name = '_cb%d_%s' % (i, rule.origin)
            i += 1

            user_callback_name = rule.alias or rule.origin.name
            try:
                f = getattr(transformer, user_callback_name)
                assert not getattr(f, 'meta', False), "Meta args not supported for internal transformer"
                # XXX InlineTransformer is deprecated!
                if getattr(f, 'inline', False) or isinstance(transformer, InlineTransformer):
                    f = inline_args(f)
            except AttributeError:
                f = partial(self.tree_class, user_callback_name)

            self.user_aliases[rule] = rule.alias
            rule.alias = internal_callback_name

            for w in wrapper_chain:
                f = w(f)

            if hasattr(callback, internal_callback_name):
                raise GrammarError("Rule '%s' already exists" % (rule,))
            setattr(callback, internal_callback_name, f)

        return callback



class _Parser:
    def __init__(self, parse_table, callbacks):
        self.states = parse_table.states
        self.start_state = parse_table.start_state
        self.end_state = parse_table.end_state
        self.callbacks = callbacks

    def parse(self, seq, set_state=None):
        i = 0
        token = None
        stream = iter(seq)
        states = self.states

        state_stack = [self.start_state]
        value_stack = []

        if set_state: set_state(self.start_state)

        def get_action(key):
            state = state_stack[-1]
            try:
                return states[state][key]
            except KeyError:
                expected = states[state].keys()
                raise UnexpectedToken(token, expected, state=state)  # TODO filter out rules from expected

        def reduce(rule):
            size = len(rule.expansion)
            if size:
                s = value_stack[-size:]
                del state_stack[-size:]
                del value_stack[-size:]
            else:
                s = []

            value = self.callbacks[rule](s)

            _action, new_state = get_action(rule.origin.name)
            assert _action is Shift
            state_stack.append(new_state)
            value_stack.append(value)

        # Main LALR-parser loop
        # print('')
        for i, token in enumerate(stream):
            # x = token; print(repr(f"[@{i},{x.pos_in_stream}:{x.pos_in_stream+len(x.value)-1}='{x.value}'<{x.type}>,{x.line}:{x.column}]"))
            while True:
                action, arg = get_action(token.type)
                assert arg != self.end_state

                if action is Shift:
                    state_stack.append(arg)
                    value_stack.append(token)
                    if set_state: set_state(arg)
                    break # next token
                else:
                    reduce(arg)

        while True:
            _action, arg = get_action('$END')
            if _action is Shift:
                assert arg == self.end_state
                val ,= value_stack
                return val
            else:
                reduce(arg)


class Symbol(object):
    is_term = NotImplemented

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        assert isinstance(other, Symbol), other
        return self.is_term == other.is_term and self.name == other.name

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash(self.name)

    def __repr__(self):
        return '%s(%r)' % (type(self).__name__, self.name)

class Terminal(Symbol):
    is_term = True

    def __init__(self, name, filter_out=False):
        self.name = name
        self.filter_out = filter_out


class NonTerminal(Symbol):
    is_term = False

class Rule(object):
    """
        origin : a symbol
        expansion : a list of symbols
    """
    def __init__(self, origin, expansion, alias=None, options=None):
        self.origin = origin
        self.expansion = expansion
        self.alias = alias
        self.options = options

    def __str__(self):
        return '<%s : %s>' % (self.origin, ' '.join(map(str,self.expansion)))

    def __repr__(self):
        return 'Rule(%r, %r, %r, %r)' % (self.origin, self.expansion, self.alias, self.options)


class RuleOptions:
    def __init__(self, keep_all_tokens=False, expand1=False, priority=None):
        self.keep_all_tokens = keep_all_tokens
        self.expand1 = expand1
        self.priority = priority

    def __repr__(self):
        return 'RuleOptions(%r, %r, %r)' % (
            self.keep_all_tokens,
            self.expand1,
            self.priority,
        )

Shift = 0
Reduce = 1
import re
MRES = (
[('(?P<_NEWLINE>(?:(?:\r'
  '?\n'
  '[\t ]*|(\\#|\\/\\/)[^\n'
  ']*))+)|(?P<MULTI_LINE_COMMENT>\\/\\*([\\s\\S]*?)\\*\\/)|(?P<__ANON_2>(\\\\{|\\\\}|[^\n'
  '{}])+)|(?P<NEWLINE>(?:(?:\\/r)?\\/n)+)|(?P<SINGLE_LINE_COMMENT>(\\#|\\/\\/)[^\n'
  ']*)|(?P<SPACES>[\t \x0c'
  ']+)|(?P<__ANON_0>[^:\n'
  ']+)|(?P<INTEGER>[0-9])',
  {1: '_NEWLINE',
   3: 'MULTI_LINE_COMMENT',
   5: '__ANON_2',
   7: 'NEWLINE',
   8: 'SINGLE_LINE_COMMENT',
   10: 'SPACES',
   11: '__ANON_0',
   12: 'INTEGER'})]
)
LEXER_CALLBACK = (
{'NEWLINE': [('(?P<LF>\\/n$)', {1: 'LF'})],
 '__ANON_0': [('(?P<__ANON_1>meta_scope$)|(?P<CAPTURES>captures$)|(?P<CONTEXTS>contexts$)|(?P<INCLUDE>include$)|(?P<MATCH>match$)|(?P<SCOPE>scope$)|(?P<NAME>name$)|(?P<PUSH>push$)|(?P<POP>pop$)|(?P<CR>\\/r$)|(?P<LF>\\/n$)|(?P<LBRACE>\\{$)|(?P<RBRACE>\\}$)',
               {1: '__ANON_1',
                2: 'CAPTURES',
                3: 'CONTEXTS',
                4: 'INCLUDE',
                5: 'MATCH',
                6: 'SCOPE',
                7: 'NAME',
                8: 'PUSH',
                9: 'POP',
                10: 'CR',
                11: 'LF',
                12: 'LBRACE',
                13: 'RBRACE'})],
 '__ANON_2': [('(?P<__ANON_1>meta_scope$)|(?P<CAPTURES>captures$)|(?P<CONTEXTS>contexts$)|(?P<INCLUDE>include$)|(?P<MATCH>match$)|(?P<SCOPE>scope$)|(?P<NAME>name$)|(?P<PUSH>push$)|(?P<POP>pop$)|(?P<CR>\\/r$)|(?P<LF>\\/n$)|(?P<COLON>\\:$)',
               {1: '__ANON_1',
                2: 'CAPTURES',
                3: 'CONTEXTS',
                4: 'INCLUDE',
                5: 'MATCH',
                6: 'SCOPE',
                7: 'NAME',
                8: 'PUSH',
                9: 'POP',
                10: 'CR',
                11: 'LF',
                12: 'COLON'})]}
)
NEWLINE_TYPES = ['SINGLE_LINE_COMMENT', '_NEWLINE', '__ANON_0', '__ANON_2']
IGNORE_TYPES = ['SPACES', 'MULTI_LINE_COMMENT', 'SINGLE_LINE_COMMENT']
class LexerRegexps: pass
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
lexer = _Lex(lexer_regexps)
def lex(stream):
    return lexer.lex(stream, NEWLINE_TYPES, IGNORE_TYPES)
class InlineTransformer: pass
RULES = {
  0: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  1: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  2: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  3: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  4: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  5: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  6: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), NonTerminal('language_construct_rules')], None, RuleOptions(False, False, None)),
  7: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  8: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  9: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  10: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  11: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  12: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  13: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  14: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  15: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  16: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  17: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), NonTerminal('language_construct_rules')], None, RuleOptions(False, False, None)),
  18: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  19: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  20: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules')], None, RuleOptions(False, False, None)),
  21: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  22: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  23: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0'), Terminal('_NEWLINE')], None, RuleOptions(False, False, None)),
  24: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  25: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), Terminal('_NEWLINE'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  26: Rule(NonTerminal('language_syntax'), [Terminal('_NEWLINE'), NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules'), NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  27: Rule(NonTerminal('language_syntax'), [NonTerminal('preamble_statements'), Terminal('_NEWLINE'), NonTerminal('language_construct_rules')], None, RuleOptions(False, False, None)),
  28: Rule(NonTerminal('preamble_statements'), [NonTerminal('__anon_plus_1')], None, RuleOptions(False, False, None)),
  29: Rule(NonTerminal('language_construct_rules'), [Terminal('CONTEXTS'), Terminal('COLON'), NonTerminal('indentation_block')], None, RuleOptions(False, False, None)),
  30: Rule(NonTerminal('miscellaneous_language_rules'), [Terminal('__ANON_0'), Terminal('COLON'), NonTerminal('indentation_block')], None, RuleOptions(False, False, None)),
  31: Rule(NonTerminal('target_language_name_statement'), [Terminal('NAME'), Terminal('COLON'), NonTerminal('free_input_string')], None, RuleOptions(False, False, None)),
  32: Rule(NonTerminal('master_scope_name_statement'), [Terminal('SCOPE'), Terminal('COLON'), NonTerminal('free_input_string')], None, RuleOptions(False, False, None)),
  33: Rule(NonTerminal('indentation_block'), [Terminal('LBRACE'), Terminal('_NEWLINE'), NonTerminal('__anon_plus_2'), Terminal('RBRACE')], None, RuleOptions(False, False, None)),
  34: Rule(NonTerminal('statements_list'), [NonTerminal('pop_statement')], None, RuleOptions(False, False, None)),
  35: Rule(NonTerminal('statements_list'), [NonTerminal('meta_scope_statement')], None, RuleOptions(False, False, None)),
  36: Rule(NonTerminal('statements_list'), [NonTerminal('include_statement')], None, RuleOptions(False, False, None)),
  37: Rule(NonTerminal('statements_list'), [NonTerminal('match_statement')], None, RuleOptions(False, False, None)),
  38: Rule(NonTerminal('statements_list'), [NonTerminal('push_statement')], None, RuleOptions(False, False, None)),
  39: Rule(NonTerminal('push_statement'), [Terminal('PUSH'), Terminal('COLON'), NonTerminal('indentation_block')], None, RuleOptions(False, False, None)),
  40: Rule(NonTerminal('include_statement'), [Terminal('INCLUDE'), Terminal('COLON'), NonTerminal('free_input_string')], None, RuleOptions(False, False, None)),
  41: Rule(NonTerminal('match_statements'), [NonTerminal('scope_name')], None, RuleOptions(False, False, None)),
  42: Rule(NonTerminal('match_statements'), [NonTerminal('statements_list')], None, RuleOptions(False, False, None)),
  43: Rule(NonTerminal('match_statements'), [NonTerminal('capturing_block')], None, RuleOptions(False, False, None)),
  44: Rule(NonTerminal('match_statement'), [Terminal('MATCH'), Terminal('COLON'), NonTerminal('free_input_string'), Terminal('LBRACE'), Terminal('_NEWLINE'), Terminal('RBRACE')], None, RuleOptions(False, False, None)),
  45: Rule(NonTerminal('match_statement'), [Terminal('MATCH'), Terminal('COLON'), NonTerminal('free_input_string'), Terminal('LBRACE'), NonTerminal('__anon_star_3'), Terminal('_NEWLINE'), Terminal('RBRACE')], None, RuleOptions(False, False, None)),
  46: Rule(NonTerminal('scope_name'), [Terminal('SCOPE'), Terminal('COLON'), NonTerminal('free_input_string')], None, RuleOptions(False, False, None)),
  47: Rule(NonTerminal('capturing_block'), [Terminal('CAPTURES'), Terminal('COLON'), Terminal('LBRACE'), NonTerminal('__anon_plus_4'), Terminal('_NEWLINE'), Terminal('RBRACE')], None, RuleOptions(False, False, None)),
  48: Rule(NonTerminal('capturing_lines'), [NonTerminal('__anon_plus_5'), Terminal('COLON'), NonTerminal('free_input_string')], None, RuleOptions(False, False, None)),
  49: Rule(NonTerminal('pop_statement'), [Terminal('POP'), Terminal('COLON'), NonTerminal('free_input_string')], None, RuleOptions(False, False, None)),
  50: Rule(NonTerminal('meta_scope_statement'), [Terminal('__ANON_1'), Terminal('COLON'), NonTerminal('free_input_string')], None, RuleOptions(False, False, None)),
  51: Rule(NonTerminal('free_input_string'), [Terminal('__ANON_2')], None, RuleOptions(False, False, None)),
  52: Rule(NonTerminal('__anon_star_0'), [NonTerminal('__anon_star_0'), NonTerminal('miscellaneous_language_rules'), Terminal('_NEWLINE')], None, None),
  53: Rule(NonTerminal('__anon_star_0'), [NonTerminal('__anon_star_0'), NonTerminal('miscellaneous_language_rules')], None, None),
  54: Rule(NonTerminal('__anon_star_0'), [NonTerminal('miscellaneous_language_rules')], None, None),
  55: Rule(NonTerminal('__anon_star_0'), [NonTerminal('miscellaneous_language_rules'), Terminal('_NEWLINE')], None, None),
  56: Rule(NonTerminal('__anon_plus_1'), [NonTerminal('master_scope_name_statement'), Terminal('_NEWLINE')], None, None),
  57: Rule(NonTerminal('__anon_plus_1'), [NonTerminal('__anon_plus_1'), NonTerminal('target_language_name_statement'), Terminal('_NEWLINE')], None, None),
  58: Rule(NonTerminal('__anon_plus_1'), [NonTerminal('__anon_plus_1'), NonTerminal('master_scope_name_statement'), Terminal('_NEWLINE')], None, None),
  59: Rule(NonTerminal('__anon_plus_1'), [NonTerminal('target_language_name_statement'), Terminal('_NEWLINE')], None, None),
  60: Rule(NonTerminal('__anon_plus_2'), [NonTerminal('statements_list'), Terminal('_NEWLINE')], None, None),
  61: Rule(NonTerminal('__anon_plus_2'), [NonTerminal('__anon_plus_2'), NonTerminal('statements_list'), Terminal('_NEWLINE')], None, None),
  62: Rule(NonTerminal('__anon_star_3'), [NonTerminal('__anon_star_3'), Terminal('_NEWLINE'), NonTerminal('match_statements')], None, None),
  63: Rule(NonTerminal('__anon_star_3'), [Terminal('_NEWLINE'), NonTerminal('match_statements')], None, None),
  64: Rule(NonTerminal('__anon_plus_4'), [NonTerminal('__anon_plus_4'), Terminal('_NEWLINE'), NonTerminal('capturing_lines')], None, None),
  65: Rule(NonTerminal('__anon_plus_4'), [Terminal('_NEWLINE'), NonTerminal('capturing_lines')], None, None),
  66: Rule(NonTerminal('__anon_plus_5'), [NonTerminal('__anon_plus_5'), Terminal('INTEGER')], None, None),
  67: Rule(NonTerminal('__anon_plus_5'), [Terminal('INTEGER')], None, None),
}
parse_tree_builder = ParseTreeBuilder(RULES.values(), Tree)
class ParseTable: pass
parse_table = ParseTable()
STATES = {
  0: {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8)},
  1: {3: (0, 9), 8: (0, 10), 9: (0, 11)},
  2: {10: (0, 12)},
  3: {9: (1, 28), 3: (1, 28), 7: (0, 13), 1: (0, 2), 5: (0, 6), 4: (0, 14)},
  4: {0: (0, 15), 1: (0, 2), 2: (0, 3), 4: (0, 5), 5: (0, 6), 7: (0, 8)},
  5: {3: (0, 16)},
  6: {10: (0, 17)},
  7: {11: (0, 18)},
  8: {3: (0, 19)},
  9: {8: (0, 20), 9: (0, 11)},
  10: {11: (1, 6), 12: (0, 21), 13: (0, 22), 3: (0, 23), 14: (0, 24)},
  11: {10: (0, 25)},
  12: {15: (0, 26), 16: (0, 27)},
  13: {3: (0, 28)},
  14: {3: (0, 29)},
  15: {3: (0, 30), 8: (0, 31), 9: (0, 11)},
  16: {5: (1, 56), 3: (1, 56), 1: (1, 56), 9: (1, 56)},
  17: {15: (0, 26), 16: (0, 32)},
  18: {},
  19: {5: (1, 59), 3: (1, 59), 1: (1, 59), 9: (1, 59)},
  20: {11: (1, 27), 12: (0, 33), 13: (0, 22), 3: (0, 34), 14: (0, 24)},
  21: {11: (1, 12), 13: (0, 35), 14: (0, 24), 3: (0, 36)},
  22: {11: (1, 54), 3: (0, 37), 14: (1, 54)},
  23: {11: (1, 19), 3: (0, 38), 12: (0, 39), 13: (0, 22), 14: (0, 24)},
  24: {10: (0, 40)},
  25: {17: (0, 41), 18: (0, 42)},
  26: {17: (1, 51), 3: (1, 51)},
  27: {3: (1, 31)},
  28: {5: (1, 57), 3: (1, 57), 1: (1, 57), 9: (1, 57)},
  29: {5: (1, 58), 3: (1, 58), 1: (1, 58), 9: (1, 58)},
  30: {8: (0, 43), 9: (0, 11)},
  31: {11: (1, 17), 12: (0, 44), 3: (0, 45), 13: (0, 22), 14: (0, 24)},
  32: {3: (1, 32)},
  33: {11: (1, 10), 13: (0, 35), 14: (0, 24), 3: (0, 46)},
  34: {11: (1, 3), 3: (0, 47), 12: (0, 48), 13: (0, 22), 14: (0, 24)},
  35: {11: (1, 53), 3: (0, 49), 14: (1, 53)},
  36: {11: (1, 22)},
  37: {11: (1, 55), 3: (1, 55), 14: (1, 55)},
  38: {11: (1, 4)},
  39: {11: (1, 11), 13: (0, 35), 3: (0, 50), 14: (0, 24)},
  40: {17: (0, 41), 18: (0, 51)},
  41: {3: (0, 52)},
  42: {11: (1, 29), 3: (1, 29), 14: (1, 29)},
  43: {11: (1, 20), 12: (0, 53), 13: (0, 22), 3: (0, 54), 14: (0, 24)},
  44: {11: (1, 21), 13: (0, 35), 3: (0, 55), 14: (0, 24)},
  45: {11: (1, 5), 12: (0, 56), 13: (0, 22), 3: (0, 57), 14: (0, 24)},
  46: {11: (1, 8)},
  47: {11: (1, 13)},
  48: {11: (1, 25), 13: (0, 35), 3: (0, 58), 14: (0, 24)},
  49: {11: (1, 52), 3: (1, 52), 14: (1, 52)},
  50: {11: (1, 16)},
  51: {3: (1, 30), 14: (1, 30), 11: (1, 30)},
  52: {19: (0, 59), 20: (0, 60), 21: (0, 61), 22: (0, 62), 23: (0, 63), 24: (0, 64), 25: (0, 65), 26: (0, 66), 27: (0, 67), 28: (0, 68), 29: (0, 69), 30: (0, 70)},
  53: {11: (1, 26), 3: (0, 71), 13: (0, 35), 14: (0, 24)},
  54: {11: (1, 2), 12: (0, 72), 13: (0, 22), 3: (0, 73), 14: (0, 24)},
  55: {11: (1, 0)},
  56: {11: (1, 15), 3: (0, 74), 13: (0, 35), 14: (0, 24)},
  57: {11: (1, 1)},
  58: {11: (1, 9)},
  59: {3: (1, 35)},
  60: {10: (0, 75)},
  61: {19: (0, 59), 20: (0, 60), 27: (0, 76), 22: (0, 62), 23: (0, 63), 24: (0, 64), 25: (0, 65), 26: (0, 66), 28: (0, 68), 31: (0, 77), 29: (0, 69), 30: (0, 70)},
  62: {10: (0, 78)},
  63: {3: (1, 36)},
  64: {10: (0, 79)},
  65: {3: (1, 38)},
  66: {10: (0, 80)},
  67: {3: (0, 81)},
  68: {3: (1, 34)},
  69: {10: (0, 82)},
  70: {3: (1, 37)},
  71: {11: (1, 23)},
  72: {11: (1, 24), 13: (0, 35), 3: (0, 83), 14: (0, 24)},
  73: {11: (1, 14)},
  74: {11: (1, 7)},
  75: {17: (0, 41), 18: (0, 84)},
  76: {3: (0, 85)},
  77: {3: (1, 33), 14: (1, 33), 11: (1, 33)},
  78: {16: (0, 86), 15: (0, 26)},
  79: {15: (0, 26), 16: (0, 87)},
  80: {16: (0, 88), 15: (0, 26)},
  81: {26: (1, 60), 29: (1, 60), 31: (1, 60), 22: (1, 60), 24: (1, 60), 20: (1, 60)},
  82: {15: (0, 26), 16: (0, 89)},
  83: {11: (1, 18)},
  84: {3: (1, 39)},
  85: {26: (1, 61), 29: (1, 61), 31: (1, 61), 22: (1, 61), 24: (1, 61), 20: (1, 61)},
  86: {17: (0, 90)},
  87: {3: (1, 50)},
  88: {3: (1, 40)},
  89: {3: (1, 49)},
  90: {32: (0, 91), 3: (0, 92)},
  91: {3: (0, 93)},
  92: {19: (0, 59), 27: (0, 94), 33: (0, 95), 34: (0, 96), 23: (0, 63), 24: (0, 64), 26: (0, 66), 28: (0, 68), 5: (0, 97), 29: (0, 69), 20: (0, 60), 31: (0, 98), 35: (0, 99), 22: (0, 62), 25: (0, 65), 36: (0, 100), 30: (0, 70)},
  93: {19: (0, 59), 27: (0, 94), 34: (0, 96), 23: (0, 63), 24: (0, 64), 26: (0, 66), 28: (0, 68), 5: (0, 97), 29: (0, 69), 20: (0, 60), 31: (0, 101), 35: (0, 99), 22: (0, 62), 25: (0, 65), 33: (0, 102), 36: (0, 100), 30: (0, 70)},
  94: {3: (1, 42)},
  95: {3: (1, 63)},
  96: {3: (1, 41)},
  97: {10: (0, 103)},
  98: {3: (1, 44)},
  99: {3: (1, 43)},
  100: {10: (0, 104)},
  101: {3: (1, 45)},
  102: {3: (1, 62)},
  103: {15: (0, 26), 16: (0, 105)},
  104: {17: (0, 106)},
  105: {3: (1, 46)},
  106: {37: (0, 107), 3: (0, 108)},
  107: {3: (0, 109)},
  108: {38: (0, 110), 39: (0, 111), 40: (0, 112)},
  109: {38: (0, 113), 31: (0, 114), 39: (0, 111), 40: (0, 112)},
  110: {3: (1, 65)},
  111: {10: (0, 115), 40: (0, 116)},
  112: {10: (1, 67), 40: (1, 67)},
  113: {3: (1, 64)},
  114: {3: (1, 47)},
  115: {15: (0, 26), 16: (0, 117)},
  116: {10: (1, 66), 40: (1, 66)},
  117: {3: (1, 48)},
}
TOKEN_TYPES = (
{0: 'preamble_statements',
 1: 'NAME',
 2: '__anon_plus_1',
 3: '_NEWLINE',
 4: 'master_scope_name_statement',
 5: 'SCOPE',
 6: 'language_syntax',
 7: 'target_language_name_statement',
 8: 'language_construct_rules',
 9: 'CONTEXTS',
 10: 'COLON',
 11: '$END',
 12: '__anon_star_0',
 13: 'miscellaneous_language_rules',
 14: '__ANON_0',
 15: '__ANON_2',
 16: 'free_input_string',
 17: 'LBRACE',
 18: 'indentation_block',
 19: 'meta_scope_statement',
 20: 'PUSH',
 21: '__anon_plus_2',
 22: 'MATCH',
 23: 'include_statement',
 24: '__ANON_1',
 25: 'push_statement',
 26: 'INCLUDE',
 27: 'statements_list',
 28: 'pop_statement',
 29: 'POP',
 30: 'match_statement',
 31: 'RBRACE',
 32: '__anon_star_3',
 33: 'match_statements',
 34: 'scope_name',
 35: 'capturing_block',
 36: 'CAPTURES',
 37: '__anon_plus_4',
 38: 'capturing_lines',
 39: '__anon_plus_5',
 40: 'INTEGER'}
)
parse_table.states = {s: {TOKEN_TYPES[t]: (a, RULES[x] if a is Reduce else x) for t, (a, x) in acts.items()}
                      for s, acts in STATES.items()}
parse_table.start_state = 0
parse_table.end_state = 18
class Lark_StandAlone:
  def __init__(self, transformer=None, postlex=None):
     callback = parse_tree_builder.create_callback(transformer=transformer)
     callbacks = {rule: getattr(callback, rule.alias or rule.origin, None) for rule in RULES.values()}
     self.parser = _Parser(parse_table, callbacks)
     self.postlex = postlex
  def parse(self, stream):
     tokens = lex(stream)
     if self.postlex: tokens = self.postlex.process(tokens)
     return self.parser.parse(tokens)
